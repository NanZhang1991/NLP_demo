{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Requirement already satisfied: pynvml in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (8.0.4)\n",
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Requirement already satisfied: transformers in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from sacremoses->transformers) (8.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from click->sacremoses->transformers) (0.4.4)\n",
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from scikit-learn) (1.6.3)\n",
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Requirement already satisfied: pandas in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pynvml\r\n",
    "!pip install transformers\r\n",
    "!pip install scikit-learn\r\n",
    "!pip install pandas\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\r\n",
    "print(tf.config.list_physical_devices('GPU'))\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    train_set, x = train_test_split(df, \n",
    "        stratify=df['label'],\n",
    "        test_size=0.1, \n",
    "        random_state=42)\n",
    "    val_set, test_set = train_test_split(x, \n",
    "        stratify=x['label'],\n",
    "        test_size=0.5, \n",
    "        random_state=43)\n",
    "\n",
    "    return train_set,val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (200000, 3)\n",
      "TRAIN Dataset: (180000, 3)\n",
      "TEST Dataset: (10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>57544</th>\n      <td>男子长时间上网昏倒后被转入重症监护室</td>\n      <td>社会</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>28829</th>\n      <td>《生化尖兵》恶搞广告片：生化经理人</td>\n      <td>游戏</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>128336</th>\n      <td>《武林OL》“决战光明顶”今日上线</td>\n      <td>游戏</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>147143</th>\n      <td>美证交会调查华尔街ETF内幕交易</td>\n      <td>股票</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>54266</th>\n      <td>美国信用评级下降 美债反受青睐</td>\n      <td>股票</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                      text label  y\n57544   男子长时间上网昏倒后被转入重症监护室    社会  5\n28829    《生化尖兵》恶搞广告片：生化经理人    游戏  8\n128336   《武林OL》“决战光明顶”今日上线    游戏  8\n147143    美证交会调查华尔街ETF内幕交易    股票  2\n54266      美国信用评级下降 美债反受青睐    股票  2"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"./data/THUCNewsChinese.txt\"\r\n",
    "# read data\r\n",
    "df_raw = pd.read_csv(data_path,sep=\"\\t\",header=None,names=[\"text\",\"label\"])\r\n",
    "# transfer label\r\n",
    "df_label = pd.DataFrame({\"label\":[\"财经\",\"房产\",\"股票\",\"教育\",\"科技\",\"社会\",\"时政\",\"体育\",\"游戏\",\"娱乐\"],\"y\":list(range(10))})\r\n",
    "df_raw = pd.merge(df_raw,df_label,on=\"label\",how=\"left\")\r\n",
    "# split data\r\n",
    "train_data, val_data, test_data = split_dataset(df_raw)\r\n",
    "print(\"FULL Dataset: {}\".format(df_raw.shape))\r\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\r\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\r\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customDataset(dataSet):\n",
    "    inputs = tokenizer(dataSet['text'].tolist(), max_length=max_length, padding='max_length', truncation=True,\\\n",
    "                   return_tensors='tf')\n",
    "    if 'y' in dataSet.columns:\n",
    "        label_list = dataSet['y'].values.tolist() \n",
    "    else:\n",
    "        label_list = None\n",
    "    result = tf.data.Dataset.from_tensor_slices((dict((k,v) for k, v in inputs.items()), label_list))          \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
      "Wall time: 36.6 s\n"
=======
<<<<<<< HEAD
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
      "GPU Memory size:4236312576\n",
      "CPU times: user 178 ms, sys: 8.68 ms, total: 187 ms\n",
      "Wall time: 201 ms\n"
=======
      "GPU Memory size:4294967296\n",
      "Wall time: 240 ms\n"
>>>>>>> d3a5e0cbba237f05dbf4cc023e845682961f840a
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "\r\n",
    "model_path = '../model_dirs/bert-base-chinese'\r\n",
    "\r\n",
    "# parameters\r\n",
    "batch_size = 8\r\n",
    "# pynvml.nvmlInit()\r\n",
    "# handle = pynvml.nvmlDeviceGetHandleByIndex(0)\r\n",
    "# meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\r\n",
    "# print(f'GPU Memory size:{meminfo.total}')\r\n",
    "# if meminfo.total> 1024**3*10:\r\n",
    "#     batch_size = 16 \r\n",
    "max_length = 128    \r\n",
    "learning_rate = 2e-5\r\n",
    "number_of_epochs = 2\r\n",
    "num_classes = 10 #\r\n",
    "\r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "# train dataset\r\n",
    "ds_train_encoded = customDataset(train_data).shuffle(1000).batch(batch_size)\r\n",
    "# val dataset\r\n",
    "ds_val_encoded = customDataset(val_data).batch(batch_size)\r\n",
    "# test dataset\r\n",
    "ds_test_encoded = customDataset(test_data).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at ../model_dirs/bert-base-chinese and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
      "Epoch 1/2\n",
      "22500/22500 [==============================] - 16944s 752ms/step - loss: 0.3750 - accuracy: 0.8901 - val_loss: 0.2706 - val_accuracy: 0.9251\n",
      "Epoch 2/2\n",
      "22500/22500 [==============================] - 15754s 700ms/step - loss: 0.2129 - accuracy: 0.9359 - val_loss: 0.2793 - val_accuracy: 0.9262\n",
      "1250/1250 [==============================] - 252s 202ms/step - loss: 0.2438 - accuracy: 0.9378\n",
      "# evaluate test_set: [0.2438269555568695, 0.9377999901771545]\n"
=======
<<<<<<< HEAD
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at ../model_dirs/bert-base-chinese and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2\n",
      "113/113 [==============================] - 46s 265ms/step - loss: 1.9609 - accuracy: 0.3831 - val_loss: 0.7642 - val_accuracy: 0.7800\n",
      "Epoch 2/2\n",
      "113/113 [==============================] - 28s 248ms/step - loss: 0.5786 - accuracy: 0.8557 - val_loss: 0.7301 - val_accuracy: 0.8200\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 0.5558 - accuracy: 0.8200\n",
      "# evaluate test_set: [0.5558286309242249, 0.8199999928474426]\n"
=======
      "113/113 [==============================] - 99s 717ms/step - loss: 1.9217 - accuracy: 0.4101 - val_loss: 0.7655 - val_accuracy: 0.7600\n",
      "Epoch 2/2\n",
      "113/113 [==============================] - 79s 700ms/step - loss: 0.5439 - accuracy: 0.8759 - val_loss: 0.5694 - val_accuracy: 0.8600\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 0.4739 - accuracy: 0.8400\n",
      "# evaluate test_set: [0.47386154532432556, 0.8399999737739563]\n"
>>>>>>> d3a5e0cbba237f05dbf4cc023e845682961f840a
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(model_path, num_labels=num_classes)\n",
    "# optimizer Adam recommended\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08, clipnorm=1)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "# fit model\n",
    "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_val_encoded)\n",
    "# evaluate test_set\n",
    "print(\"# evaluate test_set:\",model.evaluate(ds_test_encoded))\n",
    "## model save\n",
    "model.save_pretrained('../model_dirs/fine_tune_MultiClass_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
      "1250/1250 [==============================] - 260s 208ms/step - loss: 0.2793 - accuracy: 0.9262\n"
=======
<<<<<<< HEAD
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
      "7/7 [==============================] - 0s 58ms/step - loss: 0.7301 - accuracy: 0.8200\n"
=======
      "Epoch 1/2\n",
      "7/7 [==============================] - 1s 182ms/step - loss: 0.5694 - accuracy: 0.8600\n"
>>>>>>> d3a5e0cbba237f05dbf4cc023e845682961f840a
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
     ]
    },
    {
     "data": {
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
      "text/plain": "[0.2792842388153076, 0.9261999726295471]"
=======
<<<<<<< HEAD
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
      "text/plain": [
       "[0.7300801873207092, 0.8199999928474426]"
      ]
=======
      "text/plain": "[0.5693955421447754, 0.8600000143051147]"
>>>>>>> d3a5e0cbba237f05dbf4cc023e845682961f840a
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_label(logits):\n",
    "    for i in range(len(logits)):\n",
    "        y = tf.argmax(tf.nn.softmax(logits[i], axis=-1)).numpy()\n",
    "        yield y\n",
    "\n",
    "def validation(epoch):\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    for _, data in enumerate(ds_val_encoded, 0):\n",
    "        inputs = data[0]\n",
    "        targets = data[1]\n",
    "        # outputs = model.predict(dict(inputs))\n",
    "        outputs = model(inputs)       \n",
    "        fin_targets.extend(targets.numpy())\n",
    "        fin_outputs.extend(y_label(outputs.logits))\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
      "Accuracy Score = 0.9262\n",
      "recall_score (Micro) = 0.9262\n",
      "recall_score (Macro) = 0.9262\n",
      "F1 Score (Micro) = 0.9261999999999999\n",
      "F1 Score (Macro) = 0.9264376565593148\n"
=======
<<<<<<< HEAD
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
      "Accuracy Score = 0.82\nrecall_score (Micro) = 0.82\nrecall_score (Macro) = 0.8216666666666667\nF1 Score (Micro) = 0.82\nF1 Score (Macro) = 0.8236868686868688\n"
=======
      "Accuracy Score = 0.86\n",
      "recall_score (Micro) = 0.86\n",
      "recall_score (Macro) = 0.8616666666666667\n",
      "F1 Score (Micro) = 0.8599999999999999\n",
      "F1 Score (Macro) = 0.8645959595959596\n"
>>>>>>> d3a5e0cbba237f05dbf4cc023e845682961f840a
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
=======
>>>>>>> e1123f5158e5355cb7c1597739a2b8edc9b86b2a
     ]
    }
   ],
   "source": [
    "outputs, targets = validation(number_of_epochs)\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "recall_score_micro = metrics.recall_score(targets, outputs, average='micro')\n",
    "recall_score_macro = metrics.recall_score(targets, outputs, average='macro')\n",
    "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "\n",
    "print(f\"Accuracy Score = {accuracy}\")\n",
    "print(f\"recall_score (Micro) = {recall_score_micro}\")\n",
    "print(f\"recall_score (Macro) = {recall_score_macro}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tfs': conda)",
   "name": "python3710jvsc74a57bd0aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}