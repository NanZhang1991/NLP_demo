{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Requirement already satisfied: pynvml in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (8.0.4)\n",
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Requirement already satisfied: transformers in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (4.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from scikit-learn) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from scikit-learn) (1.19.5)\n",
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Requirement already satisfied: pandas in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\miniconda3\\envs\\tfs\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pynvml\r\n",
    "!pip install transformers\r\n",
    "!pip install scikit-learn\r\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertPreTrainedModel, TFBertMainLayer, BertTokenizer\r\n",
    "import tensorflow as tf\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn import metrics\r\n",
    "from tqdm import tqdm\r\n",
    "import os\r\n",
    "import pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                        comment_text               label\n0  Explanation\\r\\nWhy the edits made under my use...  [0, 0, 0, 0, 0, 0]\n1  D'aww! He matches this background colour I'm s...  [0, 0, 0, 0, 0, 0]\n2  Hey man, I'm really not trying to edit war. It...  [0, 0, 0, 0, 0, 0]\n3  \"\\r\\nMore\\r\\nI can't make any real suggestions...  [0, 0, 0, 0, 0, 0]\n4  You, sir, are my hero. Any chance you remember...  [0, 0, 0, 0, 0, 0]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters\n",
    "train_path = \"../data/Toxic Comment Classification Challenge/train.csv\"\n",
    "test_path = \"../data/Toxic Comment Classification Challenge/test.csv\"\n",
    "df = pd.read_csv(train_path)[:1000]\n",
    "df['label'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['comment_text', 'label']].copy()\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1000, 2)\n",
      "TRAIN Dataset: (900, 2)\n",
      "TEST Dataset: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size=0.9\r\n",
    "test_data = pd.read_csv(test_path)[:1000]\r\n",
    "train_data = new_df.sample(frac=train_size,random_state=200)\r\n",
    "val_data = new_df.drop(train_data.index).reset_index(drop=True)\r\n",
    "\r\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\r\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\r\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_tf_utils import get_initializer\n",
    "\n",
    "class TFBertForMultilabelClassification(TFBertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFBertForMultilabelClassification, self).__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(config.num_labels,\n",
    "                                                kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                                name='classifier',\n",
    "                                                activation='sigmoid')#--------------------- sigmoid激活函数\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        return outputs  # logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customDataset(dataSet):\n",
    "    inputs = tokenizer(dataSet['comment_text'].tolist(), max_length=max_length, padding='max_length', truncation=True,\\\n",
    "                   return_tensors='tf')\n",
    "    if 'label' in dataSet.columns:\n",
    "        label_list = dataSet['label'].values.tolist() \n",
    "    else:\n",
    "        label_list = None\n",
    "    result = tf.data.Dataset.from_tensor_slices((dict((k,v) for k, v in inputs.items()), label_list))          \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory size:4294967296\n",
      "Wall time: 3.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "\r\n",
    "model_path = '../model_dirs/bert-base-uncased'\r\n",
    "\r\n",
    "# parameters\r\n",
    "batch_size = 8\r\n",
    "# pynvml.nvmlInit()\r\n",
    "# handle = pynvml.nvmlDeviceGetHandleByIndex(0)\r\n",
    "# meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\r\n",
    "# print(f'GPU Memory size:{meminfo.total}')\r\n",
    "# if meminfo.total> 1024**3*10:\r\n",
    "#     batch_size = 16 \r\n",
    "max_length = 128   \r\n",
    "learning_rate = 1e-5\r\n",
    "num_epochs = 2\r\n",
    "num_classes = 6 \r\n",
    "\r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "\r\n",
    "ds_train_encoded = customDataset(train_data).shuffle(100).batch(batch_size)\r\n",
    "ds_val_encoded = customDataset(val_data).batch(batch_size)\r\n",
    "ds_test_encoded = customDataset(test_data).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-uncased were not used when initializing TFBertForMultilabelClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForMultilabelClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForMultilabelClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForMultilabelClassification were not initialized from the model checkpoint at ../model_dirs/bert-base-uncased and are newly initialized: ['dropout_75', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[8,128,12,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tf_bert_for_multilabel_classification_1/bert/encoder/layer_._3/attention/self/transpose_3 (defined at C:\\Users\\Administrator\\miniconda3\\envs\\tfs\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:279) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_90196]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tf_bert_for_multilabel_classification_1/bert/encoder/layer_._3/attention/self/transpose_3:\n tf_bert_for_multilabel_classification_1/bert/encoder/layer_._3/attention/self/MatMul_1 (defined at C:\\Users\\Administrator\\miniconda3\\envs\\tfs\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:278)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-82202f391ec3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mbert_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_train_encoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_val_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_val_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../model_dirs/fine_tune_multiLable_model/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[8,128,12,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tf_bert_for_multilabel_classification_1/bert/encoder/layer_._3/attention/self/transpose_3 (defined at C:\\Users\\Administrator\\miniconda3\\envs\\tfs\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:279) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_90196]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tf_bert_for_multilabel_classification_1/bert/encoder/layer_._3/attention/self/transpose_3:\n tf_bert_for_multilabel_classification_1/bert/encoder/layer_._3/attention/self/MatMul_1 (defined at C:\\Users\\Administrator\\miniconda3\\envs\\tfs\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:278)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# model initialization\r\n",
    "model = TFBertForMultilabelClassification.from_pretrained(model_path, num_labels=num_classes)#------------6个标签\r\n",
    "# optimizer Adam recommended\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08, clipnorm=1)\r\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\r\n",
    "loss = tf.keras.losses.BinaryCrossentropy()#-----------------------------------binary_crossentropy 损失函数\r\n",
    "metric = tf.keras.metrics.CategoricalAccuracy()\r\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n",
    "# fit model\r\n",
    "bert_history = model.fit(ds_train_encoded, epochs= num_epochs, validation_data=ds_val_encoded)\r\n",
    "model.evaluate(ds_val_encoded)\r\n",
    "model.save_pretrained('../model_dirs/fine_tune_multiLable_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 6)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate val_set\n",
    "pred=model.predict(ds_val_encoded)[0]\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(8, 6), dtype=float32, numpy=\n array([[0.1683824 , 0.01843818, 0.05212034, 0.01702631, 0.06402564,\n         0.0249862 ],\n        [0.85332656, 0.43762764, 0.776714  , 0.4558893 , 0.7832549 ,\n         0.48621866],\n        [0.22616144, 0.02268914, 0.06788898, 0.021499  , 0.09119362,\n         0.03336108],\n        [0.23781547, 0.02197246, 0.07834281, 0.02233656, 0.0894249 ,\n         0.03426007],\n        [0.8534158 , 0.44260222, 0.7781072 , 0.45336506, 0.7842355 ,\n         0.48268858],\n        [0.1709588 , 0.01829915, 0.05629163, 0.01744768, 0.06684747,\n         0.02518667],\n        [0.59546566, 0.09101999, 0.3442278 , 0.08860752, 0.3571328 ,\n         0.15255591],\n        [0.19769281, 0.01901881, 0.0607809 , 0.01898139, 0.07185346,\n         0.02900807]], dtype=float32)>,)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(list(ds_val_encoded)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    for _, data in enumerate(ds_val_encoded, 0):\n",
    "        inputs = data[0]\n",
    "        targets = data[1]\n",
    "        outputs = model(inputs)[0]\n",
    "        fin_targets.extend(targets.numpy().tolist())\n",
    "        fin_outputs.extend(outputs.numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, targets = validation(num_epochs)\n",
    "targets = np.array(targets)\n",
    "outputs = np.array(outputs) >= 0.5\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "recall_score_micro = metrics.recall_score(targets, outputs, average='micro')\n",
    "recall_score_macro = metrics.recall_score(targets, outputs, average='macro')\n",
    "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "\n",
    "print(f\"Accuracy Score = {accuracy}\")\n",
    "print(f\"recall_score (Micro) = {recall_score_micro}\")\n",
    "print(f\"recall_score (Macro) = {recall_score_macro}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.81\n",
      "recall_score (Micro) = 0.4523809523809524\n",
      "recall_score (Macro) = 0.29814814814814816\n",
      "F1 Score (Micro) = 0.5428571428571429\n",
      "F1 Score (Macro) = 0.35813492063492064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tfs': conda)",
   "name": "python3710jvsc74a57bd0aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}