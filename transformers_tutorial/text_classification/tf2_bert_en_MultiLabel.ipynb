{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertPreTrainedModel, TFBertMainLayer, BertTokenizer\r\n",
    "import tensorflow as tf\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn import metrics\r\n",
    "from tqdm import tqdm\r\n",
    "import os\r\n",
    "import pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-13fcf0c0cf30>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                        comment_text               label\n",
       "0  Explanation\\nWhy the edits made under my usern...  [0, 0, 0, 0, 0, 0]\n",
       "1  D'aww! He matches this background colour I'm s...  [0, 0, 0, 0, 0, 0]\n",
       "2  Hey man, I'm really not trying to edit war. It...  [0, 0, 0, 0, 0, 0]\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...  [0, 0, 0, 0, 0, 0]\n",
       "4  You, sir, are my hero. Any chance you remember...  [0, 0, 0, 0, 0, 0]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# parameters\n",
    "train_path = \"../data/Toxic Comment Classification Challenge/train.csv\"\n",
    "test_path = \"../data/Toxic Comment Classification Challenge/test.csv\"\n",
    "df = pd.read_csv(train_path)[:1000]\n",
    "df['label'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['comment_text', 'label']].copy()\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FULL Dataset: (1000, 2)\nTRAIN Dataset: (900, 2)\nTEST Dataset: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size=0.9\r\n",
    "test_data = pd.read_csv(test_path)[:1000]\r\n",
    "train_data = new_df.sample(frac=train_size,random_state=200)\r\n",
    "val_data = new_df.drop(train_data.index).reset_index(drop=True)\r\n",
    "\r\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\r\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\r\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_tf_utils import get_initializer\n",
    "\n",
    "class TFBertForMultilabelClassification(TFBertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFBertForMultilabelClassification, self).__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(config.num_labels,\n",
    "                                                kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                                name='classifier',\n",
    "                                                activation='sigmoid')#--------------------- sigmoid激活函数\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        return outputs  # logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customDataset(dataSet):\n",
    "    inputs = tokenizer(dataSet['comment_text'].tolist(), max_length=max_length, padding='max_length', truncation=True,\\\n",
    "                   return_tensors='tf')\n",
    "    if 'label' in dataSet.columns:\n",
    "        label_list = dataSet['label'].values.tolist() \n",
    "    else:\n",
    "        label_list = None\n",
    "    result = tf.data.Dataset.from_tensor_slices((dict((k,v) for k, v in inputs.items()), label_list))          \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU Memory size:4236312576\n",
      "CPU times: user 2.47 s, sys: 0 ns, total: 2.47 s\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "print(f'GPU Memory size:{meminfo.total}')\n",
    "\n",
    "model_path = '../model_dirs/bert-base-uncased'\n",
    "# parameters\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "\n",
    "if meminfo.total< 1024**3*10:\n",
    "    batch_size = 8\n",
    "    max_length = 32\n",
    "    \n",
    "learning_rate = 1e-5\n",
    "num_epochs = 2\n",
    "num_classes = 6 \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "ds_train_encoded = customDataset(train_data).shuffle(100).batch(batch_size)\n",
    "ds_val_encoded = customDataset(val_data).batch(batch_size)\n",
    "ds_test_encoded = customDataset(test_data).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-uncased were not used when initializing TFBertForMultilabelClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForMultilabelClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForMultilabelClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForMultilabelClassification were not initialized from the model checkpoint at ../model_dirs/bert-base-uncased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2\n",
      "113/113 [==============================] - 46s 268ms/step - loss: 0.3853 - categorical_accuracy: 0.7682 - val_loss: 0.1887 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "113/113 [==============================] - 29s 253ms/step - loss: 0.1077 - categorical_accuracy: 0.9866 - val_loss: 0.1483 - val_categorical_accuracy: 1.0000\n",
      "13/13 [==============================] - 1s 77ms/step - loss: 0.1483 - categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# model initialization\r\n",
    "model = TFBertForMultilabelClassification.from_pretrained(model_path, num_labels=num_classes)#------------6个标签\r\n",
    "# optimizer Adam recommended\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08, clipnorm=1)\r\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\r\n",
    "loss = tf.keras.losses.BinaryCrossentropy()#-----------------------------------binary_crossentropy 损失函数\r\n",
    "metric = tf.keras.metrics.CategoricalAccuracy()\r\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n",
    "# fit model\r\n",
    "bert_history = model.fit(ds_train_encoded, epochs= num_epochs, validation_data=ds_val_encoded)\r\n",
    "model.evaluate(ds_val_encoded)\r\n",
    "model.save_pretrained('../model_dirs/fine_tune_multiLable_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# evaluate val_set\n",
    "pred=model.predict(ds_val_encoded)[0]\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8, 6), dtype=float32, numpy=\n",
       " array([[0.07364095, 0.0145124 , 0.03028663, 0.01622849, 0.03263389,\n",
       "         0.0138443 ],\n",
       "        [0.74448687, 0.21233813, 0.55207884, 0.21324778, 0.6013849 ,\n",
       "         0.23042233],\n",
       "        [0.11123867, 0.01479384, 0.03909314, 0.02033149, 0.04345132,\n",
       "         0.01602607],\n",
       "        [0.21341197, 0.01755768, 0.06432187, 0.02605347, 0.07276884,\n",
       "         0.01978366],\n",
       "        [0.76283425, 0.26266393, 0.6042745 , 0.25697786, 0.6677944 ,\n",
       "         0.28520185],\n",
       "        [0.15371932, 0.01521484, 0.05046858, 0.02234085, 0.05517004,\n",
       "         0.01707247],\n",
       "        [0.43943295, 0.03241521, 0.15149279, 0.04636146, 0.16893132,\n",
       "         0.04092854],\n",
       "        [0.22491433, 0.01801733, 0.06814891, 0.02783837, 0.07514174,\n",
       "         0.02104727]], dtype=float32)>,)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "model(list(ds_val_encoded)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    for _, data in enumerate(ds_val_encoded, 0):\n",
    "        inputs = data[0]\n",
    "        targets = data[1]\n",
    "        outputs = model(inputs)[0]\n",
    "        fin_targets.extend(targets.numpy().tolist())\n",
    "        fin_outputs.extend(outputs.numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy Score = 0.83\nrecall_score (Micro) = 0.35714285714285715\nrecall_score (Macro) = 0.22592592592592595\nF1 Score (Micro) = 0.4918032786885246\nF1 Score (Macro) = 0.2912698412698413\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = validation(num_epochs)\n",
    "targets = np.array(targets)\n",
    "outputs = np.array(outputs) >= 0.5\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "recall_score_micro = metrics.recall_score(targets, outputs, average='micro')\n",
    "recall_score_macro = metrics.recall_score(targets, outputs, average='macro')\n",
    "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "\n",
    "print(f\"Accuracy Score = {accuracy}\")\n",
    "print(f\"recall_score (Micro) = {recall_score_micro}\")\n",
    "print(f\"recall_score (Macro) = {recall_score_macro}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd001be8e6ef3397cb3b7216ce72275f144d49556b9a40c469805dd9c056d34699f",
   "display_name": "Python 3.7.10 64-bit ('py3.7': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}