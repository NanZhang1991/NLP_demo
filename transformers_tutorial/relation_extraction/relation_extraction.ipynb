{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查尔斯·阿兰基斯（Charles Aránguiz），1989年4月17日出生于智利圣地亚哥，智利职业足球运动员，司职中场，效力于德国足球甲级联赛勒沃库森足球俱乐部 [{'predicate': '出生地', 'object_type': '地点', 'subject_type': '人物', 'object': '圣地亚哥', 'subject': '查尔斯·阿兰基斯'}, {'predicate': '出生日期', 'object_type': 'Date', 'subject_type': '人物', 'object': '1989年4月17日', 'subject': '查尔斯·阿兰基斯'}]\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    text_list = []\n",
    "    spo_list = []\n",
    "    with open(path, encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            text_list.append(literal_eval(line)['text'])\n",
    "            spo_list.append(literal_eval(line)['spo_list'])\n",
    "    return text_list, spo_list\n",
    "    \n",
    "path='./data/百度关系抽取数据集/dev_data.json'\n",
    "text_list, spo_list = load_data(path)\n",
    "print(text_list[0], spo_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'修业年限': 0, '作者': 1, '改编自': 2, '官方语言': 3, '身高': 4, '邮政编码': 5, '歌手': 6, '制片人': 7, '主持人': 8, '董事长': 9, '人口数量': 10, '父亲': 11, '气候': 12, '嘉宾': 13, '主演': 14, '所属专辑': 15, '上映时间': 16, '毕业院校': 17, '祖籍': 18, '导演': 19, '海拔': 20, '面积': 21, '创始人': 22, '作曲': 23, '出生地': 24, '丈夫': 25, '目': 26, '作词': 27, '字': 28, '出品公司': 29, '首都': 30, '母亲': 31, '所在城市': 32, '朝代': 33, '妻子': 34, '总部地点': 35, '主角': 36, '成立日期': 37, '出版社': 38, '编剧': 39, '国籍': 40, '连载网站': 41, '简称': 42, '出生日期': 43, '号': 44, '专业代码': 45, '注册资本': 46, '占地面积': 47, '民族': 48}\n",
      "{0: '修业年限', 1: '作者', 2: '改编自', 3: '官方语言', 4: '身高', 5: '邮政编码', 6: '歌手', 7: '制片人', 8: '主持人', 9: '董事长', 10: '人口数量', 11: '父亲', 12: '气候', 13: '嘉宾', 14: '主演', 15: '所属专辑', 16: '上映时间', 17: '毕业院校', 18: '祖籍', 19: '导演', 20: '海拔', 21: '面积', 22: '创始人', 23: '作曲', 24: '出生地', 25: '丈夫', 26: '目', 27: '作词', 28: '字', 29: '出品公司', 30: '首都', 31: '母亲', 32: '所在城市', 33: '朝代', 34: '妻子', 35: '总部地点', 36: '主角', 37: '成立日期', 38: '出版社', 39: '编剧', 40: '国籍', 41: '连载网站', 42: '简称', 43: '出生日期', 44: '号', 45: '专业代码', 46: '注册资本', 47: '占地面积', 48: '民族'}\n"
     ]
    }
   ],
   "source": [
    "def load_predicate(path):\n",
    "    with open(path,'r', encoding='utf-8')  as f:\n",
    "        predicate_list = [literal_eval(i)['predicate'] for i in f]\n",
    "    p2id = {}\n",
    "    id2p = {}\n",
    "    data = list(set(predicate_list))\n",
    "    for i in range(len(data)):\n",
    "        p2id[data[i]] = i\n",
    "        id2p[i] = data[i]\n",
    "    return p2id, id2p\n",
    "    \n",
    "path = './data/百度关系抽取数据集/all_50_schemas'\n",
    "p2id, id2p = load_predicate(path)\n",
    "print(p2id)\n",
    "print(id2p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_data(text_list,spo_list,p2id,id2p,tokenizer,max_length):\n",
    "    id_label = {}\n",
    "    ct = len(text_list)\n",
    "    input_ids = np.zeros((ct,max_length),dtype='int32')\n",
    "    attention_mask = np.zeros((ct,max_length),dtype='int32')\n",
    "    start_tokens = np.zeros((ct,max_length),dtype='int32')\n",
    "    end_tokens = np.zeros((ct,max_length),dtype='int32')\n",
    "    send_s_po = np.zeros((ct,2),dtype='int32')\n",
    "    object_start_tokens = np.zeros((ct,max_length,len(p2id)),dtype='int32')\n",
    "    object_end_tokens = np.zeros((ct,max_length,len(p2id)),dtype='int32')\n",
    "    invalid_index = []\n",
    "    for k in range(ct):\n",
    "        context_k = text_list[k].lower().replace(' ','')\n",
    "        enc_context = tokenizer.encode(context_k,max_length=max_length,truncation=True) \n",
    "        if len(spo_list[k])==0:\n",
    "            invalid_index.append(k)\n",
    "            continue\n",
    "        start = []\n",
    "        end = []\n",
    "        S_index = []\n",
    "        for j in range(len(spo_list[k])):\n",
    "            answers_text_k = spo_list[k][j]['subject'].lower().replace(' ','')\n",
    "            chars = np.zeros((len(context_k)))\n",
    "            index = context_k.find(answers_text_k)\n",
    "            chars[index:index+len(answers_text_k)]=1\n",
    "            offsets = []\n",
    "            idx=0\n",
    "            for t in enc_context[1:]:\n",
    "                w = tokenizer.decode([t])\n",
    "                if '#' in w and len(w)>1:\n",
    "                    w = w.replace('#','')\n",
    "                if w == '[UNK]':\n",
    "                    w = '。'\n",
    "                offsets.append((idx,idx+len(w)))\n",
    "                idx += len(w)\n",
    "            toks = []\n",
    "            for i,(a,b) in enumerate(offsets):\n",
    "                sm = np.sum(chars[a:b])\n",
    "                if sm>0: \n",
    "                    toks.append(i) \n",
    "            input_ids[k,:len(enc_context)] = enc_context\n",
    "            attention_mask[k,:len(enc_context)] = 1\n",
    "            if len(toks)>0:\n",
    "                start_tokens[k,toks[0]+1] = 1\n",
    "                end_tokens[k,toks[-1]+1] = 1\n",
    "                start.append(toks[0]+1)\n",
    "                end.append(toks[-1]+1)\n",
    "                S_index.append(j)\n",
    "                #随机抽取可以作为负样本提高准确率（不认同）\n",
    "        if len(start) > 0:\n",
    "            start_np = np.array(start)\n",
    "            end_np = np.array(end)\n",
    "            start_ = np.random.choice(start_np)\n",
    "            end_ = np.random.choice(end_np[end_np >= start_])\n",
    "            send_s_po[k,0] = start_\n",
    "            send_s_po[k,1] = end_\n",
    "            s_index = start.index(start_)\n",
    "            #随机选取object的首位，如果选取错误，则作为负样本\n",
    "            if end_ == end[s_index]:\n",
    "                for index in range(len(start)):\n",
    "                    if start[index] == start_ and end[index] == end_:\n",
    "                        object_text_k = spo_list[k][S_index[index]]['object'].lower().replace(' ','')\n",
    "                        predicate = spo_list[k][S_index[index]]['predicate']\n",
    "                        p_id = p2id[predicate]\n",
    "                        chars = np.zeros((len(context_k)))\n",
    "                        index = context_k.find(object_text_k)\n",
    "                        chars[index:index+len(object_text_k)]=1\n",
    "                        offsets = [] \n",
    "                        idx=0\n",
    "                        for t in enc_context[1:]:\n",
    "                            w = tokenizer.decode([t])\n",
    "                            if '#' in w and len(w)>1:\n",
    "                                w = w.replace('#','')\n",
    "                            if w == '[UNK]':\n",
    "                                w = '。'\n",
    "                            offsets.append((idx,idx+len(w)))\n",
    "                            idx += len(w)\n",
    "                        toks = []\n",
    "                        for i,(a,b) in enumerate(offsets):\n",
    "                            sm = np.sum(chars[a:b])\n",
    "                            if sm>0: \n",
    "                                toks.append(i) \n",
    "                        if len(toks)>0:\n",
    "                            id_label[p_id] = predicate\n",
    "                            object_start_tokens[k,toks[0]+1,p_id] = 1\n",
    "                            object_end_tokens[k,toks[-1]+1,p_id] = 1\n",
    "        else:\n",
    "            invalid_index.append(k)\n",
    "    return input_ids,attention_mask,start_tokens,end_tokens,send_s_po,object_start_tokens,object_end_tokens,invalid_index,id_label\n",
    "\n",
    "max_length = 128  \n",
    "model_path = '../model_dirs/bert-base-chinese'  \n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)    \n",
    "input_ids,attention_mask, start_tokens, end_tokens, send_s_po, object_start_tokens, object_end_tokens, invalid_index, id_label \\\n",
    "= proceed_data(text_list,spo_list,p2id,id2p,tokenizer,max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 101, 3389, 2209, ...,    0,    0,    0],\n",
       "        [ 101,  517, 4895, ...,    0,    0,    0],\n",
       "        [ 101,  517, 2699, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 3636, 3727, ...,    0,    0,    0],\n",
       "        [ 101,  517, 5381, ...,    0,    0,    0],\n",
       "        [ 101,  517, 2769, ...,    0,    0,    0]]),\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def proceed_var_data(text_list,spo_list,tokenizer,max_length):\n",
    "    ct = len(text_list)\n",
    "    input_ids = np.zeros((ct,max_length),dtype='int32')\n",
    "    attention_mask = np.zeros((ct,max_length),dtype='int32')\n",
    "    for k in range(ct):\n",
    "        context_k = text_list[k].lower().replace(' ','')\n",
    "        enc_context = tokenizer.encode(context_k,max_length=max_length,truncation=True) \n",
    "        input_ids[k,:len(enc_context)] = enc_context\n",
    "        attention_mask[k,:len(enc_context)] = 1\n",
    "    return input_ids,attention_mask\n",
    "\n",
    "proceed_var_data(text_list,spo_list,tokenizer,max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"(Conditional) Layer Normalization\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        epsilon=None,\n",
    "        conditional=False,\n",
    "        hidden_units=None,\n",
    "        hidden_activation='linear',\n",
    "        hidden_initializer='glorot_uniform',\n",
    "        **kwargs):\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.conditional = conditional\n",
    "        self.hidden_units = hidden_units\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\n",
    "        self.epsilon = epsilon or 1e-12\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if self.conditional:\n",
    "            masks = mask if mask is not None else []\n",
    "            masks = [m[None] for m in masks if m is not None]\n",
    "            if len(masks) == 0:\n",
    "                return None\n",
    "            else:\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\n",
    "        else:\n",
    "            return mask\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "        if self.conditional:\n",
    "            shape = (input_shape[0][-1],)\n",
    "        else:\n",
    "            shape = (input_shape[-1],)\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape, initializer='zeros', name='beta')\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape, initializer='ones', name='gamma')\n",
    "        if self.conditional:\n",
    "            if self.hidden_units is not None:\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\n",
    "                    units=self.hidden_units,\n",
    "                    activation=self.hidden_activation,\n",
    "                    use_bias=False,\n",
    "                    kernel_initializer=self.hidden_initializer)\n",
    "            if self.center:\n",
    "                self.beta_dense = tf.keras.layers.Dense(\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\n",
    "            if self.scale:\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\n",
    "        \"\"\"\n",
    "        if self.conditional:\n",
    "            inputs, cond = inputs\n",
    "            if self.hidden_units is not None:\n",
    "                cond = self.hidden_dense(cond)\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\n",
    "                cond = K.expand_dims(cond, 1)\n",
    "            if self.center:\n",
    "                beta = self.beta_dense(cond) + self.beta\n",
    "            if self.scale:\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\n",
    "        else:\n",
    "            if self.center:\n",
    "                beta = self.beta\n",
    "            if self.scale:\n",
    "                gamma = self.gamma\n",
    "        outputs = inputs\n",
    "        if self.center:\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\n",
    "            outputs = outputs - mean\n",
    "        if self.scale:\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            outputs = outputs / std\n",
    "            outputs = outputs * gamma\n",
    "        if self.center:\n",
    "            outputs = outputs + beta\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(true,pred):\n",
    "    true = tf.cast(true,tf.float32)\n",
    "    loss = K.sum(K.binary_crossentropy(true, pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject(inputs):\n",
    "    \"\"\"根据subject_ids从output中取出subject的向量表征\n",
    "    \"\"\"\n",
    "    output, subject_ids = inputs\n",
    "    start = tf.gather(output,subject_ids[:,0],axis=1,batch_dims=0)\n",
    "    end = tf.gather(output,subject_ids[:,1],axis=1,batch_dims=0)\n",
    "    subject = tf.keras.layers.Concatenate(axis=2)([start, end])\n",
    "    return subject[:,0]\n",
    "'''\n",
    "   output.shape = (None,128,768)\n",
    "   subjudec_ids.shape = (None,2)\n",
    "   start.shape = (None,None,768)\n",
    "   subject.shape = (None,None,1536)\n",
    "   subject[:,0].shape = (None,1536)\n",
    "   这一部分给出各个变量的shape应该一目了然\n",
    "'''\n",
    "   \n",
    "def build_model_2(pretrained_path, config, MAX_LEN, p2id):\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    s_po_index =  tf.keras.layers.Input((2,), dtype=tf.int32)\n",
    "    \n",
    "    config.output_hidden_states = True\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, config=config)\n",
    "    x, _, hidden_states = bert_model(ids, attention_mask=att)\n",
    "\n",
    "    layer_1 = hidden_states[-1]\n",
    "    \n",
    "    start_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\n",
    "    start_logits = tf.keras.layers.Lambda(lambda x: x**2)(start_logits)\n",
    "    \n",
    "    end_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\n",
    "    end_logits = tf.keras.layers.Lambda(lambda x: x**2)(end_logits)\n",
    "    \n",
    "    subject_1 = extract_subject([layer_1,s_po_index])\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject_1])\n",
    "    \n",
    "    op_out_put_start = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\n",
    "    op_out_put_start = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_start)\n",
    "    \n",
    "    op_out_put_end = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\n",
    "    op_out_put_end = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_end)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[ids,att,s_po_index], outputs=[start_logits,end_logits,op_out_put_start,op_out_put_end])\n",
    "    model_2 = tf.keras.models.Model(inputs=[ids,att], outputs=[start_logits,end_logits])\n",
    "    model_3 = tf.keras.models.Model(inputs=[ids,att,s_po_index], outputs=[op_out_put_start,op_out_put_end])\n",
    "    return model,model_2,model_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,model_2,model_3,id2tag,va_spo_list,va_input_ids,va_attention_mask,tokenizer):\n",
    "        super(Metrics, self).__init__()\n",
    "        self.model_2 = model_2\n",
    "        self.model_3 = model_3\n",
    "        self.id2tag = id2tag\n",
    "        self.va_input_ids = va_input_ids\n",
    "        self.va_attention_mask = va_attention_mask\n",
    "        self.va_spo_list = va_spo_list\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.val_f1s = []\n",
    "        self.best_val_f1 = 0\n",
    "    \n",
    "    def get_same_element_index(self,ob_list):\n",
    "        return [i for (i, v) in enumerate(ob_list) if v == 1]\n",
    "    \n",
    "    def evaluate_data(self):\n",
    "        question=[]\n",
    "        answer=[]\n",
    "        Y1 = self.model_2.predict([self.va_input_ids,self.va_attention_mask])\n",
    "        for i in range(len(Y1[0])):\n",
    "            for z in self.va_spo_list[i]:\n",
    "                question.append((z['subject'][0],z['subject'][-1],z['predicate'],z['object'][0],z['object'][-1]))\n",
    "            x_ = [self.tokenizer.decode([t]) for t in self.va_input_ids[i]]\n",
    "            x1 = np.array(Y1[0][i]>0.5,dtype='int32')\n",
    "            x2 = np.array(Y1[1][i]>0.5,dtype='int32')\n",
    "            union = x1 + x2\n",
    "            index_list = self.get_same_element_index(list(union))\n",
    "            start = 0\n",
    "            S_list=[]\n",
    "            while start+1 < len(index_list):\n",
    "                S_list.append((index_list[start],index_list[start+1]+1))\n",
    "                start += 2\n",
    "            for os_s,os_e in S_list:\n",
    "                S = ''.join(x_[os_s:os_e])\n",
    "                Y2 = self.model_3.predict([[self.va_input_ids[i]],[self.va_attention_mask[i]],np.array([[os_s,os_e]])])\n",
    "                for m in range(len(self.id2tag)):\n",
    "                    x3 = np.array(Y2[0][0][:,m]>0.4,dtype='int32')\n",
    "                    x4 = np.array(Y2[1][0][:,m]>0.4,dtype='int32')\n",
    "                    if sum(x3)>0 and sum(x4)>0:\n",
    "                        predict = self.id2tag[m]\n",
    "                        union = x3 + x4\n",
    "                        index_list = self.get_same_element_index(list(union))\n",
    "                        start = 0\n",
    "                        P_list=[]\n",
    "                        while start+1 < len(index_list):\n",
    "                            P_list.append((index_list[start],index_list[start+1]+1))\n",
    "                            start += 2\n",
    "                        for os_s,os_e in P_list:\n",
    "                            if os_e>=os_s:\n",
    "                                P = ''.join(x_[os_s:os_e])\n",
    "                                answer.append((S[0],S[-1],predict,P[0],P[-1]))\n",
    "        Q = set(question)\n",
    "        S = set(answer)\n",
    "        f1 = 2*len(Q&S)/(len(Q)+len(S))\n",
    "        return f1\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        _val_f1 = self.evaluate_data()\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        logs['val_f1'] = _val_f1\n",
    "        if _val_f1 > self.best_val_f1:\n",
    "            self.model.save_weights('./model_/02_f1={}_model.hdf5'.format(_val_f1))\n",
    "            self.best_val_f1 = _val_f1\n",
    "            print(\"best f1: {}\".format(self.best_val_f1))\n",
    "        else:\n",
    "            print(\"val f1: {}, but not the best f1\".format(_val_f1))\n",
    "        return   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese/tf_model.h5 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese/tf_model.h5 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000202F7A48BA8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000202F7A48BA8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Inputs to a layer should be tensors. Got: s",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6fd20a1580d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m model.compile(loss={'lambda': new_loss,\n",
      "\u001b[1;32m<ipython-input-8-db748654d4a3>\u001b[0m in \u001b[0;36mbuild_model_2\u001b[1;34m(pretrained_path, config, MAX_LEN, p2id)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mlayer_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mstart_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mstart_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 998\u001b[1;33m       \u001b[0minput_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# have a `shape` attribute.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Inputs to a layer should be tensors. Got: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got: s"
     ]
    }
   ],
   "source": [
    "pretrained_path = '../model_dirs/bert-base-chinese/tf_model.h5'\n",
    "MAX_LEN = 128\n",
    "config = BertConfig.from_json_file('../model_dirs/bert-base-chinese/config.json')\n",
    "TFBertModel.from_pretrained(pretrained_path, config=config)\n",
    "K.clear_session()\n",
    "model,model_2,model_3 = build_model_2(pretrained_path, config, MAX_LEN, p2id)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(loss={'lambda': new_loss,\n",
    "                'lambda_1': new_loss,\n",
    "                'lambda_2': new_loss,\n",
    "                'lambda_3': new_loss},optimizer=optimizer)\n",
    "model.fit([input_ids,attention_mask,send_s_po],\\\n",
    "          [start_tokens,end_tokens,object_start_tokens,object_end_tokens], \\\n",
    "        epochs=20, batch_size=32,callbacks=[Metrics(model_2,model_3,id2tag,va_spo_list,va_input_ids,va_attention_mask,tokenizer)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfs]",
   "language": "python",
   "name": "conda-env-tfs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
