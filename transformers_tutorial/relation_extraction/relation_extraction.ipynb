{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0m\u001b[01;32mrelation_extraction2.ipynb\u001b[0m*  \u001b[01;32mrelation_extraction.ipynb\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "查尔斯·阿兰基斯（Charles Aránguiz），1989年4月17日出生于智利圣地亚哥，智利职业足球运动员，司职中场，效力于德国足球甲级联赛勒沃库森足球俱乐部 [{'predicate': '出生地', 'object_type': '地点', 'subject_type': '人物', 'object': '圣地亚哥', 'subject': '查尔斯·阿兰基斯'}, {'predicate': '出生日期', 'object_type': 'Date', 'subject_type': '人物', 'object': '1989年4月17日', 'subject': '查尔斯·阿兰基斯'}]\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    text_list = []\n",
    "    spo_list = []\n",
    "    with open(path, encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            text_list.append(literal_eval(line)['text'])\n",
    "            spo_list.append(literal_eval(line)['spo_list'])\n",
    "    return text_list, spo_list\n",
    "    \n",
    "# path='../data百度关系抽取数据集/train_data.json'\n",
    "path='../data/百度关系抽取数据集/experiment.json'\n",
    "text_list, spo_list = load_data(path)\n",
    "print(text_list[0], spo_list[0])\n",
    "# val_path = '../data/百度关系抽取数据集/dev_data.json'\n",
    "val_path = '../data/百度关系抽取数据集/experiment.json'\n",
    "val_text_list, val_spo_list = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'所属专辑': 0, '出生地': 1, '字': 2, '作者': 3, '作曲': 4, '创始人': 5, '身高': 6, '首都': 7, '气候': 8, '嘉宾': 9, '出生日期': 10, '总部地点': 11, '成立日期': 12, '连载网站': 13, '歌手': 14, '作词': 15, '号': 16, '面积': 17, '注册资本': 18, '占地面积': 19, '人口数量': 20, '父亲': 21, '编剧': 22, '导演': 23, '国籍': 24, '民族': 25, '出品公司': 26, '朝代': 27, '主角': 28, '上映时间': 29, '妻子': 30, '海拔': 31, '董事长': 32, '官方语言': 33, '主演': 34, '专业代码': 35, '制片人': 36, '所在城市': 37, '祖籍': 38, '目': 39, '修业年限': 40, '改编自': 41, '主持人': 42, '出版社': 43, '毕业院校': 44, '邮政编码': 45, '母亲': 46, '丈夫': 47, '简称': 48}\n{0: '所属专辑', 1: '出生地', 2: '字', 3: '作者', 4: '作曲', 5: '创始人', 6: '身高', 7: '首都', 8: '气候', 9: '嘉宾', 10: '出生日期', 11: '总部地点', 12: '成立日期', 13: '连载网站', 14: '歌手', 15: '作词', 16: '号', 17: '面积', 18: '注册资本', 19: '占地面积', 20: '人口数量', 21: '父亲', 22: '编剧', 23: '导演', 24: '国籍', 25: '民族', 26: '出品公司', 27: '朝代', 28: '主角', 29: '上映时间', 30: '妻子', 31: '海拔', 32: '董事长', 33: '官方语言', 34: '主演', 35: '专业代码', 36: '制片人', 37: '所在城市', 38: '祖籍', 39: '目', 40: '修业年限', 41: '改编自', 42: '主持人', 43: '出版社', 44: '毕业院校', 45: '邮政编码', 46: '母亲', 47: '丈夫', 48: '简称'}\n"
     ]
    }
   ],
   "source": [
    "def load_predicate(path):\n",
    "    with open(path,'r', encoding='utf-8')  as f:\n",
    "        predicate_list = [literal_eval(i)['predicate'] for i in f]\n",
    "    p2id = {}\n",
    "    id2p = {}\n",
    "    data = list(set(predicate_list))\n",
    "    for i in range(len(data)):\n",
    "        p2id[data[i]] = i\n",
    "        id2p[i] = data[i]\n",
    "    return p2id, id2p\n",
    "    \n",
    "path = '../data/百度关系抽取数据集/all_50_schemas'\n",
    "p2id, id2p = load_predicate(path)\n",
    "print(p2id)\n",
    "print(id2p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_data(text_list,spo_list,p2id,id2p,tokenizer,max_length):\n",
    "    id_label = {}\n",
    "    ct = len(text_list)\n",
    "    input_ids = np.zeros((ct,max_length),dtype='int32')\n",
    "    attention_mask = np.zeros((ct,max_length),dtype='int32')\n",
    "    start_tokens = np.zeros((ct,max_length),dtype='int32')\n",
    "    end_tokens = np.zeros((ct,max_length),dtype='int32')\n",
    "    send_s_po = np.zeros((ct,2),dtype='int32')\n",
    "    object_start_tokens = np.zeros((ct,max_length,len(p2id)),dtype='int32')\n",
    "    object_end_tokens = np.zeros((ct,max_length,len(p2id)),dtype='int32')\n",
    "    invalid_index = []\n",
    "    for k in range(ct):\n",
    "        context_k = text_list[k].lower().replace(' ','')\n",
    "        enc_context = tokenizer.encode(context_k,max_length=max_length,padding='max_length',truncation=True) \n",
    "        if len(spo_list[k])==0:\n",
    "            invalid_index.append(k)\n",
    "            continue\n",
    "        start = []\n",
    "        end = []\n",
    "        S_index = []\n",
    "        for j in range(len(spo_list[k])):\n",
    "            answers_text_k = spo_list[k][j]['subject'].lower().replace(' ','')\n",
    "            chars = np.zeros((len(context_k)))\n",
    "            index = context_k.find(answers_text_k)\n",
    "            chars[index:index+len(answers_text_k)]=1\n",
    "            offsets = []\n",
    "            idx=0\n",
    "            for t in enc_context[1:]:\n",
    "                w = tokenizer.decode([t])\n",
    "                if '#' in w and len(w)>1:\n",
    "                    w = w.replace('#','')\n",
    "                if w == '[UNK]':\n",
    "                    w = '。'\n",
    "                offsets.append((idx,idx+len(w)))\n",
    "                \n",
    "                idx += len(w)\n",
    "            toks = []\n",
    "            for i,(a,b) in enumerate(offsets):\n",
    "                sm = np.sum(chars[a:b])\n",
    "                if sm>0: \n",
    "                    toks.append(i) \n",
    "            input_ids[k,:len(enc_context)] = enc_context\n",
    "            attention_mask[k,:len(enc_context)] = 1\n",
    "            if len(toks)>0:\n",
    "                start_tokens[k,toks[0]+1] = 1\n",
    "                end_tokens[k,toks[-1]+1] = 1\n",
    "                start.append(toks[0]+1)\n",
    "                end.append(toks[-1]+1)\n",
    "                S_index.append(j)\n",
    "                #随机抽取可以作为负样本提高准确率（不认同）\n",
    "        if len(start) > 0:\n",
    "            start_np = np.array(start)\n",
    "            end_np = np.array(end)\n",
    "            start_ = np.random.choice(start_np)\n",
    "            end_ = np.random.choice(end_np[end_np >= start_])\n",
    "            send_s_po[k,0] = start_\n",
    "            send_s_po[k,1] = end_\n",
    "            s_index = start.index(start_)\n",
    "            #随机选取object的首位，如果选取错误，则作为负样本\n",
    "            if end_ == end[s_index]:\n",
    "                for index in range(len(start)):\n",
    "                    if start[index] == start_ and end[index] == end_:\n",
    "                        object_text_k = spo_list[k][S_index[index]]['object'].lower().replace(' ','')\n",
    "                        predicate = spo_list[k][S_index[index]]['predicate']\n",
    "                        p_id = p2id[predicate]\n",
    "                        chars = np.zeros((len(context_k)))\n",
    "                        index = context_k.find(object_text_k)\n",
    "                        chars[index:index+len(object_text_k)]=1\n",
    "                        offsets = [] \n",
    "                        idx=0\n",
    "                        for t in enc_context[1:]:\n",
    "                            w = tokenizer.decode([t])\n",
    "                            if '#' in w and len(w)>1:\n",
    "                                w = w.replace('#','')\n",
    "                            if w == '[UNK]':\n",
    "                                w = '。'\n",
    "                            offsets.append((idx,idx+len(w)))\n",
    "                            idx += len(w)\n",
    "                        toks = []\n",
    "                        for i,(a,b) in enumerate(offsets):\n",
    "                            sm = np.sum(chars[a:b])\n",
    "                            if sm>0: \n",
    "                                toks.append(i) \n",
    "                        if len(toks)>0:\n",
    "                            id_label[p_id] = predicate\n",
    "                            object_start_tokens[k,toks[0]+1,p_id] = 1\n",
    "                            object_end_tokens[k,toks[-1]+1,p_id] = 1\n",
    "        else:\n",
    "            invalid_index.append(k)\n",
    "    input_ids = tf.constant(input_ids)\n",
    "    attention_mask = tf.constant(attention_mask)\n",
    "    start_tokens = tf.constant(start_tokens)\n",
    "    end_tokens = tf.constant(end_tokens)\n",
    "    send_s_po = tf.constant(send_s_po)\n",
    "    object_start_tokens = tf.constant(object_start_tokens)\n",
    "    object_end_tokens = tf.constant(object_end_tokens)\n",
    "    # invalid_index = tf.constant(invalid_index)\n",
    "    return input_ids, attention_mask, start_tokens, end_tokens, send_s_po, object_start_tokens, object_end_tokens, invalid_index, id_label\n",
    "\n",
    "max_length = 128  \n",
    "model_path = '../model_dirs/bert-base-chinese'  \n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)    \n",
    "input_ids, attention_mask, start_tokens, end_tokens, send_s_po, object_start_tokens, object_end_tokens, invalid_index, id_label \\\n",
    "= proceed_data(text_list,spo_list,p2id,id2p,tokenizer,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs = tokenizer(text_list, max_length=max_length, padding='max_length', truncation=True, return_tensors='tf') \r\n",
    "val_input_ids, val_attention_mask = val_inputs['input_ids'], val_inputs['attention_mask']\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\r\n",
    "    \"\"\"(Conditional) Layer Normalization\r\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        center=True,\r\n",
    "        scale=True,\r\n",
    "        epsilon=None,\r\n",
    "        conditional=False,\r\n",
    "        hidden_units=None,\r\n",
    "        hidden_activation='linear',\r\n",
    "        hidden_initializer='glorot_uniform',\r\n",
    "        **kwargs):\r\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\r\n",
    "        self.center = center\r\n",
    "        self.scale = scale\r\n",
    "        self.conditional = conditional\r\n",
    "        self.hidden_units = hidden_units\r\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\r\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\r\n",
    "        self.epsilon = epsilon or 1e-12\r\n",
    "        \r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        if self.conditional:\r\n",
    "            masks = mask if mask is not None else []\r\n",
    "            masks = [m[None] for m in masks if m is not None]\r\n",
    "            if len(masks) == 0:\r\n",
    "                return None\r\n",
    "            else:\r\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\r\n",
    "        else:\r\n",
    "            return mask\r\n",
    "        \r\n",
    "    def build(self, input_shape):\r\n",
    "        super(LayerNormalization, self).build(input_shape)\r\n",
    "        if self.conditional:\r\n",
    "            shape = (input_shape[0][-1],)\r\n",
    "        else:\r\n",
    "            shape = (input_shape[-1],)\r\n",
    "        if self.center:\r\n",
    "            self.beta = self.add_weight(\r\n",
    "                shape=shape, initializer='zeros', name='beta')\r\n",
    "        if self.scale:\r\n",
    "            self.gamma = self.add_weight(\r\n",
    "                shape=shape, initializer='ones', name='gamma')\r\n",
    "        if self.conditional:\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\r\n",
    "                    units=self.hidden_units,\r\n",
    "                    activation=self.hidden_activation,\r\n",
    "                    use_bias=False,\r\n",
    "                    kernel_initializer=self.hidden_initializer)\r\n",
    "            if self.center:\r\n",
    "                self.beta_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "            if self.scale:\r\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\r\n",
    "        \"\"\"\r\n",
    "        if self.conditional:\r\n",
    "            inputs, cond = inputs\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                cond = self.hidden_dense(cond)\r\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\r\n",
    "                cond = K.expand_dims(cond, 1)\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta_dense(cond) + self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\r\n",
    "        else:\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma\r\n",
    "        outputs = inputs\r\n",
    "        if self.center:\r\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\r\n",
    "            outputs = outputs - mean\r\n",
    "        if self.scale:\r\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\r\n",
    "            std = K.sqrt(variance + self.epsilon)\r\n",
    "            outputs = outputs / std\r\n",
    "            outputs = outputs * gamma\r\n",
    "        if self.center:\r\n",
    "            outputs = outputs + beta\r\n",
    "        return outputs\r\n",
    "        \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(true,pred):\n",
    "    true = tf.cast(true,tf.float32)\n",
    "    loss = K.sum(K.binary_crossentropy(true, pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject(inputs):\r\n",
    "    \"\"\"根据subject_ids从output中取出subject的向量表征\r\n",
    "    \"\"\"\r\n",
    "    output, subject_ids = inputs\r\n",
    "    start = tf.gather(output,subject_ids[:,0],axis=1,batch_dims=0)\r\n",
    "    end = tf.gather(output,subject_ids[:,1],axis=1,batch_dims=0)\r\n",
    "    subject = tf.keras.layers.Concatenate(axis=2)([start, end])\r\n",
    "    return subject[:,0]\r\n",
    "'''\r\n",
    "   output.shape = (None,128,768)\r\n",
    "   subjudec_ids.shape = (None,2)\r\n",
    "   start.shape = (None,None,768)\r\n",
    "   subject.shape = (None,None,1536)\r\n",
    "   subject[:,0].shape = (None,1536)\r\n",
    "   这一部分给出各个变量的shape应该一目了然\r\n",
    "'''\r\n",
    "   \r\n",
    "def build_model_2(pretrained_path, MAX_LEN, p2id):\r\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    s_po_index =  tf.keras.layers.Input((2,), dtype=tf.int32)\r\n",
    "    \r\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, output_hidden_states=True)\r\n",
    "    outputs = bert_model(ids, attention_mask=att)\r\n",
    "    x, _, hidden_states  = outputs[:3]\r\n",
    "    layer_1 = hidden_states[-1]\r\n",
    "    start_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    start_logits = tf.keras.layers.Lambda(lambda x: x**2)(start_logits)\r\n",
    "    \r\n",
    "    end_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    end_logits = tf.keras.layers.Lambda(lambda x: x**2)(end_logits)\r\n",
    "    \r\n",
    "    subject_1 = extract_subject([layer_1,s_po_index])\r\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject_1])\r\n",
    "    \r\n",
    "    op_out_put_start = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_start = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_start)\r\n",
    "    \r\n",
    "    op_out_put_end = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_end = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_end)\r\n",
    "    \r\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[start_logits, end_logits, op_out_put_start, op_out_put_end])\r\n",
    "    model_2 = tf.keras.models.Model(inputs=[ids, att], outputs=[start_logits,end_logits])\r\n",
    "    model_3 = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[op_out_put_start, op_out_put_end])\r\n",
    "    return model, model_2, model_3\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model_2, model_3, id2tag, val_spo_list, val_input_ids, val_attention_mask, tokenizer):\n",
    "        super(Metrics, self).__init__()\n",
    "        self.model_2 = model_2\n",
    "        self.model_3 = model_3\n",
    "        self.id2tag = id2tag\n",
    "        self.val_input_ids = val_input_ids\n",
    "        self.val_attention_mask = val_attention_mask\n",
    "        self.val_spo_list = val_spo_list\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.val_f1s = []\n",
    "        self.best_val_f1 = 0\n",
    "    \n",
    "    def get_same_element_index(self,ob_list):\n",
    "        return [i for (i, v) in enumerate(ob_list) if v == 1]\n",
    "    \n",
    "    def evaluate_data(self):\n",
    "        question=[]\n",
    "        answer=[]\n",
    "        y1 = self.model_2.predict([self.val_input_ids,self.val_attention_mask])\n",
    "        for i in range(len(y1[0])):\n",
    "            for z in self.val_spo_list[i]:\n",
    "                question.append((z['subject'][0],z['subject'][-1],z['predicate'],z['object'][0],z['object'][-1]))\n",
    "            x_ = [self.tokenizer.decode([t]) for t in self.val_input_ids[i]]\n",
    "            x1 = np.array(y1[0][i]>0.5,dtype='int32')\n",
    "            x2 = np.array(y1[1][i]>0.5,dtype='int32')\n",
    "            union = x1 + x2\n",
    "            index_list = self.get_same_element_index(list(union))\n",
    "            start = 0\n",
    "            S_list=[]\n",
    "            while start+1 < len(index_list):\n",
    "                S_list.append((index_list[start], index_list[start+1]+1))\n",
    "                start += 2\n",
    "            for os_s,os_e in S_list:\n",
    "                S = ''.join(x_[os_s:os_e])\n",
    "                s_e = tf.constant(np.array([[os_s,os_e]]))\n",
    "                Y2 = self.model_3.predict([self.val_input_ids[i:i+1], self.val_attention_mask[i:i+1], s_e]) \n",
    "                for m in range(len(self.id2tag)):\n",
    "                    x3 = np.array(Y2[0][0][:,m]>0.4,dtype='int32')\n",
    "                    x4 = np.array(Y2[1][0][:,m]>0.4,dtype='int32')\n",
    "                    if sum(x3)>0 and sum(x4)>0:\n",
    "                        predict = self.id2tag[m]\n",
    "                        union = x3 + x4\n",
    "                        index_list = self.get_same_element_index(list(union))\n",
    "                        start = 0\n",
    "                        P_list=[]\n",
    "                        while start+1 < len(index_list):\n",
    "                            P_list.append((index_list[start],index_list[start+1]+1))\n",
    "                            start += 2\n",
    "                        for os_s,os_e in P_list:\n",
    "                            if os_e>=os_s:\n",
    "                                P = ''.join(x_[os_s:os_e])\n",
    "                                answer.append((S[0],S[-1],predict,P[0],P[-1]))\n",
    "        Q = set(question)\n",
    "        S = set(answer)\n",
    "        f1 = 2*len(Q&S)/(len(Q)+len(S))\n",
    "        return f1\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        _val_f1 = self.evaluate_data()\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        logs['val_f1'] = _val_f1\n",
    "        if _val_f1 > self.best_val_f1:\n",
    "            self.model.save_weights(f'../model_dirs/fine_tune_relation_extraction/{_val_f1}_tf_model.h5')\n",
    "            self.best_val_f1 = _val_f1\n",
    "            print(\"best f1: {}\".format(self.best_val_f1))\n",
    "        else:\n",
    "            self.model.save_weights(f'../model_dirs/fine_tune_relation_extraction/{_val_f1}_tf_model.h5')\n",
    "            print(\"val f1: {}, but not the best f1\".format(_val_f1))\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f04012356e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f04012356e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "1/1 [==============================] - 13s 13s/step - loss: 18497.1992 - lambda_loss: 152.3401 - lambda_1_loss: 600.6039 - lambda_2_loss: 8742.5820 - lambda_3_loss: 9001.6719\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NotImplementedError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ab0c1e214c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m model.fit([input_ids, attention_mask, send_s_po],\\\n\u001b[1;32m     12\u001b[0m           \u001b[0;34m[\u001b[0m\u001b[0mstart_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobject_start_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobject_end_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         epochs=3, batch_size=8, callbacks=[Metrics(model_2, model_3 ,id2p, val_spo_list,val_input_ids,val_attention_mask,tokenizer)])\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-16d7a86daad5>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best f1: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_val_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../model_dirs/fine_tune_relation_extraction/{_val_f1}_tf_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val f1: {}, but not the best f1\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_val_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 2002\u001b[0;31m                     signatures, options, save_traces)\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    152\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    153\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 154\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    155\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mmodel_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   metadata = dict(\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_network_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m     \u001b[0mlayer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialize_layer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     \u001b[0mlayer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0mlayer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inbound_nodes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    248\u001b[0m         return serialize_keras_class_and_config(\n\u001b[1;32m    249\u001b[0m             name, {_LAYER_UNDEFINED_CONFIG_KEY: True})\n\u001b[0;32m--> 250\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mserialization_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_SKIP_FAILED_SERIALIZATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2254\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "# config = BertConfig.from_json_file('../model_dirs/bert-base-chinese/config.json')\n",
    "# TFBertModel.from_pretrained(pretrained_path, config=config)\n",
    "K.clear_session()\n",
    "model,model_2,model_3 = build_model_2(model_path,  MAX_LEN, p2id)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(loss={'lambda': new_loss,\n",
    "                'lambda_1': new_loss,\n",
    "                'lambda_2': new_loss,\n",
    "                'lambda_3': new_loss},optimizer=optimizer)\n",
    "model.fit([input_ids, attention_mask, send_s_po],\\\n",
    "          [start_tokens,end_tokens,object_start_tokens,object_end_tokens], \\\n",
    "        epochs=3, batch_size=8, callbacks=[Metrics(model_2, model_3 ,id2p, val_spo_list,val_input_ids,val_attention_mask,tokenizer)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \r\n",
    "pid = os.getpid()\r\n",
    "!kill -9 $pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd001be8e6ef3397cb3b7216ce72275f144d49556b9a40c469805dd9c056d34699f",
   "display_name": "Python 3.7.10 64-bit ('py3.7': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}