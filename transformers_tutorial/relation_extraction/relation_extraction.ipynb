{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\r\n",
    "from random import choice\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集整理\r\n",
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(path, nrows=False):\r\n",
    "    if nrows:\r\n",
    "        df = pd.read_json(path, nrows=nrows, lines=True)\r\n",
    "    else:\r\n",
    "        df = pd.read_json(path, lines=True)\r\n",
    "    df = df[['text', 'spo_list']]\r\n",
    "    return df\r\n",
    "\r\n",
    "def merge_df(dir_path):\r\n",
    "    total_df = pd.DataFrame()\r\n",
    "    for fn in os.listdir(dir_path):\r\n",
    "        df = json_to_df(os.path.join(dir_path, fn))\r\n",
    "        df_fn = fn[:fn.rfind('.')]\r\n",
    "        df.insert(0, 'fn', df_fn)\r\n",
    "        total_df =  total_df.append(df)\r\n",
    "    total_df.reset_index(drop=True, inplace=True)\r\n",
    "    print(f'data size: {total_df.shape}') #\r\n",
    "    print(f'data sample: {df.sample(5)}')\r\n",
    "    return total_df   \r\n",
    "\r\n",
    "def read_schemads(path_or_df):\r\n",
    "    if not isinstance(path_or_df, pd.DataFrame):\r\n",
    "        print(1)\r\n",
    "        schemads_path = path_or_df\r\n",
    "        predicate_data = pd.read_json(schemads_path, lines=True)\r\n",
    "        id2p = predicate_data['predicate'].drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    else:\r\n",
    "        df = path_or_df\r\n",
    "        id2p = df['spo_list'].apply(lambda spo_list: [spo['predicate'] for spo in spo_list])\r\n",
    "        id2p = id2p.explode().drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    p2id = dict(zip(id2p.values(), id2p.keys()))\r\n",
    "    print(f'length of p2id :{len(p2id)}')#\r\n",
    "    print(f'random p2id sample:{random.sample(p2id.items(), 5)}')#\r\n",
    "    return id2p, p2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\r\n",
    "# # 百度三元组关系数据集\r\n",
    "# train_path ='../data/百度关系抽取数据集/train_data.json'\r\n",
    "# train_data = json_to_df(train_path, nrows=10000)\r\n",
    "# print(f'Train data size: {train_data.shape}') #\r\n",
    "\r\n",
    "# dev_path = '../data/百度关系抽取数据集/dev_data.json'\r\n",
    "# dev_data = json_to_df(dev_path, nrows=5000)\r\n",
    "# print(f'Validation data size: {dev_data.shape}') \r\n",
    "\r\n",
    "# schemads_path = '../data/百度关系抽取数据集/all_50_schemas'\r\n",
    "# id2p, p2id = read_schemads(schemads_path)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: (10698, 3)\n",
      "data sample:                    fn                                               text  \\\n",
      "41   阿科力首次公开发行股票招股说明书  2017年1-6月，公司光学材料产品境外销售收入为2,411.01万元，主要是因为公司的光学...   \n",
      "118  阿科力首次公开发行股票招股说明书  2017年6月末应收账款变动的原因与2016年6月末应收账款余额的变动原因一致。2016年6...   \n",
      "24   阿科力首次公开发行股票招股说明书  2014年末、2015年末、2016年末和2017年6月末，公司流动资产分别为7,402.3...   \n",
      "55   阿科力首次公开发行股票招股说明书  2013年6月18日，致同会计师事务所（特殊普通合伙）出具了致同验字（2013）第110ZC...   \n",
      "81   阿科力首次公开发行股票招股说明书  2016年末，公司长期借款余额为3,686.00万元，主要原因进行了固定资产建设项目贷款，使...   \n",
      "\n",
      "                                              spo_list  \n",
      "41   [{'predicate': '营业收入', 'object_type': '金额', 's...  \n",
      "118  [{'predicate': '长期应收款', 'object_type': '金额', '...  \n",
      "24   [{'predicate': '流动资产', 'object_type': '金额', 's...  \n",
      "55   [{'predicate': '出具日期', 'object_type': '日期', 's...  \n",
      "81   [{'predicate': '长期借款', 'object_type': '金额', 's...  \n",
      "length of p2id :116\n",
      "random p2id sample:[('持有5%以上股份的主要股东', 48), ('营业外收入', 32), ('本科', 12), ('信用减值损失', 28), ('流动负债', 75)]\n"
     ]
    }
   ],
   "source": [
    "## 招股说明书三元组数据集\r\n",
    "dir_path = '../data/招股说明书三元组数据集'\r\n",
    "df = merge_df(dir_path)\r\n",
    "id2p, p2id = read_schemads(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spo(spo_list):\r\n",
    "    for spo in spo_list:\r\n",
    "        spo['predicate'] = spo['predicate'].lower()\r\n",
    "        spo['subject'] = spo['subject'].lower()\r\n",
    "        spo['object'] = spo['object'].lower()\r\n",
    "    return spo_list\r\n",
    "\r\n",
    "def data_clean(df):\r\n",
    "    df.dropna(how='any', inplace=True)\r\n",
    "    df = df[df['spo_list'].apply(lambda x: len(x)>0)]\r\n",
    "    df.drop_duplicates(subset=['text'], inplace=True)\r\n",
    "    df.reset_index(drop=True, inplace=True)\r\n",
    "    df['text'] = df['text'].str.lower()\r\n",
    "    df['spo_list'] = df['spo_list'].apply(clean_spo)\r\n",
    "    print(f'Real data size is {df.shape[0]}')\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real data size is 8436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "ipykernel_launcher:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "ipykernel_launcher:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "df = data_clean(df)\r\n",
    "# train_data = data_clean(train_data)\r\n",
    "# dev_data = data_clean(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (7592, 3)\n",
      "Validation data size: (844, 3)\n",
      "spo_count 22035\n"
     ]
    }
   ],
   "source": [
    "train_size=0.9\r\n",
    "\r\n",
    "train_data = df.sample(frac=train_size,random_state=200)\r\n",
    "dev_data = df.drop(train_data.index)\r\n",
    "dev_data.reset_index(drop=True, inplace=True)\r\n",
    "train_data.reset_index(drop=True, inplace=True)\r\n",
    "print(f'Train data size: {train_data.shape}') #\r\n",
    "print(f'Validation data size: {dev_data.shape}') \r\n",
    "\r\n",
    "spo_single_count = df['spo_list'].apply(lambda x: len(x))\r\n",
    "spo_count = spo_single_count.sum()\r\n",
    "print('spo_count', spo_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spo_count 24451\n"
     ]
    }
   ],
   "source": [
    "train_text = train_data['text'].to_list()\r\n",
    "train_spo = train_data['spo_list'].to_list()\r\n",
    "                                                \r\n",
    "dev_text = dev_data['text'].to_list()\r\n",
    "dev_spo = dev_data['spo_list'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签集分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "dataset_name = '招股说明书'\r\n",
    "spo = df['spo_list'].explode().reset_index(drop=True)\r\n",
    "spo_group = spo.apply(lambda x: '-'.join([x['predicate'] , x['subject_type'], x['object_type']]))\r\n",
    "print(spo_group.head())\r\n",
    "spo_group_count = spo_group.groupby(spo_group).count()\r\n",
    "spo_group_count.to_csv('../data/' + dataset_name + '_spo_group_count.csv', encoding='utf_8_sig')\r\n",
    "spo_group_count.plot(kind='bar', figsize=(30,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_head_idx(pattern, sequence):\r\n",
    "    \"\"\"从sequence中寻找子串pattern\r\n",
    "    如果找到，返回第一个下标；否则返回-1。\r\n",
    "    \"\"\"\r\n",
    "    n = len(pattern)\r\n",
    "    for i in range(len(sequence)):\r\n",
    "        if sequence[i:i + n] == pattern:\r\n",
    "            return i\r\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(df, p2id, tokenizer, max_length):\r\n",
    "    long_text_count = 0\r\n",
    "    total_token_ids, total_token_type_ids, total_attention_mask = [], [], []\r\n",
    "    total_subject_labels, total_subject_ids, total_object_labels = [], [], []\r\n",
    "    for idx in df.index:\r\n",
    "        row = df.loc[idx]\r\n",
    "        inputs = tokenizer(row['text'], max_length=max_length, padding='max_length', truncation=True)\r\n",
    "        token_ids, token_type_ids, attention_mask = inputs['input_ids'], inputs['token_type_ids'], inputs['attention_mask']\r\n",
    "        # 实体关系token id 位置字典\r\n",
    "        s2op_map = {}\r\n",
    "        for sop_n, spo in enumerate(row['spo_list']):\r\n",
    "            sub_ids = tokenizer.encode(spo['subject'])[1:-1]\r\n",
    "            p_id = p2id[spo['predicate']]\r\n",
    "            obj_ids = tokenizer.encode(spo['object'])[1:-1]\r\n",
    "            # 查找subject 和 object对应的token id 起始索引\r\n",
    "            sub_head_idx = find_head_idx(sub_ids, inputs['input_ids'])\r\n",
    "            obj_head_idx = find_head_idx(obj_ids, inputs['input_ids'])\r\n",
    "            if sub_head_idx != -1 and obj_head_idx != -1:\r\n",
    "                # 获取subject 起始位置的索引和结束位置索引的元组\r\n",
    "                sub = (sub_head_idx, sub_head_idx + len(sub_ids) - 1)\r\n",
    "                # 获取object 起始位置的索引和结束位置索引元组以及与关系标签id的元组对\r\n",
    "                obj = (obj_head_idx, obj_head_idx + len(obj_ids) - 1, p_id)\r\n",
    "                # print('---- sub -----', sub)#    \r\n",
    "                if sub not in s2op_map:\r\n",
    "                    s2op_map[sub] = []\r\n",
    "                s2op_map[sub].append(obj)\r\n",
    "                # print('-----------', s2op_map)#                \r\n",
    "                {(22,23):[(28, 29, 7), (25, 26, 8)]}  \r\n",
    "            else:\r\n",
    "                long_text_count +=1\r\n",
    "                # print('--idx--', idx, '--text--', row['text'])\r\n",
    "                # print('--sop_n--', sop_n, '--spo--', spo, '\\n')\r\n",
    "\r\n",
    "\r\n",
    "        if s2op_map:\r\n",
    "        # subject标签\r\n",
    "            subject_labels = np.zeros((max_length, 2))\r\n",
    "            for s in s2op_map:\r\n",
    "                #sub_head\r\n",
    "                subject_labels[s[0], 0] = 1\r\n",
    "                #sub_tail\r\n",
    "                subject_labels[s[1], 1] = 1\r\n",
    "            # 随机选一个subject\r\n",
    "            sub_head, sub_tail = choice(list(s2op_map.keys()))\r\n",
    "            subject_ids = (sub_head, sub_tail)\r\n",
    "            # sub_head, sub_tail = np.array(list(s2op_map.keys())).T\r\n",
    "            # sub_head = np.random.choice(sub_head)\r\n",
    "            # sub_tail = np.random.choice(sub_tail[sub_tail >= sub_head])\r\n",
    "            # 对应的object标签\r\n",
    "            object_labels = np.zeros((len(token_ids), len(p2id), 2))\r\n",
    "            for op in s2op_map.get((sub_head, sub_tail), []):\r\n",
    "                # print(op)\r\n",
    "                # obj_head\r\n",
    "                object_labels[op[0], op[2], 0] = 1\r\n",
    "                # obj_tail\r\n",
    "                object_labels[op[1], op[2], 1] = 1\r\n",
    "\r\n",
    "            # 所有数据汇总\r\n",
    "            total_token_ids.append(token_ids)\r\n",
    "            total_token_type_ids.append(token_type_ids)\r\n",
    "            total_attention_mask.append(attention_mask)\r\n",
    "            total_subject_labels.append(subject_labels)\r\n",
    "            total_subject_ids.append(subject_ids)\r\n",
    "            total_object_labels.append(object_labels)  \r\n",
    "    print('long_text_count', long_text_count)                      \r\n",
    "    return total_token_ids, total_token_type_ids, total_attention_mask, \\\r\n",
    "           total_subject_labels, total_subject_ids, total_object_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_demo = data_process(train_data[3:4], p2id, tokenizer, max_length)\r\n",
    "# print(np.array(input_demo[5]).shape)\r\n",
    "# print(train_spo[3])\r\n",
    "# print(np.array(input_demo[5])[0, 25, 14, 0], np.array(input_demo[5])[0, 27, 14, 1])\r\n",
    "# print(np.array(input_demo[5])[0, 14, 17, 0], np.array(input_demo[5])[0, 14, 17, 1])\r\n",
    "# # tokenizer.decode(train_input_ids[3][op_s:op_e+1])\r\n",
    "# print(train_spo[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_text_count 181\n",
      "[(48, 49), (1, 3)]\n",
      "(7521, 512, 2)\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "model_path = '../model_dirs/bert-base-chinese'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "max_length = 512\r\n",
    "train_input_ids, train_token_type_ids, train_attention_mask, train_subject_labels, train_subject_ids, train_object_labels  = data_process(train_data, p2id, tokenizer, max_length)\r\n",
    "print(train_subject_ids[:2])\r\n",
    "print(np.array(train_subject_labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "val_inputs = tokenizer(dev_text, max_length=max_length, padding='max_length', truncation=True, return_tensors='tf') \r\n",
    "val_input_ids, val_attention_mask = val_inputs['input_ids'], val_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 2009年12月，经重庆市国资委批准，重钢集团发起设立发行人前身重钢环保。重钢环保成立后，为优化管理架构、做大做强环保产业，将重钢环保打造为下属环保产业的投资、管理平台，重钢集团对其所属环保产业进行了重组，将其所持有的重庆丰盛100%的股权、三峰科技60%的股权、三峰卡万塔60%的股权、成都九江51%的股权和重庆同兴9.9%的股权以股权增资的方式注入重钢环保。\n",
      "重钢集团 重钢环保\n",
      "[101, 8170, 2399, 8110, 3299, 8024, 5307, 7028, 2412, 2356, 1744, 6598, 1999, 2821, 1114, 8024, 7028, 7167, 7415, 1730, 1355, 6629, 6392, 4989, 1355, 6121, 782, 1184, 6716, 7028, 7167, 4384, 924, 511, 7028, 7167, 4384, 924, 2768, 4989, 1400, 8024, 711, 831, 1265, 5052, 4415, 3373, 3354, 510, 976, 1920, 976, 2487, 4384, 924, 772, 689, 8024, 2199, 7028, 7167, 4384, 924, 2802, 6863, 711, 678, 2247, 4384, 924, 772, 689, 4638, 2832, 6598, 510, 5052, 4415, 2398, 1378, 8024, 7028, 7167, 7415, 1730, 2190, 1071, 2792, 2247, 4384, 924, 772, 689, 6822, 6121, 749, 7028, 5299, 8024, 2199, 1071, 2792, 2898, 3300, 4638, 7028, 2412, 705, 4670, 8135, 110, 4638, 5500, 3326, 510, 676, 2292, 4906, 2825, 8183, 110, 4638, 5500, 3326, 510, 676, 2292, 1305, 674, 1849, 8183, 110, 4638, 5500, 3326, 510, 2768, 6963, 736, 3736, 8246, 110, 4638, 5500, 3326, 1469, 7028, 2412, 1398, 1069, 130, 119, 130, 110, 4638, 5500, 3326, 809, 5500, 3326, 1872, 6598, 4638, 3175, 2466, 3800, 1057, 7028, 7167, 4384, 924, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[7028, 7167, 7415, 1730]\n",
      "[7028, 7167, 4384, 924]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16, 29)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = 1178\r\n",
    "text = train_data.loc[ix,'text']\r\n",
    "print(len(text), text)\r\n",
    "sub = train_data.loc[ix,'spo_list'][0]['subject']\r\n",
    "obj = train_data.loc[ix,'spo_list'][0]['object']\r\n",
    "print(sub, obj)\r\n",
    "max_length = 256\r\n",
    "s_ids = tokenizer.encode(sub, max_length=max_length, truncation=True)[1:-1]\r\n",
    "o_ids = tokenizer.encode(obj, max_length=max_length, truncation=True)[1:-1]\r\n",
    "text_ids = tokenizer.encode(text, max_length=max_length, padding='max_length', truncation=True)\r\n",
    "print(text_ids)\r\n",
    "print(s_ids)\r\n",
    "print(o_ids)\r\n",
    "find_head_idx(s_ids, text_ids), find_head_idx(o_ids, text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text ---\n",
      " 47      近年来，全球软件用户群体快速增长，据wearesocial和hootsuite联合发布的《d...\n",
      "3608    2014年6月9日，天鹅股份召开第三届董事会第二十一次会议，审议通过了《关于转让图木舒克市天...\n",
      "5045    宝应锦程村镇银行主营业务为银行业务。截至2016年12月31日，宝应锦程村镇银行总资产为56...\n",
      "2499    公司2020年度、2019年度、2018年度的非经常性损益净额分别为1,237.73万元、1...\n",
      "7074    殷奇先生，39岁，本科，工程师，一级项目经理。1984年在上海城建学院参加工作，1995年到...\n",
      "                              ...                        \n",
      "1856    黄军林先生，汉族，1974年出生，中国国籍，无境外永久居留权。东南大学学士。曾就职于上海长江...\n",
      "7432    除因邵毅平辞任独立董事于2015年8月28日股东大会选举杨鹰彪为公司独立董事、因胡雄辞任监事...\n",
      "380     公司成立于1996年11月，设立时名称为海宁市马桥华生经编针织厂；华生针织厂于2005年5月...\n",
      "7148    资产总额逐年增长。2013年末、2014年末、2015年末及2016年6月末，公司资产总额分...\n",
      "100         最近三年，公司公允价值变动收益分别为77.36万元、-89.08万元及-109.08万元，\n",
      "Name: text, Length: 3554, dtype: object\n",
      "inputs keys --\n",
      " dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input_ids --\n",
      " [[ 101 6818 2399 ...    0    0    0]\n",
      " [ 101 8127 2399 ...    0    0    0]\n",
      " [ 101 2140 2418 ...    0    0    0]\n",
      " ...\n",
      " [ 101 1062 1385 ...    0    0    0]\n",
      " [ 101 6598  772 ... 5763 5709  102]\n",
      " [ 101 3297 6818 ...    0    0    0]]\n",
      "tokens --\n",
      " [CLS] 近 年 来 ， 全 球 软 件 用 户 群 体 快 速 增 长 ， 据 wearesocial 和 hootsuite 联 合 发 布 的 《 digital2019 》 显 示 ， 当 前 全 球 网 民 数 量 达 43. 9 亿 人 ， 渗 透 率 达 57 % ， 网 民 数 量 的 增 长 将 带 动 pdf 等 通 用 软 件 的 市 场 需 求 ， 具 有 广 阔 的 市 场 空 间 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spo_list ------\n",
      " [{'predicate': '关联方', 'object': 'hootsuite', 'subject': 'wearesocia', 'object_type': '公司', 'subject_type': '公司', 'object_index': {'begin': 30, 'end': 39}, 'subject_index': {'begin': 18, 'end': 28}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3554,128,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Tile]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-77de7b6917e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtext_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mbert_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mlast_hidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpooler_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[0;32m    895\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"output_hidden_states\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"return_dict\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m             \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"training\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m         )\n\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"token_type_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"inputs_embeds\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m             \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"training\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m         )\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, training)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mposition_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mposition_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mtoken_type_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mfinal_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_embeds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mtile\u001b[1;34m(input, multiples, name)\u001b[0m\n\u001b[0;32m  11452\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11453\u001b[0m       return tile_eager_fallback(\n\u001b[1;32m> 11454\u001b[1;33m           input, multiples, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m  11455\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11456\u001b[0m       \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mtile_eager_fallback\u001b[1;34m(input, multiples, name, ctx)\u001b[0m\n\u001b[0;32m  11492\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"T\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tmultiples\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_Tmultiples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11493\u001b[0m   _result = _execute.execute(b\"Tile\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[1;32m> 11494\u001b[1;33m                              ctx=ctx, name=name)\n\u001b[0m\u001b[0;32m  11495\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11496\u001b[0m     _execute.record_gradient(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tfs\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3554,128,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Tile]"
     ]
    }
   ],
   "source": [
    "## debug\r\n",
    "model_path = '../model_dirs/bert-base-chinese'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "max_length = 128\r\n",
    "print(f\"text ---\\n {train_data.loc[47:100,'text']}\")\r\n",
    "inputs = tokenizer(train_data.loc[47:100,'text'].to_list(), max_length=max_length, padding='max_length', return_tensors='tf', truncation=True)\r\n",
    "print('inputs keys --\\n', inputs.keys())\r\n",
    "print(f\"input_ids --\\n {inputs['input_ids']}\")\r\n",
    "tokens = tokenizer.decode(inputs['input_ids'][0])\r\n",
    "print('tokens --\\n', tokens)\r\n",
    "print(f\"spo_list ------\\n {train_data.loc[47,'spo_list']}\")\r\n",
    "text_len = len(tokens)\r\n",
    "text_len\r\n",
    "bert_model = TFBertModel.from_pretrained(model_path, output_hidden_states=True)\r\n",
    "outputs = bert_model(inputs)\r\n",
    "last_hidden_state, pooler_output, hidden_states = outputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([54, 128, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(54, 1536), dtype=float32, numpy=\n",
       "array([[ 0.25873297,  0.49238703,  0.63826686, ...,  0.11304718,\n",
       "        -0.9661733 , -0.4333823 ],\n",
       "       [ 0.42852157, -0.17608842,  0.8069094 , ...,  1.4964044 ,\n",
       "        -0.49943933, -0.20207542],\n",
       "       [ 0.5664912 , -0.12230435,  0.19252312, ...,  0.5424045 ,\n",
       "         0.27006415,  0.08944741],\n",
       "       ...,\n",
       "       [ 0.46302688, -0.4424666 ,  0.5141795 , ...,  0.4153874 ,\n",
       "         0.5806328 ,  0.04473295],\n",
       "       [-0.11299253, -0.3138919 , -1.4666147 , ...,  0.36222684,\n",
       "         0.43323463,  0.29208207],\n",
       "       [ 0.6611088 ,  0.7782924 , -0.7245413 , ...,  0.38568816,\n",
       "         0.23528749,  0.6906782 ]], dtype=float32)>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_subject(output, subject_ids):\r\n",
    "    start = tf.gather(output, subject_ids[:,:1], batch_dims=1)\r\n",
    "    end = tf.gather(output, subject_ids[:,1:], batch_dims=1)\r\n",
    "    subject = tf.keras.layers.Concatenate(axis=2)([start, end])\r\n",
    "    return subject[:, 0]\r\n",
    "\r\n",
    "extract_subject(hidden_states[-1], tf.constant(train_subject_ids[:hidden_states[-1].shape[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\r\n",
    "    \"\"\"(Conditional) Layer Normalization\r\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        center=True,\r\n",
    "        scale=True,\r\n",
    "        epsilon=None,\r\n",
    "        conditional=False,\r\n",
    "        hidden_units=None,\r\n",
    "        hidden_activation='linear',\r\n",
    "        hidden_initializer='glorot_uniform',\r\n",
    "        **kwargs):\r\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\r\n",
    "        self.center = center\r\n",
    "        self.scale = scale\r\n",
    "        self.conditional = conditional\r\n",
    "        self.hidden_units = hidden_units\r\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\r\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\r\n",
    "        self.epsilon = epsilon or 1e-12\r\n",
    "        \r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        if self.conditional:\r\n",
    "            masks = mask if mask is not None else []\r\n",
    "            masks = [m[None] for m in masks if m is not None]\r\n",
    "            if len(masks) == 0:\r\n",
    "                return None\r\n",
    "            else:\r\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\r\n",
    "        else:\r\n",
    "            return mask\r\n",
    "        \r\n",
    "    def build(self, input_shape):\r\n",
    "        super(LayerNormalization, self).build(input_shape)\r\n",
    "        if self.conditional:\r\n",
    "            shape = (input_shape[0][-1],)\r\n",
    "        else:\r\n",
    "            shape = (input_shape[-1],)\r\n",
    "        if self.center:\r\n",
    "            self.beta = self.add_weight(\r\n",
    "                shape=shape, initializer='zeros', name='beta')\r\n",
    "        if self.scale:\r\n",
    "            self.gamma = self.add_weight(\r\n",
    "                shape=shape, initializer='ones', name='gamma')\r\n",
    "        if self.conditional:\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\r\n",
    "                    units=self.hidden_units,\r\n",
    "                    activation=self.hidden_activation,\r\n",
    "                    use_bias=False,\r\n",
    "                    kernel_initializer=self.hidden_initializer)\r\n",
    "            if self.center:\r\n",
    "                self.beta_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "            if self.scale:\r\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\r\n",
    "        \"\"\"\r\n",
    "        if self.conditional:\r\n",
    "            inputs, cond = inputs\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                cond = self.hidden_dense(cond)\r\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\r\n",
    "                cond = K.expand_dims(cond, 1)\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta_dense(cond) + self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\r\n",
    "        else:\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma\r\n",
    "        outputs = inputs\r\n",
    "        if self.center:\r\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\r\n",
    "            outputs = outputs - mean\r\n",
    "        if self.scale:\r\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\r\n",
    "            std = K.sqrt(variance + self.epsilon)\r\n",
    "            outputs = outputs / std\r\n",
    "            outputs = outputs * gamma\r\n",
    "        if self.center:\r\n",
    "            outputs = outputs + beta\r\n",
    "        return outputs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "def E2EModel(pretrained_path, max_length, p2id):\r\n",
    "    input_ids = tf.keras.layers.Input((max_length,), dtype=tf.int32, name='input_ids')\r\n",
    "    token_type_ids = tf.keras.layers.Input((max_length,), dtype=tf.int32, name='total_segment_ids')\r\n",
    "    attention_mask = tf.keras.layers.Input((max_length,), dtype=tf.int32, name='attention_mask')\r\n",
    "    subject_ids = tf.keras.layers.Input((2,), dtype=tf.int32, name='subject_ids')\r\n",
    "\r\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, output_hidden_states=True)\r\n",
    "    outputs = bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\r\n",
    "    last_hidden_state, pooler_output, hidden_states = outputs[:3]\r\n",
    "    layer_1 = hidden_states[-1]\r\n",
    "    \r\n",
    "    subject_preds = tf.keras.layers.Dense(units=2, activation='sigmoid',)(layer_1)\r\n",
    "    subject_preds = tf.keras.layers.Lambda(lambda x: x**2)(subject_preds)\r\n",
    "    subject_model = tf.keras.models.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=subject_preds)\r\n",
    "\r\n",
    "    subject = extract_subject([layer_1, subject_ids])\r\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject])\r\n",
    "    output = tf.keras.layers.Dense(units=len(p2d) * 2, activation='sigmoid' )(output)\r\n",
    "    output = tf.keras.layers.Lambda(lambda x: x**4)(output)\r\n",
    "    object_preds = tf.reshape((-1, len(p2id), 2))(output)\r\n",
    "    object_model = tf.keras.models.Model(input=[input_ids, token_type_ids, attention_mask, subject_ids], outputs=object_preds)\r\n",
    "\r\n",
    "    train_model = tf.keras.models.Model(input=[input_ids, token_type_ids, attention_mask, subject_ids], outputs= [subject_preds, object_preds])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "pid = os.getpid()\r\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 24 11:41:14 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 466.27       Driver Version: 466.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   39C    P8    N/A /  N/A |    233MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1356    C+G   ...4__8j3eq9eme6ctt\\IGCC.exe    N/A      |\n",
      "|    0   N/A  N/A      8248    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10808    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tfs': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}