{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\r\n",
    "from random import choice\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集整理\r\n",
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(path, nrows=False):\r\n",
    "    if nrows:\r\n",
    "        df = pd.read_json(path, nrows=nrows, lines=True)\r\n",
    "    else:\r\n",
    "        df = pd.read_json(path, lines=True)\r\n",
    "    df = df[['text', 'spo_list']]\r\n",
    "    return df\r\n",
    "\r\n",
    "def merge_df(dir_path):\r\n",
    "    total_df = pd.DataFrame()\r\n",
    "    for fn in os.listdir(dir_path):\r\n",
    "        df = json_to_df(os.path.join(dir_path, fn))\r\n",
    "        df_fn = fn[:fn.rfind('.')]\r\n",
    "        df.insert(0, 'fn', df_fn)\r\n",
    "        total_df =  total_df.append(df)\r\n",
    "    total_df.reset_index(drop=True, inplace=True)\r\n",
    "    print(f'data size: {total_df.shape}') #\r\n",
    "    print(f'data sample: {df.sample(5)}')\r\n",
    "    return total_df   \r\n",
    "\r\n",
    "def read_schemads(path_or_df):\r\n",
    "    if not isinstance(path_or_df, pd.DataFrame):\r\n",
    "        print(1)\r\n",
    "        schemads_path = path_or_df\r\n",
    "        predicate_data = pd.read_json(schemads_path, lines=True)\r\n",
    "        id2p = predicate_data['predicate'].drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    else:\r\n",
    "        df = path_or_df\r\n",
    "        id2p = df['spo_list'].apply(lambda spo_list: [spo['predicate'] for spo in spo_list])\r\n",
    "        id2p = id2p.explode().drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    p2id = dict(zip(id2p.values(), id2p.keys()))\r\n",
    "    print(f'length of p2id :{len(p2id)}')#\r\n",
    "    print(f'random p2id sample:{random.sample(p2id.items(), 5)}')#\r\n",
    "    return id2p, p2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\r\n",
    "# # 百度三元组关系数据集\r\n",
    "# train_path ='../data/百度关系抽取数据集/train_data.json'\r\n",
    "# train_data = json_to_df(train_path, nrows=10000)\r\n",
    "# print(f'Train data size: {train_data.shape}') #\r\n",
    "\r\n",
    "# dev_path = '../data/百度关系抽取数据集/dev_data.json'\r\n",
    "# dev_data = json_to_df(dev_path, nrows=5000)\r\n",
    "# print(f'Validation data size: {dev_data.shape}') \r\n",
    "\r\n",
    "# schemads_path = '../data/百度关系抽取数据集/all_50_schemas'\r\n",
    "# id2p, p2id = read_schemads(schemads_path)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: (6093, 3)\n",
      "data sample:                   fn                                               text  \\\n",
      "3   阿科力首次公开发行股票招股说明书  2013年6月3日，阿科力有限股东会做出决议，同意阿科力有限整体变更为股份有限公司，公司整体...   \n",
      "46  阿科力首次公开发行股票招股说明书  2001年7月11日，经公司股东会决议批准，将原注册资本50万元增资至200万元，各股东资金...   \n",
      "86  阿科力首次公开发行股票招股说明书  2017年1-6月经营活动现金流量净额为1,990.65万元，2016年1-6月经营活动现金...   \n",
      "4   阿科力首次公开发行股票招股说明书  2015年5月26日，无锡诚鼎创业投资中心（有限合伙）向上海中汇金玖三期创业投资基金合伙企业...   \n",
      "84  阿科力首次公开发行股票招股说明书  报告期内，公司管理费用主要由职工薪酬、研发费用、环境保护费、中介服务费等项目构成。2015年...   \n",
      "\n",
      "                                             spo_list  \n",
      "3   [{'predicate': '变更日期', 'object_type': '日期', 's...  \n",
      "46  [{'predicate': '注册资本', 'object_type': '金额', 's...  \n",
      "86  [{'predicate': '公司经营活动产生的现金流量净额', 'object_type...  \n",
      "4   [{'predicate': '转让日期', 'object_type': '日期', 's...  \n",
      "84  [{'predicate': '管理费用', 'object_type': '金额', 's...  \n",
      "length of p2id :114\n",
      "random p2id sample:[('其他流动资产', 65), ('速动比率', 18), ('非流动负债', 107), ('每股净资产', 50), ('应收账款周转率', 73)]\n"
     ]
    }
   ],
   "source": [
    "## 招股说明书三元组数据集\r\n",
    "dir_path = '../data/招股说明书三元组数据集'\r\n",
    "train_size=0.9\r\n",
    "df = merge_df(dir_path)\r\n",
    "\r\n",
    "train_data = df.sample(frac=train_size,random_state=200)\r\n",
    "dev_data = df.drop(train_data.index).reset_index(drop=True)\r\n",
    "# schemads_path = '../data/招股说明书三元组数据集/all_50_schemas'\r\n",
    "id2p, p2id = read_schemads(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spo(spo_list):\r\n",
    "    for spo in spo_list:\r\n",
    "        spo['predicate'] = spo['predicate'].lower()\r\n",
    "        spo['subject'] = spo['subject'].lower()\r\n",
    "        spo['object'] = spo['object'].lower()\r\n",
    "    return spo_list\r\n",
    "\r\n",
    "def data_clean(df):\r\n",
    "    df['text'] = df['text'].str.lower()\r\n",
    "    df['spo_list'] = df['spo_list'].apply(clean_spo)\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spo_count 15412\n"
     ]
    }
   ],
   "source": [
    "train_data = data_clean(train_data)\r\n",
    "dev_data = data_clean(dev_data)\r\n",
    "\r\n",
    "train_text = train_data['text'].to_list()\r\n",
    "train_spo = train_data['spo_list'].to_list()\r\n",
    "\r\n",
    "spo_single_count = train_data['spo_list'].apply(lambda x: len(x))\r\n",
    "spo_count = spo_single_count.sum()\r\n",
    "print('spo_count', spo_count)\r\n",
    "                                                \r\n",
    "dev_text = dev_data['text'].to_list()\r\n",
    "dev_spo = dev_data['spo_list'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_head_idx(pattern, sequence):\r\n",
    "    \"\"\"从sequence中寻找子串pattern\r\n",
    "    如果找到，返回第一个下标；否则返回-1。\r\n",
    "    \"\"\"\r\n",
    "    n = len(pattern)\r\n",
    "    for i in range(len(sequence)):\r\n",
    "        if sequence[i:i + n] == pattern:\r\n",
    "            return i\r\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(df, p2id, tokenizer, max_length):\r\n",
    "    long_text_count = 0\r\n",
    "    total_token_ids, total_token_type_ids, total_attention_mask = [], [], []\r\n",
    "    total_subject_labels, total_subject_ids, total_object_labels = [], [], []\r\n",
    "    for idx in df.index:\r\n",
    "        row = df.loc[idx]\r\n",
    "        inputs = tokenizer(row['text'], max_length=max_length, padding='max_length', truncation=True)\r\n",
    "        token_ids, token_type_ids, attention_mask = inputs['input_ids'], inputs['token_type_ids'], inputs['attention_mask']\r\n",
    "        # 实体关系token id 位置字典\r\n",
    "        s2op_map = {}\r\n",
    "        for sop_n, spo in enumerate(row['spo_list']):\r\n",
    "            sub_ids = tokenizer.encode(spo['subject'])[1:-1]\r\n",
    "            p_id = p2id[spo['predicate']]\r\n",
    "            obj_ids = tokenizer.encode(spo['object'])[1:-1]\r\n",
    "            # 查找subject 和 object对应的token id 起始索引\r\n",
    "            sub_head_idx = find_head_idx(sub_ids, inputs['input_ids'])\r\n",
    "            obj_head_idx = find_head_idx(obj_ids, inputs['input_ids'])\r\n",
    "            if sub_head_idx != -1 and obj_head_idx != -1:\r\n",
    "                # 获取subject 起始位置的索引和结束位置索引的元组\r\n",
    "                sub = (sub_head_idx, sub_head_idx + len(sub_ids) - 1)\r\n",
    "                # 获取object 起始位置的索引和结束位置索引元组以及与关系标签id的元组对\r\n",
    "                obj = (obj_head_idx, obj_head_idx + len(obj_ids) - 1, p_id)\r\n",
    "                # print('---- sub -----', sub)#    \r\n",
    "                if sub not in s2op_map:\r\n",
    "                    s2op_map[sub] = []\r\n",
    "                s2op_map[sub].append(obj)\r\n",
    "                # print('-----------', s2op_map)#                \r\n",
    "                {(22,23):[(28, 29, 7), (25, 26, 8)]}  \r\n",
    "            else:\r\n",
    "                long_text_count +=1\r\n",
    "                # print('--idx--', idx, '--text--', row['text'])\r\n",
    "                # print('--sop_n--', sop_n, '--spo--', spo, '\\n')\r\n",
    "\r\n",
    "\r\n",
    "        if s2op_map:\r\n",
    "        # subject标签\r\n",
    "            subject_labels = np.zeros((max_length, 2))\r\n",
    "            for s in s2op_map:\r\n",
    "                #sub_head\r\n",
    "                subject_labels[s[0], 0] = 1\r\n",
    "                #sub_tail\r\n",
    "                subject_labels[s[1], 1] = 1\r\n",
    "            # 随机选一个subject\r\n",
    "            sub_head, sub_tail = choice(list(s2op_map.keys()))\r\n",
    "            subject_ids = (sub_head, sub_tail)\r\n",
    "            # sub_head, sub_tail = np.array(list(s2op_map.keys())).T\r\n",
    "            # sub_head = np.random.choice(sub_head)\r\n",
    "            # sub_tail = np.random.choice(sub_tail[sub_tail >= sub_head])\r\n",
    "            # 对应的object标签\r\n",
    "            object_labels = np.zeros((len(token_ids), len(p2id), 2))\r\n",
    "            for op in s2op_map.get((sub_head, sub_tail), []):\r\n",
    "                # print(op)\r\n",
    "                # obj_head\r\n",
    "                object_labels[op[0], op[2], 0] = 1\r\n",
    "                # obj_tail\r\n",
    "                object_labels[op[1], op[2], 1] = 1\r\n",
    "\r\n",
    "            # 所有数据汇总\r\n",
    "            total_token_ids.append(token_ids)\r\n",
    "            total_token_type_ids.append(token_type_ids)\r\n",
    "            total_attention_mask.append(attention_mask)\r\n",
    "            total_subject_labels.append(subject_labels)\r\n",
    "            total_subject_ids.append(subject_ids)\r\n",
    "            total_object_labels.append(object_labels)  \r\n",
    "    print('long_text_count', long_text_count)                      \r\n",
    "    return total_token_ids, total_token_type_ids, total_attention_mask, \\\r\n",
    "           total_subject_labels, total_subject_ids, total_object_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_demo = data_process(train_data[3:4], p2id, tokenizer, max_length)\r\n",
    "# print(np.array(input_demo[5]).shape)\r\n",
    "# print(train_spo[3])\r\n",
    "# print(np.array(input_demo[5])[0, 25, 14, 0], np.array(input_demo[5])[0, 27, 14, 1])\r\n",
    "# print(np.array(input_demo[5])[0, 14, 17, 0], np.array(input_demo[5])[0, 14, 17, 1])\r\n",
    "# # tokenizer.decode(train_input_ids[3][op_s:op_e+1])\r\n",
    "# print(train_spo[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_text_count 126\n",
      "[(3, 4), (1, 6)]\n",
      "(5275, 512, 2)\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "model_path = '../model_dirs/bert-base-chinese'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "max_length = 512\r\n",
    "train_input_ids, train_token_type_ids, train_attention_mask, train_subject_labels, train_subject_ids, train_object_labels  = data_process(train_data, p2id, tokenizer, max_length)\r\n",
    "print(train_subject_ids[:2])\r\n",
    "print(np.array(train_subject_labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 621 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "val_inputs = tokenizer(dev_text, max_length=max_length, padding='max_length', truncation=True, return_tensors='tf') \r\n",
    "val_input_ids, val_attention_mask = val_inputs['input_ids'], val_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 福州三期epc项目、洛碛epc项目和汕尾二期epc项目受土建工程及设备到货安装进度影响，2018年第四季度合计确认收入14,603.42万元，高于本年度其他季度收入确认金额；\n",
      "2018年第四季度 14,603.42万元\n",
      "[101, 4886, 2336, 676, 3309, 10676, 8177, 7555, 4680, 510, 3821, 4816, 10676, 8177, 7555, 4680, 1469, 3730, 2227, 753, 3309, 10676, 8177, 7555, 4680, 1358, 1759, 2456, 2339, 4923, 1350, 6392, 1906, 1168, 6573, 2128, 6163, 6822, 2428, 2512, 1510, 8024, 8271, 2399, 5018, 1724, 2108, 2428, 1394, 6369, 4802, 6371, 3119, 1057, 8122, 117, 13277, 119, 8239, 674, 1039, 8024, 7770, 754, 3315, 2399, 2428, 1071, 800, 2108, 2428, 3119, 1057, 4802, 6371, 7032, 7583, 8039, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[8271, 2399, 5018, 1724, 2108, 2428]\n",
      "[8122, 117, 13277, 119, 8239, 674, 1039]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42, 54)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = 1178\r\n",
    "text = train_data.loc[ix,'text']\r\n",
    "print(len(text), text)\r\n",
    "sub = train_data.loc[ix,'spo_list'][0]['subject']\r\n",
    "obj = train_data.loc[ix,'spo_list'][0]['object']\r\n",
    "print(sub, obj)\r\n",
    "max_length = 256\r\n",
    "s_ids = tokenizer.encode(sub, max_length=max_length, truncation=True)[1:-1]\r\n",
    "o_ids = tokenizer.encode(obj, max_length=max_length, truncation=True)[1:-1]\r\n",
    "text_ids = tokenizer.encode(text, max_length=max_length, padding='max_length', truncation=True)\r\n",
    "print(text_ids)\r\n",
    "print(s_ids)\r\n",
    "print(o_ids)\r\n",
    "find_head_idx(s_ids, text_ids), find_head_idx(o_ids, text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text ---\n",
      " 47      当前，信息技术和信息化的不断发展和普及促使全球互联网渗透率不断提升，由此扩大了网络用户的数量...\n",
      "2466    2017年11月3日，上海市徐汇区商务委员会出具《外商投资企业变更备案回执》（沪徐外资备20...\n",
      "5399    2002年8月，由本公司与控股子公司上海博佳投资管理有限公司共同投资组建的台州市路桥至泽国至...\n",
      "6022    所以，公司根据未来集中精力发展壮大聚醚胺和光学材料的产品发展的战略规划，经过审慎决策和过渡期...\n",
      "1787    2019年末，公司库存商品账面价值为8,654.92万元，较2018年末有所增长，主要系20...\n",
      "                              ...                        \n",
      "4299    2017年9月，成燃有限整体变更为股份公司，成都燃气选举产生了股份公司的新一届管理层。201...\n",
      "4116    公司以1999年9月30日资产评估基准日的资产状况，于1999年12月正式移交建账，根据江西...\n",
      "3309    截至本招股说明书出具日，新疆生产建设兵团国有资产经营公司持有新疆生产建设兵团供销合作总公司1...\n",
      "1071    2010年6月30日，重钢集团与重钢环保签订股权转让协议，重钢集团将其持有的三峰卡万塔60%...\n",
      "100     2019年度，公司信用减值损失回转204.09万元，主要由于公司根据财政部\\n的规定执行《企...\n",
      "Name: text, Length: 1779, dtype: object\n",
      "inputs keys --\n",
      " dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input_ids --\n",
      " [[ 101 2496 1184 ... 5635 8109  102]\n",
      " [ 101 8109 2399 ...    0    0    0]\n",
      " [ 101 8301 2399 ...  119 8360  102]\n",
      " ...\n",
      " [ 101 2779 5635 ...    0    0    0]\n",
      " [ 101 8166 2399 ...    0    0    0]\n",
      " [ 101 9160 2399 ...    0    0    0]]\n",
      "tokens --\n",
      " [CLS] 当 前 ， 信 息 技 术 和 信 息 化 的 不 断 发 展 和 普 及 促 使 全 球 互 联 网 渗 透 率 不 断 提 升 ， 由 此 扩 大 了 网 络 用 户 的 数 量 ， 增 加 了 电 子 文 档 交 换 的 需 求 ， pdf 作 为 电 子 文 档 交 换 的 格 式 标 准 ， 需 求 将 日 益 增 加 。 据 世 界 银 行 数 据 显 示 ， 自 2000 年 以 来 ， 全 球 互 联 网 普 及 率 呈 快 速 上 升 态 势 ， 由 2000 年 的 6. 7 % 增 至 2017 [SEP]\n",
      "spo_list ------\n",
      " [{'predicate': '流动比率', 'object': '2017年', 'subject': '77.0%', 'object_type': '日期', 'subject_type': '比率', 'object_index': {'begin': 184, 'end': 189}, 'subject_index': {'begin': 198, 'end': 203}}, {'predicate': '速动比率', 'object': '2000年', 'subject': '2.46个百分点', 'object_type': '日期', 'subject_type': '比率', 'object_index': {'begin': 96, 'end': 101}, 'subject_index': {'begin': 149, 'end': 157}}, {'predicate': '流动比率', 'object': '2000年', 'subject': '6.7%', 'object_type': '日期', 'subject_type': '比率', 'object_index': {'begin': 121, 'end': 126}, 'subject_index': {'begin': 127, 'end': 131}}, {'predicate': '流动比率', 'object': '2017年', 'subject': '48.6%', 'object_type': '日期', 'subject_type': '比率', 'object_index': {'begin': 133, 'end': 138}, 'subject_index': {'begin': 139, 'end': 144}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "## debug\r\n",
    "model_path = '../model_dirs/bert-base-chinese'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "max_length = 128\r\n",
    "print(f\"text ---\\n {train_data.loc[47:100,'text']}\")\r\n",
    "inputs = tokenizer(train_data.loc[47:100,'text'].to_list(), max_length=max_length, padding='max_length', return_tensors='tf', truncation=True)\r\n",
    "print('inputs keys --\\n', inputs.keys())\r\n",
    "print(f\"input_ids --\\n {inputs['input_ids']}\")\r\n",
    "tokens = tokenizer.decode(inputs['input_ids'][0])\r\n",
    "print('tokens --\\n', tokens)\r\n",
    "print(f\"spo_list ------\\n {train_data.loc[47,'spo_list']}\")\r\n",
    "text_len = len(tokens)\r\n",
    "text_len\r\n",
    "bert_model = TFBertModel.from_pretrained(model_path, output_hidden_states=True)\r\n",
    "outputs = bert_model(inputs)\r\n",
    "last_hidden_state, pooler_output, hidden_states = outputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([54, 128, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(54, 1536), dtype=float32, numpy=\n",
       "array([[ 0.25873297,  0.49238703,  0.63826686, ...,  0.11304718,\n",
       "        -0.9661733 , -0.4333823 ],\n",
       "       [ 0.42852157, -0.17608842,  0.8069094 , ...,  1.4964044 ,\n",
       "        -0.49943933, -0.20207542],\n",
       "       [ 0.5664912 , -0.12230435,  0.19252312, ...,  0.5424045 ,\n",
       "         0.27006415,  0.08944741],\n",
       "       ...,\n",
       "       [ 0.46302688, -0.4424666 ,  0.5141795 , ...,  0.4153874 ,\n",
       "         0.5806328 ,  0.04473295],\n",
       "       [-0.11299253, -0.3138919 , -1.4666147 , ...,  0.36222684,\n",
       "         0.43323463,  0.29208207],\n",
       "       [ 0.6611088 ,  0.7782924 , -0.7245413 , ...,  0.38568816,\n",
       "         0.23528749,  0.6906782 ]], dtype=float32)>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_subject(output, subject_ids):\r\n",
    "    start = tf.gather(output, subject_ids[:,:1], batch_dims=1)\r\n",
    "    end = tf.gather(output, subject_ids[:,1:], batch_dims=1)\r\n",
    "    subject = tf.keras.layers.Concatenate(axis=2)([start, end])\r\n",
    "    return subject[:, 0]\r\n",
    "\r\n",
    "extract_subject(hidden_states[-1], tf.constant(train_subject_ids[:hidden_states[-1].shape[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\r\n",
    "    \"\"\"(Conditional) Layer Normalization\r\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        center=True,\r\n",
    "        scale=True,\r\n",
    "        epsilon=None,\r\n",
    "        conditional=False,\r\n",
    "        hidden_units=None,\r\n",
    "        hidden_activation='linear',\r\n",
    "        hidden_initializer='glorot_uniform',\r\n",
    "        **kwargs):\r\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\r\n",
    "        self.center = center\r\n",
    "        self.scale = scale\r\n",
    "        self.conditional = conditional\r\n",
    "        self.hidden_units = hidden_units\r\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\r\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\r\n",
    "        self.epsilon = epsilon or 1e-12\r\n",
    "        \r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        if self.conditional:\r\n",
    "            masks = mask if mask is not None else []\r\n",
    "            masks = [m[None] for m in masks if m is not None]\r\n",
    "            if len(masks) == 0:\r\n",
    "                return None\r\n",
    "            else:\r\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\r\n",
    "        else:\r\n",
    "            return mask\r\n",
    "        \r\n",
    "    def build(self, input_shape):\r\n",
    "        super(LayerNormalization, self).build(input_shape)\r\n",
    "        if self.conditional:\r\n",
    "            shape = (input_shape[0][-1],)\r\n",
    "        else:\r\n",
    "            shape = (input_shape[-1],)\r\n",
    "        if self.center:\r\n",
    "            self.beta = self.add_weight(\r\n",
    "                shape=shape, initializer='zeros', name='beta')\r\n",
    "        if self.scale:\r\n",
    "            self.gamma = self.add_weight(\r\n",
    "                shape=shape, initializer='ones', name='gamma')\r\n",
    "        if self.conditional:\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\r\n",
    "                    units=self.hidden_units,\r\n",
    "                    activation=self.hidden_activation,\r\n",
    "                    use_bias=False,\r\n",
    "                    kernel_initializer=self.hidden_initializer)\r\n",
    "            if self.center:\r\n",
    "                self.beta_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "            if self.scale:\r\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\r\n",
    "        \"\"\"\r\n",
    "        if self.conditional:\r\n",
    "            inputs, cond = inputs\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                cond = self.hidden_dense(cond)\r\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\r\n",
    "                cond = K.expand_dims(cond, 1)\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta_dense(cond) + self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\r\n",
    "        else:\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma\r\n",
    "        outputs = inputs\r\n",
    "        if self.center:\r\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\r\n",
    "            outputs = outputs - mean\r\n",
    "        if self.scale:\r\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\r\n",
    "            std = K.sqrt(variance + self.epsilon)\r\n",
    "            outputs = outputs / std\r\n",
    "            outputs = outputs * gamma\r\n",
    "        if self.center:\r\n",
    "            outputs = outputs + beta\r\n",
    "        return outputs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "def E2EModel(pretrained_path, max_length, p2id):\r\n",
    "    input_ids = tf.keras.layers.Input((max_length,), dtype=tf.int32, name='input_ids')\r\n",
    "    token_type_ids = tf.keras.layers.Input((max_length,), dtype=tf.int32, name='total_segment_ids')\r\n",
    "    attention_mask = tf.keras.layers.Input((max_length,), dtype=tf.int32, name='attention_mask')\r\n",
    "    subject_ids = tf.keras.layers.Input((2,), dtype=tf.int32, name='subject_ids')\r\n",
    "\r\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, output_hidden_states=True)\r\n",
    "    outputs = bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\r\n",
    "    last_hidden_state, pooler_output, hidden_states = outputs[:3]\r\n",
    "    layer_1 = hidden_states[-1]\r\n",
    "    \r\n",
    "    subject_preds = tf.keras.layers.Dense(units=2, activation='sigmoid',)(layer_1)\r\n",
    "    subject_preds = tf.keras.layers.Lambda(lambda x: x**2)(subject_preds)\r\n",
    "    subject_model = tf.keras.models.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=subject_preds)\r\n",
    "\r\n",
    "    subject = extract_subject([layer_1, subject_ids])\r\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject])\r\n",
    "    output = tf.keras.layers.Dense(units=len(p2d) * 2, activation='sigmoid' )(output)\r\n",
    "    output = tf.keras.layers.Lambda(lambda x: x**4)(output)\r\n",
    "    object_preds = tf.reshape((-1, len(p2id), 2))(output)\r\n",
    "    object_model = tf.keras.models.Model(input=[input_ids, token_type_ids, attention_mask, subject_ids], outputs=object_preds)\r\n",
    "\r\n",
    "    train_model = tf.keras.models.Model(input=[input_ids, token_type_ids, attention_mask, subject_ids], outputs= [subject_preds, object_preds])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 22 21:06:44 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 466.27       Driver Version: 466.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   43C    P8    N/A /  N/A |    173MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1356    C+G   ...4__8j3eq9eme6ctt\\IGCC.exe    N/A      |\n",
      "|    0   N/A  N/A      4232    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      7980    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      8248    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10808    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "pid = os.getpid()\r\n",
    "!kill -9 $pid\r\n",
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tfs': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}