{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from ast import literal_eval\r\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如何演好自己的角色，请读《演员自我修养》《喜剧之王》周星驰崛起于穷困潦倒之中的独门秘笈 [{'predicate': '主演', 'object_type': '人物', 'subject_type': '影视作品', 'object': '周星驰', 'subject': '喜剧之王'}]\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\r\n",
    "    text_list = []\r\n",
    "    spo_list = []\r\n",
    "    with open(path, encoding='utf-8') as json_file:\r\n",
    "        for line in json_file:\r\n",
    "            text_list.append(literal_eval(line)['text'])\r\n",
    "            spo_list.append(literal_eval(line)['spo_list'])\r\n",
    "    return text_list, spo_list\r\n",
    "    \r\n",
    "path='../data/百度关系抽取数据集/train_data.json'\r\n",
    "# path='../data/百度关系抽取数据集/experiment.json'\r\n",
    "text_list, spo_list = load_data(path)\r\n",
    "print(text_list[0], spo_list[0])\r\n",
    "val_path = '../data/百度关系抽取数据集/dev_data.json'\r\n",
    "# val_path = '../data/百度关系抽取数据集/experiment.json'\r\n",
    "val_text_list, val_spo_list = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "173108"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'字': 0, '号': 1, '占地面积': 2, '出生日期': 3, '祖籍': 4, '目': 5, '邮政编码': 6, '所在城市': 7, '气候': 8, '作者': 9, '毕业院校': 10, '身高': 11, '制片人': 12, '民族': 13, '董事长': 14, '嘉宾': 15, '导演': 16, '专业代码': 17, '主持人': 18, '作词': 19, '朝代': 20, '编剧': 21, '连载网站': 22, '人口数量': 23, '修业年限': 24, '所属专辑': 25, '海拔': 26, '面积': 27, '出品公司': 28, '作曲': 29, '创始人': 30, '改编自': 31, '官方语言': 32, '简称': 33, '国籍': 34, '丈夫': 35, '注册资本': 36, '妻子': 37, '父亲': 38, '出版社': 39, '成立日期': 40, '首都': 41, '主演': 42, '上映时间': 43, '总部地点': 44, '出生地': 45, '歌手': 46, '主角': 47, '母亲': 48}\n",
      "{0: '字', 1: '号', 2: '占地面积', 3: '出生日期', 4: '祖籍', 5: '目', 6: '邮政编码', 7: '所在城市', 8: '气候', 9: '作者', 10: '毕业院校', 11: '身高', 12: '制片人', 13: '民族', 14: '董事长', 15: '嘉宾', 16: '导演', 17: '专业代码', 18: '主持人', 19: '作词', 20: '朝代', 21: '编剧', 22: '连载网站', 23: '人口数量', 24: '修业年限', 25: '所属专辑', 26: '海拔', 27: '面积', 28: '出品公司', 29: '作曲', 30: '创始人', 31: '改编自', 32: '官方语言', 33: '简称', 34: '国籍', 35: '丈夫', 36: '注册资本', 37: '妻子', 38: '父亲', 39: '出版社', 40: '成立日期', 41: '首都', 42: '主演', 43: '上映时间', 44: '总部地点', 45: '出生地', 46: '歌手', 47: '主角', 48: '母亲'}\n"
     ]
    }
   ],
   "source": [
    "def load_predicate(path):\r\n",
    "    with open(path,'r', encoding='utf-8')  as f:\r\n",
    "        predicate_list = [literal_eval(i)['predicate'] for i in f]\r\n",
    "    p2id = {}\r\n",
    "    id2p = {}\r\n",
    "    data = list(set(predicate_list))\r\n",
    "    for i in range(len(data)):\r\n",
    "        p2id[data[i]] = i\r\n",
    "        id2p[i] = data[i]\r\n",
    "    return p2id, id2p\r\n",
    "    \r\n",
    "path = '../data/百度关系抽取数据集/all_50_schemas'\r\n",
    "p2id, id2p = load_predicate(path)\r\n",
    "print(p2id)\r\n",
    "print(id2p)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_data(text_list,spo_list,p2id,id2p,tokenizer,max_length):\r\n",
    "    id_label = {}\r\n",
    "    ct = len(text_list)\r\n",
    "    input_ids = np.zeros((ct,max_length),dtype='int32')\r\n",
    "    attention_mask = np.zeros((ct,max_length),dtype='int32')\r\n",
    "    start_tokens = np.zeros((ct,max_length),dtype='int32')\r\n",
    "    end_tokens = np.zeros((ct,max_length),dtype='int32')\r\n",
    "    send_s_po = np.zeros((ct,2),dtype='int32')\r\n",
    "    object_start_tokens = np.zeros((ct,max_length,len(p2id)),dtype='int32')\r\n",
    "    object_end_tokens = np.zeros((ct,max_length,len(p2id)),dtype='int32')\r\n",
    "    invalid_index = []\r\n",
    "    for k in range(ct):\r\n",
    "        context_k = text_list[k].lower().replace(' ','')\r\n",
    "        enc_context = tokenizer.encode(context_k,max_length=max_length,padding='max_length',truncation=True) \r\n",
    "        if len(spo_list[k])==0:\r\n",
    "            invalid_index.append(k)\r\n",
    "            continue\r\n",
    "        start = []\r\n",
    "        end = []\r\n",
    "        S_index = []\r\n",
    "        for j in range(len(spo_list[k])):\r\n",
    "            answers_text_k = spo_list[k][j]['subject'].lower().replace(' ','')\r\n",
    "            chars = np.zeros((len(context_k)))\r\n",
    "            index = context_k.find(answers_text_k)\r\n",
    "            chars[index:index+len(answers_text_k)]=1\r\n",
    "            offsets = []\r\n",
    "            idx=0\r\n",
    "            for t in enc_context[1:]:\r\n",
    "                w = tokenizer.decode([t])\r\n",
    "                if '#' in w and len(w)>1:\r\n",
    "                    w = w.replace('#','')\r\n",
    "                if w == '[UNK]':\r\n",
    "                    w = '。'\r\n",
    "                offsets.append((idx,idx+len(w)))\r\n",
    "                \r\n",
    "                idx += len(w)\r\n",
    "            toks = []\r\n",
    "            for i,(a,b) in enumerate(offsets):\r\n",
    "                sm = np.sum(chars[a:b])\r\n",
    "                if sm>0: \r\n",
    "                    toks.append(i) \r\n",
    "            input_ids[k,:len(enc_context)] = enc_context\r\n",
    "            attention_mask[k,:len(enc_context)] = 1\r\n",
    "            if len(toks)>0:\r\n",
    "                start_tokens[k,toks[0]+1] = 1\r\n",
    "                end_tokens[k,toks[-1]+1] = 1\r\n",
    "                start.append(toks[0]+1)\r\n",
    "                end.append(toks[-1]+1)\r\n",
    "                S_index.append(j)\r\n",
    "                #随机抽取可以作为负样本提高准确率（不认同）\r\n",
    "        if len(start) > 0:\r\n",
    "            start_np = np.array(start)\r\n",
    "            end_np = np.array(end)\r\n",
    "            start_ = np.random.choice(start_np)\r\n",
    "            end_ = np.random.choice(end_np[end_np >= start_])\r\n",
    "            send_s_po[k,0] = start_\r\n",
    "            send_s_po[k,1] = end_\r\n",
    "            s_index = start.index(start_)\r\n",
    "            #随机选取object的首位，如果选取错误，则作为负样本\r\n",
    "            if end_ == end[s_index]:\r\n",
    "                for index in range(len(start)):\r\n",
    "                    if start[index] == start_ and end[index] == end_:\r\n",
    "                        object_text_k = spo_list[k][S_index[index]]['object'].lower().replace(' ','')\r\n",
    "                        predicate = spo_list[k][S_index[index]]['predicate']\r\n",
    "                        p_id = p2id[predicate]\r\n",
    "                        chars = np.zeros((len(context_k)))\r\n",
    "                        index = context_k.find(object_text_k)\r\n",
    "                        chars[index:index+len(object_text_k)]=1\r\n",
    "                        offsets = [] \r\n",
    "                        idx=0\r\n",
    "                        for t in enc_context[1:]:\r\n",
    "                            w = tokenizer.decode([t])\r\n",
    "                            if '#' in w and len(w)>1:\r\n",
    "                                w = w.replace('#','')\r\n",
    "                            if w == '[UNK]':\r\n",
    "                                w = '。'\r\n",
    "                            offsets.append((idx,idx+len(w)))\r\n",
    "                            idx += len(w)\r\n",
    "                        toks = []\r\n",
    "                        for i,(a,b) in enumerate(offsets):\r\n",
    "                            sm = np.sum(chars[a:b])\r\n",
    "                            if sm>0: \r\n",
    "                                toks.append(i) \r\n",
    "                        if len(toks)>0:\r\n",
    "                            id_label[p_id] = predicate\r\n",
    "                            object_start_tokens[k,toks[0]+1,p_id] = 1\r\n",
    "                            object_end_tokens[k,toks[-1]+1,p_id] = 1\r\n",
    "        else:\r\n",
    "            invalid_index.append(k)\r\n",
    "    input_ids = tf.constant(input_ids)\r\n",
    "    attention_mask = tf.constant(attention_mask)\r\n",
    "    start_tokens = tf.constant(start_tokens)\r\n",
    "    end_tokens = tf.constant(end_tokens)\r\n",
    "    send_s_po = tf.constant(send_s_po)\r\n",
    "    object_start_tokens = tf.constant(object_start_tokens)\r\n",
    "    object_end_tokens = tf.constant(object_end_tokens)\r\n",
    "    # invalid_index = tf.constant(invalid_index)\r\n",
    "    return input_ids, attention_mask, start_tokens, end_tokens, send_s_po, object_start_tokens, object_end_tokens, invalid_index, id_label\r\n",
    "\r\n",
    "max_length = 128  \r\n",
    "model_path = '../model_dirs/bert-base-chinese'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)    \r\n",
    "input_ids, attention_mask, start_tokens, end_tokens, send_s_po, object_start_tokens, object_end_tokens, invalid_index, id_label \\\r\n",
    "= proceed_data(text_list,spo_list,p2id,id2p,tokenizer,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs = tokenizer(text_list, max_length=max_length, padding='max_length', truncation=True, return_tensors='tf') \r\n",
    "val_input_ids, val_attention_mask = val_inputs['input_ids'], val_inputs['attention_mask']\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\r\n",
    "    \"\"\"(Conditional) Layer Normalization\r\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        center=True,\r\n",
    "        scale=True,\r\n",
    "        epsilon=None,\r\n",
    "        conditional=False,\r\n",
    "        hidden_units=None,\r\n",
    "        hidden_activation='linear',\r\n",
    "        hidden_initializer='glorot_uniform',\r\n",
    "        **kwargs):\r\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\r\n",
    "        self.center = center\r\n",
    "        self.scale = scale\r\n",
    "        self.conditional = conditional\r\n",
    "        self.hidden_units = hidden_units\r\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\r\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\r\n",
    "        self.epsilon = epsilon or 1e-12\r\n",
    "        \r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        if self.conditional:\r\n",
    "            masks = mask if mask is not None else []\r\n",
    "            masks = [m[None] for m in masks if m is not None]\r\n",
    "            if len(masks) == 0:\r\n",
    "                return None\r\n",
    "            else:\r\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\r\n",
    "        else:\r\n",
    "            return mask\r\n",
    "        \r\n",
    "    def build(self, input_shape):\r\n",
    "        super(LayerNormalization, self).build(input_shape)\r\n",
    "        if self.conditional:\r\n",
    "            shape = (input_shape[0][-1],)\r\n",
    "        else:\r\n",
    "            shape = (input_shape[-1],)\r\n",
    "        if self.center:\r\n",
    "            self.beta = self.add_weight(\r\n",
    "                shape=shape, initializer='zeros', name='beta')\r\n",
    "        if self.scale:\r\n",
    "            self.gamma = self.add_weight(\r\n",
    "                shape=shape, initializer='ones', name='gamma')\r\n",
    "        if self.conditional:\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\r\n",
    "                    units=self.hidden_units,\r\n",
    "                    activation=self.hidden_activation,\r\n",
    "                    use_bias=False,\r\n",
    "                    kernel_initializer=self.hidden_initializer)\r\n",
    "            if self.center:\r\n",
    "                self.beta_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "            if self.scale:\r\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\r\n",
    "        \"\"\"\r\n",
    "        if self.conditional:\r\n",
    "            inputs, cond = inputs\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                cond = self.hidden_dense(cond)\r\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\r\n",
    "                cond = K.expand_dims(cond, 1)\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta_dense(cond) + self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\r\n",
    "        else:\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma\r\n",
    "        outputs = inputs\r\n",
    "        if self.center:\r\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\r\n",
    "            outputs = outputs - mean\r\n",
    "        if self.scale:\r\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\r\n",
    "            std = K.sqrt(variance + self.epsilon)\r\n",
    "            outputs = outputs / std\r\n",
    "            outputs = outputs * gamma\r\n",
    "        if self.center:\r\n",
    "            outputs = outputs + beta\r\n",
    "        return outputs\r\n",
    "        \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(true,pred):\n",
    "    true = tf.cast(true,tf.float32)\n",
    "    loss = K.sum(K.binary_crossentropy(true, pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject(inputs):\r\n",
    "    \"\"\"根据subject_ids从output中取出subject的向量表征\r\n",
    "    \"\"\"\r\n",
    "    output, subject_ids = inputs\r\n",
    "    start = tf.gather(output,subject_ids[:,0],axis=1,batch_dims=0)\r\n",
    "    end = tf.gather(output,subject_ids[:,1],axis=1,batch_dims=0)\r\n",
    "    subject = tf.keras.layers.Concatenate(axis=2)([start, end])\r\n",
    "    return subject[:,0]\r\n",
    "'''\r\n",
    "   output.shape = (None,128,768)\r\n",
    "   subjudec_ids.shape = (None,2)\r\n",
    "   start.shape = (None,None,768)\r\n",
    "   subject.shape = (None,None,1536)\r\n",
    "   subject[:,0].shape = (None,1536)\r\n",
    "   这一部分给出各个变量的shape应该一目了然\r\n",
    "'''\r\n",
    "   \r\n",
    "def build_model_2(pretrained_path, MAX_LEN, p2id):\r\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    s_po_index =  tf.keras.layers.Input((2,), dtype=tf.int32)\r\n",
    "    \r\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, output_hidden_states=True)\r\n",
    "    outputs = bert_model(ids, attention_mask=att)\r\n",
    "    x, _, hidden_states  = outputs[:3]\r\n",
    "    layer_1 = hidden_states[-1]\r\n",
    "    start_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    start_logits = tf.keras.layers.Lambda(lambda x: x**2)(start_logits)\r\n",
    "    \r\n",
    "    end_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    end_logits = tf.keras.layers.Lambda(lambda x: x**2)(end_logits)\r\n",
    "    \r\n",
    "    subject_1 = extract_subject([layer_1,s_po_index])\r\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject_1])\r\n",
    "    \r\n",
    "    op_out_put_start = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_start = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_start)\r\n",
    "    \r\n",
    "    op_out_put_end = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_end = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_end)\r\n",
    "    \r\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[start_logits, end_logits, op_out_put_start, op_out_put_end])\r\n",
    "    model_2 = tf.keras.models.Model(inputs=[ids, att], outputs=[start_logits,end_logits])\r\n",
    "    model_3 = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[op_out_put_start, op_out_put_end])\r\n",
    "    return model, model_2, model_3\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model_2, model_3, id2tag, val_spo_list, val_input_ids, val_attention_mask, tokenizer):\n",
    "        super(Metrics, self).__init__()\n",
    "        self.model_2 = model_2\n",
    "        self.model_3 = model_3\n",
    "        self.id2tag = id2tag\n",
    "        self.val_input_ids = val_input_ids\n",
    "        self.val_attention_mask = val_attention_mask\n",
    "        self.val_spo_list = val_spo_list\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.val_f1s = []\n",
    "        self.best_val_f1 = 0\n",
    "    \n",
    "    def get_same_element_index(self,ob_list):\n",
    "        return [i for (i, v) in enumerate(ob_list) if v == 1]\n",
    "    \n",
    "    def evaluate_data(self):\n",
    "        question=[]\n",
    "        answer=[]\n",
    "        y1 = self.model_2.predict([self.val_input_ids,self.val_attention_mask])\n",
    "        for i in range(len(y1[0])):\n",
    "            for z in self.val_spo_list[i]:\n",
    "                question.append((z['subject'][0],z['subject'][-1],z['predicate'],z['object'][0],z['object'][-1]))\n",
    "            x_ = [self.tokenizer.decode([t]) for t in self.val_input_ids[i]]\n",
    "            x1 = np.array(y1[0][i]>0.5,dtype='int32')\n",
    "            x2 = np.array(y1[1][i]>0.5,dtype='int32')\n",
    "            union = x1 + x2\n",
    "            index_list = self.get_same_element_index(list(union))\n",
    "            start = 0\n",
    "            S_list=[]\n",
    "            while start+1 < len(index_list):\n",
    "                S_list.append((index_list[start], index_list[start+1]+1))\n",
    "                start += 2\n",
    "            for os_s,os_e in S_list:\n",
    "                s_e = ''.join(x_[os_s:os_e])\n",
    "                she = tf.constant(np.array([[os_s,os_e]]))\n",
    "                Y2 = self.model_3.predict([self.val_input_ids[i:i+1], self.val_attention_mask[i:i+1], s_e]) \n",
    "                for m in range(len(self.id2tag)):\n",
    "                    x3 = np.array(Y2[0][0][:,m]>0.4,dtype='int32')\n",
    "                    x4 = np.array(Y2[1][0][:,m]>0.4,dtype='int32')\n",
    "                    if sum(x3)>0 and sum(x4)>0:\n",
    "                        predict = self.id2tag[m]\n",
    "                        union = x3 + x4\n",
    "                        index_list = self.get_same_element_index(list(union))\n",
    "                        start = 0\n",
    "                        P_list=[]\n",
    "                        while start+1 < len(index_list):\n",
    "                            P_list.append((index_list[start],index_list[start+1]+1))\n",
    "                            start += 2\n",
    "                        for os_s,os_e in P_list:\n",
    "                            if os_e>=os_s:\n",
    "                                P = ''.join(x_[os_s:os_e])\n",
    "                                answer.append((S[0],S[-1],predict,P[0],P[-1]))\n",
    "        Q = set(question)\n",
    "        S = set(answer)\n",
    "        f1 = 2*len(Q&S)/(len(Q)+len(S))\n",
    "        return f1\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        _val_f1 = self.evaluate_data()\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        logs['val_f1'] = _val_f1\n",
    "        if _val_f1 > self.best_val_f1:\n",
    "            self.model.save_weights('..model_dirs/fine_tune_relation_extraction'.format(_val_f1))\n",
    "            self.best_val_f1 = _val_f1\n",
    "            print(\"best f1: {}\".format(self.best_val_f1))\n",
    "        else:\n",
    "            print(\"val f1: {}, but not the best f1\".format(_val_f1))\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000166CB27FF28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000166CB27FF28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      " 2161/21639 [=>............................] - ETA: 3:20:38 - loss: 650.0965 - lambda_loss: 35.7846 - lambda_1_loss: 43.1129 - lambda_2_loss: 286.4878 - lambda_3_loss: 284.7110"
     ]
    }
   ],
   "source": [
    "pretrained_path = '../model_dirs/bert-base-chinese'\r\n",
    "MAX_LEN = 128\r\n",
    "# config = BertConfig.from_json_file('../model_dirs/bert-base-chinese/config.json')\r\n",
    "# TFBertModel.from_pretrained(pretrained_path, config=config)\r\n",
    "K.clear_session()\r\n",
    "model,model_2,model_3 = build_model_2(pretrained_path,  MAX_LEN, p2id)\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\r\n",
    "model.compile(loss={'lambda': new_loss,\r\n",
    "                'lambda_1': new_loss,\r\n",
    "                'lambda_2': new_loss,\r\n",
    "                'lambda_3': new_loss},optimizer=optimizer)\r\n",
    "model.fit([input_ids, attention_mask, send_s_po],\\\r\n",
    "          [start_tokens,end_tokens,object_start_tokens,object_end_tokens], \\\r\n",
    "        epochs=3, batch_size=8, callbacks=[Metrics(model_2, model_3 ,id2p, val_spo_list,val_input_ids,val_attention_mask,tokenizer)])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-36ac06f50597>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kill -9 $pid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os \r\n",
    "pid = os.getpid()\r\n",
    "!kill -9 $pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tfs': conda)",
   "name": "python3710jvsc74a57bd0aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}