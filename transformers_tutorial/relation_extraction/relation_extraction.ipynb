{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\r\n",
    "from random import choice\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集整理\r\n",
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(path, nrows=False):\r\n",
    "    if nrows:\r\n",
    "        df = pd.read_json(path, nrows=nrows, lines=True)\r\n",
    "    else:\r\n",
    "        df = pd.read_json(path, lines=True)\r\n",
    "    df = df[['text', 'spo_list']]\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (5000, 2)\n",
      "Validation data size: (5000, 2)\n",
      "Wall time: 365 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "train_path ='../data/百度关系抽取数据集/train_data.json'\r\n",
    "# train_path = '../data/百度关系抽取数据集/experiment.json']\r\n",
    "\r\n",
    "train_data = json_to_df(train_path, nrows=5000)\r\n",
    "print(f'Train data size: {train_data.shape}') #\r\n",
    "\r\n",
    "dev_path = '../data/百度关系抽取数据集/dev_data.json'\r\n",
    "# dev_path =  '../data/百度关系抽取数据集/experiment.json'\r\n",
    "dev_data = json_to_df(dev_path, nrows=5000)\r\n",
    "print(f'Validation data size: {dev_data.shape}') #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spo(spo_list):\r\n",
    "    for spo in spo_list:\r\n",
    "        spo['predicate'] = spo['predicate'].lower()\r\n",
    "        spo['subject'] = spo['subject'].lower()\r\n",
    "        spo['object'] = spo['object'].lower()\r\n",
    "    return spo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(df):\r\n",
    "    df['text'] = df['text'].str.lower()\r\n",
    "    df['spo_list'] = df['spo_list'].apply(clean_spo)\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'predicate': '主演',\n",
       "   'object_type': '人物',\n",
       "   'subject_type': '影视作品',\n",
       "   'object': '周星驰',\n",
       "   'subject': '喜剧之王'}],\n",
       " [{'predicate': '目',\n",
       "   'object_type': '目',\n",
       "   'subject_type': '生物',\n",
       "   'object': '半翅目',\n",
       "   'subject': '茶树茶网蝽'}]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data_clean(train_data)\r\n",
    "dev_data = data_clean(train_data)\r\n",
    "\r\n",
    "train_text = train_data['text'].to_list()\r\n",
    "train_spo = train_data['spo_list'].to_list()\r\n",
    "\r\n",
    "dev_text = dev_data['text'].to_list()\r\n",
    "dev_spo = dev_data['spo_list'].to_list()\r\n",
    "dev_spo[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "def read_schemads(path):\r\n",
    "    predicate_data = pd.read_json(schemads_path, lines=True)\r\n",
    "    id2p = predicate_data['predicate'].drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    p2id = dict(zip(id2p.values(), id2p.keys()))\r\n",
    "    print(f'length of p2id :{len(p2id)}')#\r\n",
    "    print(f'random p2id sample:{random.sample(p2id.items(), 5)}')#\r\n",
    "    return id2p, p2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of p2id :49\n",
      "random p2id sample:[('所在城市', 41), ('简称', 6), ('嘉宾', 45), ('编剧', 20), ('所属专辑', 9)]\n"
     ]
    }
   ],
   "source": [
    "schemads_path = '../data/百度关系抽取数据集/all_50_schemas'\r\n",
    "id2p, p2id = read_schemads(schemads_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_head_idx(pattern, sequence):\r\n",
    "    \"\"\"从sequence中寻找子串pattern\r\n",
    "    如果找到，返回第一个下标；否则返回-1。\r\n",
    "    \"\"\"\r\n",
    "    n = len(pattern)\r\n",
    "    for i in range(len(sequence)):\r\n",
    "        if sequence[i:i + n] == pattern:\r\n",
    "            return i\r\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(df, p2id, tokenizer, max_length):\r\n",
    "    total_token_ids, total_token_type_ids, total_attention_mask = [], [], []\r\n",
    "    total_subject_labels, total_subject_ids, total_object_labels = [], [], []\r\n",
    "    for idx in df.index:\r\n",
    "        row = df.loc[idx]\r\n",
    "        inputs = tokenizer(row['text'], max_length=max_length, padding='max_length', truncation=True)\r\n",
    "        token_ids, token_type_ids, attention_mask = inputs['input_ids'], inputs['token_type_ids'], inputs['attention_mask']\r\n",
    "        # 实体关系token id 位置字典\r\n",
    "        s2op_map = {}\r\n",
    "        for sop_n, spo in enumerate(row['spo_list']):\r\n",
    "            sub_ids = tokenizer.encode(spo['subject'])[1:-1]\r\n",
    "            p_id = p2id[spo['predicate']]\r\n",
    "            obj_ids = tokenizer.encode(spo['object'])[1:-1]\r\n",
    "            # 查找subject 和 object对应的token id 起始索引\r\n",
    "            sub_head_idx = find_head_idx(sub_ids, inputs['input_ids'])\r\n",
    "            obj_head_idx = find_head_idx(obj_ids, inputs['input_ids'])\r\n",
    "            if sub_head_idx != -1 and obj_head_idx != -1:\r\n",
    "                # 获取subject 起始位置的索引和结束位置索引的元组\r\n",
    "                sub = (sub_head_idx, sub_head_idx + len(sub_ids) - 1)\r\n",
    "                # 获取object 起始位置的索引和结束位置索引元组以及与关系标签id的元组对\r\n",
    "                obj = (obj_head_idx, obj_head_idx + len(obj_ids) - 1, p_id)\r\n",
    "                # print('---- sub -----', sub)#    \r\n",
    "                if sub not in s2op_map:\r\n",
    "                    s2op_map[sub] = []\r\n",
    "                s2op_map[sub].append(obj)\r\n",
    "                # print('-----------', s2op_map)#                \r\n",
    "                {(22,23):[(28, 29, 7), (25, 26, 8)]}  \r\n",
    "            else:\r\n",
    "                print('--idx--', idx, '--text--', row['text'])\r\n",
    "                print('--sop_n--', sop_n, '--spo--', spo, '\\n')\r\n",
    "\r\n",
    "\r\n",
    "        if s2op_map:\r\n",
    "        # subject标签\r\n",
    "            subject_labels = np.zeros((max_length, 2))\r\n",
    "            for s in s2op_map:\r\n",
    "                #sub_head\r\n",
    "                subject_labels[s[0], 0] = 1\r\n",
    "                #sub_tail\r\n",
    "                subject_labels[s[1], 1] = 1\r\n",
    "            # 随机选一个subject\r\n",
    "            sub_head, sub_tail = choice(list(s2op_map.keys()))\r\n",
    "            subject_ids = (sub_head, sub_tail)\r\n",
    "            # sub_head, sub_tail = np.array(list(s2op_map.keys())).T\r\n",
    "            # sub_head = np.random.choice(sub_head)\r\n",
    "            # sub_tail = np.random.choice(sub_tail[sub_tail >= sub_head])\r\n",
    "            # 对应的object标签\r\n",
    "            object_labels = np.zeros((len(token_ids), len(p2id), 2))\r\n",
    "            for op in s2op_map.get((sub_head, sub_tail), []):\r\n",
    "                # print(op)\r\n",
    "                # obj_head\r\n",
    "                object_labels[op[0], op[2], 0] = 1\r\n",
    "                # obj_tail\r\n",
    "                object_labels[op[1], op[2], 1] = 1\r\n",
    "\r\n",
    "            # 所有数据汇总\r\n",
    "            total_token_ids.append(token_ids)\r\n",
    "            total_token_type_ids.append(token_type_ids)\r\n",
    "            total_attention_mask.append(attention_mask)\r\n",
    "            total_subject_labels.append(subject_labels)\r\n",
    "            total_subject_ids.append(subject_ids)\r\n",
    "            total_object_labels.append(object_labels)                \r\n",
    "    return total_token_ids, total_token_type_ids, total_attention_mask, \\\r\n",
    "           total_subject_labels, total_subject_ids, total_object_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 27, 14)\n",
      "(14, 14, 17)\n",
      "(28, 31, 21)\n",
      "(1, 256, 49, 2)\n",
      "[{'predicate': '身高', 'object_type': 'Number', 'subject_type': '人物', 'object': '70公分', 'subject': '爱德华·尼科·埃尔南迪斯'}, {'predicate': '出生日期', 'object_type': 'Date', 'subject_type': '人物', 'object': '1986', 'subject': '爱德华·尼科·埃尔南迪斯'}, {'predicate': '国籍', 'object_type': '国家', 'subject_type': '人物', 'object': '哥伦比亚', 'subject': '爱德华·尼科·埃尔南迪斯'}]\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "[{'predicate': '身高', 'object_type': 'Number', 'subject_type': '人物', 'object': '70公分', 'subject': '爱德华·尼科·埃尔南迪斯'}, {'predicate': '出生日期', 'object_type': 'Date', 'subject_type': '人物', 'object': '1986', 'subject': '爱德华·尼科·埃尔南迪斯'}, {'predicate': '国籍', 'object_type': '国家', 'subject_type': '人物', 'object': '哥伦比亚', 'subject': '爱德华·尼科·埃尔南迪斯'}]\n"
     ]
    }
   ],
   "source": [
    "input_demo = data_process(train_data[3:4], p2id, tokenizer, max_length)\r\n",
    "print(np.array(input_demo[5]).shape)\r\n",
    "print(train_spo[3])\r\n",
    "print(np.array(input_demo[5])[0, 25, 14, 0], np.array(input_demo[5])[0, 27, 14, 1])\r\n",
    "print(np.array(input_demo[5])[0, 14, 17, 0], np.array(input_demo[5])[0, 14, 17, 1])\r\n",
    "# tokenizer.decode(train_input_ids[3][op_s:op_e+1])\r\n",
    "print(train_spo[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--idx-- 1178 --text-- ▌1999年：「喜剧之王」前两年的贺岁档其实都有星爷，只不过作品票房一直跟不上\n",
      "--sop_n-- 0 --spo-- {'predicate': '上映时间', 'object_type': 'Date', 'subject_type': '影视作品', 'object': '1999年', 'subject': '喜剧之王'} \n",
      "\n",
      "[(22, 25), (1, 5)]\n",
      "(9715, 256, 2)\n",
      "Wall time: 6.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "model_path = '../model_dirs/bert-base-chinese'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "max_length = 256\r\n",
    "train_input_ids, train_token_type_ids, train_attention_mask, train_subject_labels, train_subject_ids, train_object_labels  = data_process(train_data, p2id, tokenizer, max_length)\r\n",
    "print(train_subject_ids[:2])\r\n",
    "print(np.array(train_subject_labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "val_inputs = tokenizer(dev_text, max_length=max_length, padding='max_length', truncation=True, return_tensors='tf') \r\n",
    "val_input_ids, val_attention_mask = val_inputs['input_ids'], val_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 ▌1999年：「喜剧之王」前两年的贺岁档其实都有星爷，只不过作品票房一直跟不上\n",
      "喜剧之王 1999年\n",
      "[101, 456, 8818, 8653, 2399, 8038, 519, 1599, 1196, 722, 4374, 520, 1184, 697, 2399, 4638, 6590, 2259, 3440, 1071, 2141, 6963, 3300, 3215, 4267, 8024, 1372, 679, 6814, 868, 1501, 4873, 2791, 671, 4684, 6656, 679, 677, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1599, 1196, 722, 4374]\n",
      "[8338, 2399]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, -1)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = 1178\r\n",
    "text = train_data.loc[ix,'text']\r\n",
    "print(len(text), text)\r\n",
    "sub = train_data.loc[ix,'spo_list'][0]['subject']\r\n",
    "obj = train_data.loc[ix,'spo_list'][0]['object']\r\n",
    "print(sub, obj)\r\n",
    "max_length = 256\r\n",
    "s_ids = tokenizer.encode(sub, max_length=max_length, truncation=True)[1:-1]\r\n",
    "o_ids = tokenizer.encode(obj, max_length=max_length, truncation=True)[1:-1]\r\n",
    "text_ids = tokenizer.encode(text, max_length=max_length, padding='max_length', truncation=True)\r\n",
    "print(text_ids)\r\n",
    "print(s_ids)\r\n",
    "print(o_ids)\r\n",
    "find_head_idx(s_ids, text_ids), find_head_idx(o_ids, text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text ---\n",
      " 47                       《头文字d4》是2005年接力出版社出版的图书，作者是重野秀一\n",
      "48          苏州硕诺尔自动化设备有限公司于2014年11月14日在苏州市吴中区市场监督管理局登记成立\n",
      "49     爱德华多·戈塔尔迪1985年7月22日出生于巴西，身高193厘米，惯用脚右脚，主要效力于莱里...\n",
      "50     个人信息姓名：伊万-佩恩 erwan peron (法国)  国籍：法国  性别：男  生日...\n",
      "51     【陈赫告白母亲，张子萱晒女儿萌照，低调示爱的二人获得大家的认可】文/热门电影君（原创文章，谢...\n",
      "52                    严九岳(1574～1621年)，字以赞，号海日，明代福建永安市贡川人\n",
      "53                         张辉，女，心血管内科，汉族，1963年6月8日生，天津市人\n",
      "54                  《是我不小心》是陈明演唱的一首歌曲，收录在专辑《相信你总会被我感动》当中\n",
      "55     覃林盛，湖北力帝机床股份有限公司董事长，在机械制造行业已辛勤耕耘了三十余年，在董事长兼总经理...\n",
      "56                               姚官保，男，1985年10月出生于河南省汝南县\n",
      "57                        2015年10月13日，北京龙泉寺举行装藏法会，贤佳法师主持\n",
      "58                                 刘才光  男，1930年4月生，福建福州人\n",
      "59     尼尔森总部位于美国纽约，并在伊利诺伊州的商堡（schaumburg）、比利时的瓦韦尔（wav...\n",
      "60     孙晓健，男，1964年9月出生，研究生学历，1986年2月加入中国共产党，现任泰兴市粮食系统...\n",
      "61                         《成交百分百》是2003年海潮出版社出版的图书，作者是景斓\n",
      "62               云南展博电气设备有限公司于2012年11月13日在昆明市工商行政管理局登记成立\n",
      "63                0后的第三波主持人：纳豆、夏宇童-(代班2周) 节目改名电玩快打 ver 2\n",
      "64                          《保密局的枪声》是由常彦执导的剧情电影，陈少泽、正华主演\n",
      "65            粟裕大将有3个子女，长子粟戎生、次子粟寒生、女儿粟惠宁，粟裕将3个子女都送到部队锻炼\n",
      "66                        2003年在欧阳震华、佘诗曼主演的《洗冤录ⅱ》中饰演紫霞郡主\n",
      "67              《中国建筑画选1991 （平装）》是1992年07月中国建筑工业出版社出版的图书\n",
      "68          歌曲介绍由大飞作词，深白色作曲，吕绍淳编曲的歌曲《直线》，属于专辑中深具爆发力的一首歌曲\n",
      "69     《java程序设计教程》是2013年清华大学出版社出版的图书，作者是牛晓太、王杰、管涛、李向...\n",
      "70                                      歹培林，1978年出生于云南楚雄\n",
      "71     ”观察人士认为，国家宣传在朝鲜三位领导人金日成(kim il-sung)、金正日(kim j...\n",
      "72                   历史评价李氏朝鲜的创立并非太祖大王李成桂一人之功﹐其五子李芳远功不可没\n",
      "73                                     褐边环翅卷蛾是鳞翅目、卷蛾科的动物\n",
      "74                   黄学博，男，汉族，1987年11月生，籍贯广东阳春，中共党员，大学学历\n",
      "75             《香港客家》是2007年广西师范大学出版的客家区域文化丛书系列图书之一，作者刘义章\n",
      "76                                  褐缘散白蚁，属等翅目，鼻白蚁科，散白蚁属\n",
      "77                      《祖圣》是一部连载于17k小说网的东方玄幻类型的小说，作者是证道\n",
      "78      信息时报讯(记者 蔡慕嘉) 由张翰、张钧甯主演,根据安宁同名小说改编的电视剧《温暖的弦》即将开播\n",
      "79     彼得·盖布瑞尔（peter gabriel，1950年2月13日－）出生地是英国伦敦，是英国音乐家\n",
      "80                   佩科维奇，1986年1月出生于塞尔维亚贝尔格莱德，目前效力菲尔特俱乐部\n",
      "81     “上海华普汽车有限公司”是浙江吉利控股集团有限公司于2002年8月4日兼并原“上海杰士达企业...\n",
      "82     基本资料球员:阮辉煌  生日:1981年1月4日  身高:173cm  球衣号码:3  国籍...\n",
      "83                            阿方索十二世是西班牙女王伊莎贝拉二世之子，生于马德里\n",
      "84                  《风雪古城大院人》是连载于17k小说网的热血青春小说，作者是舒穆禄鄂特恩\n",
      "85     烟花易冷周杰伦 - 跨时代后来的《青花瓷》《烟花易冷》等等，至少，在我听过的华语流行歌曲中，...\n",
      "86     深圳市华之阳科技有限公司成立于2006年，是一家专业从事各类电子类产品以及电脑周边线材的开发...\n",
      "87     漫威和dc电影发展梳理漫威创建于1939年，1961年正式定名为marvel，2010年9月...\n",
      "88     1986年，与平鑫涛成立怡人传播公司，进军电视业界，推出30集电视连续剧《几度夕阳红》12，...\n",
      "89     《我们的少年时代》是由湖南卫视、湖南芒果娱乐有限公司、北京时代峰峻文化艺术发展有限公司、上象...\n",
      "90             戴冉，男，1964年7月生，1986年毕业于大连海运学院航海系，副教授、硕士生导师\n",
      "91     专辑简介张敬轩重新演绎王菀之〈下次爱你〉  由男方承受一遍压抑而无可奈何的爱  张敬轩与王菀...\n",
      "92     宿迁市宿豫区人民法院成立于1949年10月，前身是江苏省宿迁县人民法院、宿迁市（县级）人民法...\n",
      "93             《世界主义的观点》是由华东师范大学出版社出版的一部教育作品，作者是德乌尔里希·贝克\n",
      "94              《儿童学画新概念 5-6岁》是2004-4上海教育出版社出版的图书，作者是史广荣\n",
      "95                          《缘起鬼吹灯》是八加一等于九创作的网络小说，发表于起点网\n",
      "96     《初中物理竞赛热点专题/竞赛热点专题丛书》是2001年湖南师范大学出版社出版的图书，作者是武...\n",
      "97                    《爱情不过是一种普通的玩意》是起点中文网连载的言情小说，作者是孟侠贤\n",
      "98                          个人简介杨明丽，女，汉族，生于1954年3月，陕西安康人\n",
      "99                            《中国与英美国家习俗文化比较》，由浙江大学出版社出版\n",
      "100                        《不分手恋爱》是陌独成伤创作的网络小说，发表于17k小说网\n",
      "Name: text, dtype: object\n",
      "inputs keys --\n",
      " dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input_ids --\n",
      " [[ 101  517 1928 ...    0    0    0]\n",
      " [ 101 5722 2336 ...    0    0    0]\n",
      " [ 101 4263 2548 ...    0    0    0]\n",
      " ...\n",
      " [ 101  702  782 ...    0    0    0]\n",
      " [ 101  517  704 ...    0    0    0]\n",
      " [ 101  517  679 ...    0    0    0]]\n",
      "tokens --\n",
      " [CLS] 《 头 文 字 d4 》 是 2005 年 接 力 出 版 社 出 版 的 图 书 ， 作 者 是 重 野 秀 一 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spo_list ------\n",
      " [{'predicate': '出版社', 'object_type': '出版社', 'subject_type': '书籍', 'object': '接力出版社', 'subject': '头文字d4'}, {'predicate': '作者', 'object_type': '人物', 'subject_type': '图书作品', 'object': '重野秀一', 'subject': '头文字d4'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "## debug\r\n",
    "model_path = '../model_dirs/bert-base-chinese'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "max_length = 128\r\n",
    "print(f\"text ---\\n {train_data.loc[47:100,'text']}\")\r\n",
    "inputs = tokenizer(train_data.loc[47:100,'text'].to_list(), max_length=max_length, padding='max_length', return_tensors='tf', truncation=True)\r\n",
    "print('inputs keys --\\n', inputs.keys())\r\n",
    "print(f\"input_ids --\\n {inputs['input_ids']}\")\r\n",
    "tokens = tokenizer.decode(inputs['input_ids'][0])\r\n",
    "print('tokens --\\n', tokens)\r\n",
    "print(f\"spo_list ------\\n {train_data.loc[47,'spo_list']}\")\r\n",
    "text_len = len(tokens)\r\n",
    "text_len\r\n",
    "bert_model = TFBertModel.from_pretrained(model_path, output_hidden_states=True)\r\n",
    "outputs = bert_model(inputs)\r\n",
    "last_hidden_state, pooler_output, hidden_states = outputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([54, 128, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(54, 1536), dtype=float32, numpy=\n",
       "array([[ 0.25873297,  0.49238703,  0.63826686, ...,  0.11304718,\n",
       "        -0.9661733 , -0.4333823 ],\n",
       "       [ 0.42852157, -0.17608842,  0.8069094 , ...,  1.4964044 ,\n",
       "        -0.49943933, -0.20207542],\n",
       "       [ 0.5664912 , -0.12230435,  0.19252312, ...,  0.5424045 ,\n",
       "         0.27006415,  0.08944741],\n",
       "       ...,\n",
       "       [ 0.46302688, -0.4424666 ,  0.5141795 , ...,  0.4153874 ,\n",
       "         0.5806328 ,  0.04473295],\n",
       "       [-0.11299253, -0.3138919 , -1.4666147 , ...,  0.36222684,\n",
       "         0.43323463,  0.29208207],\n",
       "       [ 0.6611088 ,  0.7782924 , -0.7245413 , ...,  0.38568816,\n",
       "         0.23528749,  0.6906782 ]], dtype=float32)>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_subject(output, subject_ids):\r\n",
    "    start = tf.gather(output, subject_ids[:,:1], batch_dims=1)\r\n",
    "    end = tf.gather(output, subject_ids[:,1:], batch_dims=1)\r\n",
    "    subject = tf.keras.layers.Concatenate(axis=2)([start, end])\r\n",
    "    return subject[:, 0]\r\n",
    "\r\n",
    "extract_subject(hidden_states[-1], tf.constant(train_subject_ids[:hidden_states[-1].shape[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\r\n",
    "    \"\"\"(Conditional) Layer Normalization\r\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        center=True,\r\n",
    "        scale=True,\r\n",
    "        epsilon=None,\r\n",
    "        conditional=False,\r\n",
    "        hidden_units=None,\r\n",
    "        hidden_activation='linear',\r\n",
    "        hidden_initializer='glorot_uniform',\r\n",
    "        **kwargs):\r\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\r\n",
    "        self.center = center\r\n",
    "        self.scale = scale\r\n",
    "        self.conditional = conditional\r\n",
    "        self.hidden_units = hidden_units\r\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\r\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\r\n",
    "        self.epsilon = epsilon or 1e-12\r\n",
    "        \r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        if self.conditional:\r\n",
    "            masks = mask if mask is not None else []\r\n",
    "            masks = [m[None] for m in masks if m is not None]\r\n",
    "            if len(masks) == 0:\r\n",
    "                return None\r\n",
    "            else:\r\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\r\n",
    "        else:\r\n",
    "            return mask\r\n",
    "        \r\n",
    "    def build(self, input_shape):\r\n",
    "        super(LayerNormalization, self).build(input_shape)\r\n",
    "        if self.conditional:\r\n",
    "            shape = (input_shape[0][-1],)\r\n",
    "        else:\r\n",
    "            shape = (input_shape[-1],)\r\n",
    "        if self.center:\r\n",
    "            self.beta = self.add_weight(\r\n",
    "                shape=shape, initializer='zeros', name='beta')\r\n",
    "        if self.scale:\r\n",
    "            self.gamma = self.add_weight(\r\n",
    "                shape=shape, initializer='ones', name='gamma')\r\n",
    "        if self.conditional:\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\r\n",
    "                    units=self.hidden_units,\r\n",
    "                    activation=self.hidden_activation,\r\n",
    "                    use_bias=False,\r\n",
    "                    kernel_initializer=self.hidden_initializer)\r\n",
    "            if self.center:\r\n",
    "                self.beta_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "            if self.scale:\r\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\r\n",
    "        \"\"\"\r\n",
    "        if self.conditional:\r\n",
    "            inputs, cond = inputs\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                cond = self.hidden_dense(cond)\r\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\r\n",
    "                cond = K.expand_dims(cond, 1)\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta_dense(cond) + self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\r\n",
    "        else:\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma\r\n",
    "        outputs = inputs\r\n",
    "        if self.center:\r\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\r\n",
    "            outputs = outputs - mean\r\n",
    "        if self.scale:\r\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\r\n",
    "            std = K.sqrt(variance + self.epsilon)\r\n",
    "            outputs = outputs / std\r\n",
    "            outputs = outputs * gamma\r\n",
    "        if self.center:\r\n",
    "            outputs = outputs + beta\r\n",
    "        return outputs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "def E2EModel(pretrained_path, max_length, p2id):\r\n",
    "    input_ids = tf.keras.layers.Input((max_length,), dtype=tf.int32, name='input_ids')\r\n",
    "    token_type_ids = tf.keras.layers.Input((max_length,), dtype=tf.int32, name='total_segment_ids')\r\n",
    "    attention_mask = tf.keras.layers.Input((max_length,), dtype=tf.int32, name='attention_mask')\r\n",
    "    subject_ids = tf.keras.layers.Input((2,), dtype=tf.int32, name='subject_ids')\r\n",
    "\r\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, output_hidden_states=True)\r\n",
    "    outputs = bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\r\n",
    "    last_hidden_state, pooler_output, hidden_states = outputs[:3]\r\n",
    "    layer_1 = hidden_states[-1]\r\n",
    "    \r\n",
    "    subject_preds = tf.keras.layers.Dense(units=2, activation='sigmoid',)(layer_1)\r\n",
    "    subject_preds = tf.keras.layers.Lambda(lambda x: x**2)(subject_preds)\r\n",
    "    subject_model = tf.keras.models.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=subject_preds)\r\n",
    "\r\n",
    "    subject = extract_subject([layer_1, subject_ids])\r\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject])\r\n",
    "    output = tf.keras.layers.Dense(units=len(p2d) * 2, activation='sigmoid' )(output)\r\n",
    "    output = tf.keras.layers.Lambda(lambda x: x**4)(output)\r\n",
    "    object_preds = tf.reshape((-1, len(p2id), 2))(output)\r\n",
    "    object_model = tf.keras.models.Model(input=[input_ids, token_type_ids, attention_mask, subject_ids], outputs=object_preds)\r\n",
    "\r\n",
    "    train_model = tf.keras.models.Model(input=[input_ids, token_type_ids, attention_mask, subject_ids], outputs= [subject_preds, object_preds])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "pid = os.getpid()\r\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 7 1 2 2]\n",
      " [1 7 3 4 3]\n",
      " [2 7 5 6 6]\n",
      " [3 7 7 8 7]\n",
      " [4 7 7 8 7]\n",
      " [5 7 7 8 7]], shape=(6, 5), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1, 5), dtype=int32, numpy=\n",
       "array([[[0, 7, 1, 2, 2]],\n",
       "\n",
       "       [[1, 7, 3, 4, 3]],\n",
       "\n",
       "       [[2, 7, 5, 6, 6]],\n",
       "\n",
       "       [[3, 7, 7, 8, 7]],\n",
       "\n",
       "       [[4, 7, 7, 8, 7]],\n",
       "\n",
       "       [[5, 7, 7, 8, 7]]])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\r\n",
    "current=np.array([\r\n",
    "        [0,7,1,2,2],\r\n",
    "        [1,7,3,4,3],\r\n",
    "        [2,7,5,6,6],\r\n",
    "        [3,7,7,8,7],\r\n",
    "        [4,7,7,8,7],\r\n",
    "        [5,7,7,8,7]\r\n",
    "])\r\n",
    "\r\n",
    "current =tf.constant(current)\r\n",
    "print(current)\r\n",
    "points_e = tf.expand_dims(current, axis=1)\r\n",
    "points_e "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tfs': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}