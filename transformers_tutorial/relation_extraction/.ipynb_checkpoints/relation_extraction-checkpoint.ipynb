{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查尔斯·阿兰基斯（Charles Aránguiz），1989年4月17日出生于智利圣地亚哥，智利职业足球运动员，司职中场，效力于德国足球甲级联赛勒沃库森足球俱乐部 [{'predicate': '出生地', 'object_type': '地点', 'subject_type': '人物', 'object': '圣地亚哥', 'subject': '查尔斯·阿兰基斯'}, {'predicate': '出生日期', 'object_type': 'Date', 'subject_type': '人物', 'object': '1989年4月17日', 'subject': '查尔斯·阿兰基斯'}]\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    text_list = []\n",
    "    spo_list = []\n",
    "    with open(path, encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            text_list.append(literal_eval(line)['text'])\n",
    "            spo_list.append(literal_eval(line)['spo_list'])\n",
    "    return text_list, spo_list\n",
    "    \n",
    "path='./data/百度关系抽取数据集/dev_data.json'\n",
    "text_list, spo_list = load_data(path)\n",
    "print(text_list[0], spo_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'海拔': 0, '祖籍': 1, '毕业院校': 2, '朝代': 3, '总部地点': 4, '导演': 5, '成立日期': 6, '制片人': 7, '丈夫': 8, '作曲': 9, '上映时间': 10, '出品公司': 11, '专业代码': 12, '邮政编码': 13, '出版社': 14, '号': 15, '气候': 16, '身高': 17, '作者': 18, '连载网站': 19, '主持人': 20, '官方语言': 21, '面积': 22, '出生地': 23, '父亲': 24, '妻子': 25, '主角': 26, '作词': 27, '嘉宾': 28, '母亲': 29, '董事长': 30, '创始人': 31, '所属专辑': 32, '主演': 33, '首都': 34, '占地面积': 35, '目': 36, '修业年限': 37, '简称': 38, '国籍': 39, '改编自': 40, '民族': 41, '歌手': 42, '出生日期': 43, '注册资本': 44, '所在城市': 45, '字': 46, '编剧': 47, '人口数量': 48}\n",
      "{0: '海拔', 1: '祖籍', 2: '毕业院校', 3: '朝代', 4: '总部地点', 5: '导演', 6: '成立日期', 7: '制片人', 8: '丈夫', 9: '作曲', 10: '上映时间', 11: '出品公司', 12: '专业代码', 13: '邮政编码', 14: '出版社', 15: '号', 16: '气候', 17: '身高', 18: '作者', 19: '连载网站', 20: '主持人', 21: '官方语言', 22: '面积', 23: '出生地', 24: '父亲', 25: '妻子', 26: '主角', 27: '作词', 28: '嘉宾', 29: '母亲', 30: '董事长', 31: '创始人', 32: '所属专辑', 33: '主演', 34: '首都', 35: '占地面积', 36: '目', 37: '修业年限', 38: '简称', 39: '国籍', 40: '改编自', 41: '民族', 42: '歌手', 43: '出生日期', 44: '注册资本', 45: '所在城市', 46: '字', 47: '编剧', 48: '人口数量'}\n"
     ]
    }
   ],
   "source": [
    "def load_predicate(path):\n",
    "    with open(path,'r', encoding='utf-8')  as f:\n",
    "        predicate_list = [literal_eval(i)['predicate'] for i in f]\n",
    "    p2id = {}\n",
    "    id2p = {}\n",
    "    data = list(set(predicate_list))\n",
    "    for i in range(len(data)):\n",
    "        p2id[data[i]] = i\n",
    "        id2p[i] = data[i]\n",
    "    return p2id, id2p\n",
    "    \n",
    "path = './data/百度关系抽取数据集/all_50_schemas'\n",
    "p2id, id2p = load_predicate(path)\n",
    "print(p2id)\n",
    "print(id2p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_data(text_list,spo_list,p2id,id2p,tokenizer,max_length):\n",
    "    id_label = {}\n",
    "    ct = len(text_list)\n",
    "    input_ids = np.zeros((ct,max_length),dtype='int32')\n",
    "    attention_mask = np.zeros((ct,max_length),dtype='int32')\n",
    "    start_tokens = np.zeros((ct,max_length),dtype='int32')\n",
    "    end_tokens = np.zeros((ct,max_length),dtype='int32')\n",
    "    send_s_po = np.zeros((ct,2),dtype='int32')\n",
    "    object_start_tokens = np.zeros((ct,max_length,len(p2id)),dtype='int32')\n",
    "    object_end_tokens = np.zeros((ct,max_length,len(p2id)),dtype='int32')\n",
    "    invalid_index = []\n",
    "    for k in range(ct):\n",
    "        context_k = text_list[k].lower().replace(' ','')\n",
    "        enc_context = tokenizer.encode(context_k,max_length=max_length,truncation=True) \n",
    "        if len(spo_list[k])==0:\n",
    "            invalid_index.append(k)\n",
    "            continue\n",
    "        start = []\n",
    "        end = []\n",
    "        S_index = []\n",
    "        for j in range(len(spo_list[k])):\n",
    "            answers_text_k = spo_list[k][j]['subject'].lower().replace(' ','')\n",
    "            chars = np.zeros((len(context_k)))\n",
    "            index = context_k.find(answers_text_k)\n",
    "            chars[index:index+len(answers_text_k)]=1\n",
    "            offsets = []\n",
    "            idx=0\n",
    "            for t in enc_context[1:]:\n",
    "                w = tokenizer.decode([t])\n",
    "                if '#' in w and len(w)>1:\n",
    "                    w = w.replace('#','')\n",
    "                if w == '[UNK]':\n",
    "                    w = '。'\n",
    "                offsets.append((idx,idx+len(w)))\n",
    "                idx += len(w)\n",
    "            toks = []\n",
    "            for i,(a,b) in enumerate(offsets):\n",
    "                sm = np.sum(chars[a:b])\n",
    "                if sm>0: \n",
    "                    toks.append(i) \n",
    "            input_ids[k,:len(enc_context)] = enc_context\n",
    "            attention_mask[k,:len(enc_context)] = 1\n",
    "            if len(toks)>0:\n",
    "                start_tokens[k,toks[0]+1] = 1\n",
    "                end_tokens[k,toks[-1]+1] = 1\n",
    "                start.append(toks[0]+1)\n",
    "                end.append(toks[-1]+1)\n",
    "                S_index.append(j)\n",
    "                #随机抽取可以作为负样本提高准确率（不认同）\n",
    "        if len(start) > 0:\n",
    "            start_np = np.array(start)\n",
    "            end_np = np.array(end)\n",
    "            start_ = np.random.choice(start_np)\n",
    "            end_ = np.random.choice(end_np[end_np >= start_])\n",
    "            send_s_po[k,0] = start_\n",
    "            send_s_po[k,1] = end_\n",
    "            s_index = start.index(start_)\n",
    "            #随机选取object的首位，如果选取错误，则作为负样本\n",
    "            if end_ == end[s_index]:\n",
    "                for index in range(len(start)):\n",
    "                    if start[index] == start_ and end[index] == end_:\n",
    "                        object_text_k = spo_list[k][S_index[index]]['object'].lower().replace(' ','')\n",
    "                        predicate = spo_list[k][S_index[index]]['predicate']\n",
    "                        p_id = p2id[predicate]\n",
    "                        chars = np.zeros((len(context_k)))\n",
    "                        index = context_k.find(object_text_k)\n",
    "                        chars[index:index+len(object_text_k)]=1\n",
    "                        offsets = [] \n",
    "                        idx=0\n",
    "                        for t in enc_context[1:]:\n",
    "                            w = tokenizer.decode([t])\n",
    "                            if '#' in w and len(w)>1:\n",
    "                                w = w.replace('#','')\n",
    "                            if w == '[UNK]':\n",
    "                                w = '。'\n",
    "                            offsets.append((idx,idx+len(w)))\n",
    "                            idx += len(w)\n",
    "                        toks = []\n",
    "                        for i,(a,b) in enumerate(offsets):\n",
    "                            sm = np.sum(chars[a:b])\n",
    "                            if sm>0: \n",
    "                                toks.append(i) \n",
    "                        if len(toks)>0:\n",
    "                            id_label[p_id] = predicate\n",
    "                            object_start_tokens[k,toks[0]+1,p_id] = 1\n",
    "                            object_end_tokens[k,toks[-1]+1,p_id] = 1\n",
    "        else:\n",
    "            invalid_index.append(k)\n",
    "    return input_ids,attention_mask,start_tokens,end_tokens,send_s_po,object_start_tokens,object_end_tokens,invalid_index,id_label\n",
    "\n",
    "max_length = 128  \n",
    "model_path = '../model_dirs/bert-base-chinese'  \n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)    \n",
    "input_ids,attention_mask, start_tokens, end_tokens, send_s_po, object_start_tokens, object_end_tokens, invalid_index, id_label \\\n",
    "= proceed_data(text_list,spo_list,p2id,id2p,tokenizer,max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 101, 3389, 2209, ...,    0,    0,    0],\n",
       "        [ 101,  517, 4895, ...,    0,    0,    0],\n",
       "        [ 101,  517, 2699, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 3636, 3727, ...,    0,    0,    0],\n",
       "        [ 101,  517, 5381, ...,    0,    0,    0],\n",
       "        [ 101,  517, 2769, ...,    0,    0,    0]]),\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def proceed_var_data(text_list,spo_list,tokenizer,max_length):\n",
    "    ct = len(text_list)\n",
    "    input_ids = np.zeros((ct,max_length),dtype='int32')\n",
    "    attention_mask = np.zeros((ct,max_length),dtype='int32')\n",
    "    for k in range(ct):\n",
    "        context_k = text_list[k].lower().replace(' ','')\n",
    "        enc_context = tokenizer.encode(context_k,max_length=max_length,truncation=True) \n",
    "        input_ids[k,:len(enc_context)] = enc_context\n",
    "        attention_mask[k,:len(enc_context)] = 1\n",
    "    return input_ids,attention_mask\n",
    "\n",
    "proceed_var_data(text_list,spo_list,tokenizer,max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"(Conditional) Layer Normalization\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        epsilon=None,\n",
    "        conditional=False,\n",
    "        hidden_units=None,\n",
    "        hidden_activation='linear',\n",
    "        hidden_initializer='glorot_uniform',\n",
    "        **kwargs):\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.conditional = conditional\n",
    "        self.hidden_units = hidden_units\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\n",
    "        self.epsilon = epsilon or 1e-12\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if self.conditional:\n",
    "            masks = mask if mask is not None else []\n",
    "            masks = [m[None] for m in masks if m is not None]\n",
    "            if len(masks) == 0:\n",
    "                return None\n",
    "            else:\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\n",
    "        else:\n",
    "            return mask\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "        if self.conditional:\n",
    "            shape = (input_shape[0][-1],)\n",
    "        else:\n",
    "            shape = (input_shape[-1],)\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape, initializer='zeros', name='beta')\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape, initializer='ones', name='gamma')\n",
    "        if self.conditional:\n",
    "            if self.hidden_units is not None:\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\n",
    "                    units=self.hidden_units,\n",
    "                    activation=self.hidden_activation,\n",
    "                    use_bias=False,\n",
    "                    kernel_initializer=self.hidden_initializer)\n",
    "            if self.center:\n",
    "                self.beta_dense = tf.keras.layers.Dense(\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\n",
    "            if self.scale:\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\n",
    "        \"\"\"\n",
    "        if self.conditional:\n",
    "            inputs, cond = inputs\n",
    "            if self.hidden_units is not None:\n",
    "                cond = self.hidden_dense(cond)\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\n",
    "                cond = K.expand_dims(cond, 1)\n",
    "            if self.center:\n",
    "                beta = self.beta_dense(cond) + self.beta\n",
    "            if self.scale:\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\n",
    "        else:\n",
    "            if self.center:\n",
    "                beta = self.beta\n",
    "            if self.scale:\n",
    "                gamma = self.gamma\n",
    "        outputs = inputs\n",
    "        if self.center:\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\n",
    "            outputs = outputs - mean\n",
    "        if self.scale:\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            outputs = outputs / std\n",
    "            outputs = outputs * gamma\n",
    "        if self.center:\n",
    "            outputs = outputs + beta\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(true,pred):\n",
    "    true = tf.cast(true,tf.float32)\n",
    "    loss = K.sum(K.binary_crossentropy(true, pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject(inputs):\n",
    "    \"\"\"根据subject_ids从output中取出subject的向量表征\n",
    "    \"\"\"\n",
    "    output, subject_ids = inputs\n",
    "    start = tf.gather(output,subject_ids[:,0],axis=1,batch_dims=0)\n",
    "    end = tf.gather(output,subject_ids[:,1],axis=1,batch_dims=0)\n",
    "    subject = tf.keras.layers.Concatenate(axis=2)([start, end])\n",
    "    return subject[:,0]\n",
    "'''\n",
    "   output.shape = (None,128,768)\n",
    "   subjudec_ids.shape = (None,2)\n",
    "   start.shape = (None,None,768)\n",
    "   subject.shape = (None,None,1536)\n",
    "   subject[:,0].shape = (None,1536)\n",
    "   这一部分给出各个变量的shape应该一目了然\n",
    "'''\n",
    "   \n",
    "def build_model_2(pretrained_path, config, MAX_LEN, p2id):\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    s_po_index =  tf.keras.layers.Input((2,), dtype=tf.int32)\n",
    "    \n",
    "    config.output_hidden_states = True\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, config=config)\n",
    "    x, _, hidden_states = bert_model(ids, attention_mask=att)\n",
    "\n",
    "    layer_1 = hidden_states[-1]\n",
    "    \n",
    "    start_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\n",
    "    start_logits = tf.keras.layers.Lambda(lambda x: x**2)(start_logits)\n",
    "    \n",
    "    end_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\n",
    "    end_logits = tf.keras.layers.Lambda(lambda x: x**2)(end_logits)\n",
    "    \n",
    "    subject_1 = extract_subject([layer_1,s_po_index])\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject_1])\n",
    "    \n",
    "    op_out_put_start = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\n",
    "    op_out_put_start = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_start)\n",
    "    \n",
    "    op_out_put_end = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\n",
    "    op_out_put_end = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_end)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[ids,att,s_po_index], outputs=[start_logits,end_logits,op_out_put_start,op_out_put_end])\n",
    "    model_2 = tf.keras.models.Model(inputs=[ids,att], outputs=[start_logits,end_logits])\n",
    "    model_3 = tf.keras.models.Model(inputs=[ids,att,s_po_index], outputs=[op_out_put_start,op_out_put_end])\n",
    "    return model,model_2,model_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,model_2,model_3,id2tag,va_spo_list,va_input_ids,va_attention_mask,tokenizer):\n",
    "        super(Metrics, self).__init__()\n",
    "        self.model_2 = model_2\n",
    "        self.model_3 = model_3\n",
    "        self.id2tag = id2tag\n",
    "        self.va_input_ids = va_input_ids\n",
    "        self.va_attention_mask = va_attention_mask\n",
    "        self.va_spo_list = va_spo_list\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.val_f1s = []\n",
    "        self.best_val_f1 = 0\n",
    "    \n",
    "    def get_same_element_index(self,ob_list):\n",
    "        return [i for (i, v) in enumerate(ob_list) if v == 1]\n",
    "    \n",
    "    def evaluate_data(self):\n",
    "        question=[]\n",
    "        answer=[]\n",
    "        Y1 = self.model_2.predict([self.va_input_ids,self.va_attention_mask])\n",
    "        for i in range(len(Y1[0])):\n",
    "            for z in self.va_spo_list[i]:\n",
    "                question.append((z['subject'][0],z['subject'][-1],z['predicate'],z['object'][0],z['object'][-1]))\n",
    "            x_ = [self.tokenizer.decode([t]) for t in self.va_input_ids[i]]\n",
    "            x1 = np.array(Y1[0][i]>0.5,dtype='int32')\n",
    "            x2 = np.array(Y1[1][i]>0.5,dtype='int32')\n",
    "            union = x1 + x2\n",
    "            index_list = self.get_same_element_index(list(union))\n",
    "            start = 0\n",
    "            S_list=[]\n",
    "            while start+1 < len(index_list):\n",
    "                S_list.append((index_list[start],index_list[start+1]+1))\n",
    "                start += 2\n",
    "            for os_s,os_e in S_list:\n",
    "                S = ''.join(x_[os_s:os_e])\n",
    "                Y2 = self.model_3.predict([[self.va_input_ids[i]],[self.va_attention_mask[i]],np.array([[os_s,os_e]])])\n",
    "                for m in range(len(self.id2tag)):\n",
    "                    x3 = np.array(Y2[0][0][:,m]>0.4,dtype='int32')\n",
    "                    x4 = np.array(Y2[1][0][:,m]>0.4,dtype='int32')\n",
    "                    if sum(x3)>0 and sum(x4)>0:\n",
    "                        predict = self.id2tag[m]\n",
    "                        union = x3 + x4\n",
    "                        index_list = self.get_same_element_index(list(union))\n",
    "                        start = 0\n",
    "                        P_list=[]\n",
    "                        while start+1 < len(index_list):\n",
    "                            P_list.append((index_list[start],index_list[start+1]+1))\n",
    "                            start += 2\n",
    "                        for os_s,os_e in P_list:\n",
    "                            if os_e>=os_s:\n",
    "                                P = ''.join(x_[os_s:os_e])\n",
    "                                answer.append((S[0],S[-1],predict,P[0],P[-1]))\n",
    "        Q = set(question)\n",
    "        S = set(answer)\n",
    "        f1 = 2*len(Q&S)/(len(Q)+len(S))\n",
    "        return f1\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        _val_f1 = self.evaluate_data()\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        logs['val_f1'] = _val_f1\n",
    "        if _val_f1 > self.best_val_f1:\n",
    "            self.model.save_weights('./model_/02_f1={}_model.hdf5'.format(_val_f1))\n",
    "            self.best_val_f1 = _val_f1\n",
    "            print(\"best f1: {}\".format(self.best_val_f1))\n",
    "        else:\n",
    "            print(\"val f1: {}, but not the best f1\".format(_val_f1))\n",
    "        return   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = '../model_dirs/bert-base-chinese/tf_model.h5'\n",
    "MAX_LEN = 128\n",
    "config = BertConfig.from_json_file('../model_dirs/bert-base-chinese/config.json')\n",
    "TFBertModel.from_pretrained(pretrained_path, config=config)\n",
    "K.clear_session()\n",
    "model,model_2,model_3 = build_model_2(pretrained_path, config, MAX_LEN, p2id)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(loss={'lambda': new_loss,\n",
    "                'lambda_1': new_loss,\n",
    "                'lambda_2': new_loss,\n",
    "                'lambda_3': new_loss},optimizer=optimizer)\n",
    "model.fit([input_ids,attention_mask,send_s_po],\\\n",
    "          [start_tokens,end_tokens,object_start_tokens,object_end_tokens], \\\n",
    "        epochs=20, batch_size=32,callbacks=[Metrics(model_2,model_3,id2tag,va_spo_list,va_input_ids,va_attention_mask,tokenizer)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfs]",
   "language": "python",
   "name": "conda-env-tfs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
