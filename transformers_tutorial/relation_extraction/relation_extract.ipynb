{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from ast import literal_eval\r\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(path, nrows=False):\r\n",
    "    if nrows:\r\n",
    "        df = pd.read_json(path, nrows=nrows, lines=True)\r\n",
    "    else:\r\n",
    "        df = pd.read_json(path, lines=True)\r\n",
    "    df = df[['text', 'spo_list']]\r\n",
    "    return df\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spo(spo_list):\r\n",
    "    for spo in spo_list:\r\n",
    "        spo['predicate'] = spo['predicate'].lower()\r\n",
    "        spo['subject'] = spo['subject'].lower()\r\n",
    "        spo['object'] = spo['object'].lower()\r\n",
    "    return spo_list\r\n",
    "\r\n",
    "def data_clean(df):\r\n",
    "    df['text'] = df['text'].str.lower()\r\n",
    "    df['spo_list'] = df['spo_list'].apply(clean_spo)\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (20000, 2)\n",
      "Validation data size: (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_path ='../data/百度关系抽取数据集/train_data.json'\r\n",
    "# train_path = '../data/百度关系抽取数据集/experiment.json']\r\n",
    "\r\n",
    "train_data = json_to_df(train_path, nrows=20000)\r\n",
    "print(f'Train data size: {train_data.shape}') #\r\n",
    "\r\n",
    "dev_path = '../data/百度关系抽取数据集/dev_data.json'\r\n",
    "# dev_path =  '../data/百度关系抽取数据集/experiment.json'\r\n",
    "dev_data = json_to_df(dev_path, nrows=10000)\r\n",
    "print(f'Validation data size: {dev_data.shape}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spo_count 42692\n"
     ]
    }
   ],
   "source": [
    "train_data = data_clean(train_data)\r\n",
    "dev_data = data_clean(train_data)\r\n",
    "\r\n",
    "train_text = train_data['text'].to_list()\r\n",
    "train_spo = train_data['spo_list'].to_list()\r\n",
    "spo_single_count = train_data['spo_list'].apply(lambda x: len(x))\r\n",
    "spo_count = spo_single_count.sum()\r\n",
    "print('spo_count', spo_count)\r\n",
    "                                                \r\n",
    "dev_text = dev_data['text'].to_list()\r\n",
    "dev_spo = dev_data['spo_list'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_schemads(path):\r\n",
    "    predicate_data = pd.read_json(schemads_path, lines=True)\r\n",
    "    id2p = predicate_data['predicate'].drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    p2id = dict(zip(id2p.values(), id2p.keys()))\r\n",
    "    print(f'length of p2id :{len(p2id)}')#\r\n",
    "    print(f'random p2id sample:{random.sample(p2id.items(), 5)}')#\r\n",
    "    return id2p, p2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of p2id :49\n",
      "random p2id sample:[('作者', 42), ('出生日期', 17), ('出版社', 28), ('号', 27), ('总部地点', 2)]\n"
     ]
    }
   ],
   "source": [
    "schemads_path = '../data/百度关系抽取数据集/all_50_schemas'\r\n",
    "id2p, p2id = read_schemads(schemads_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_data(text_list, spo_list, p2id, tokenizer, MAX_LEN, spo_count):\r\n",
    "    id_label = {}\r\n",
    "    ct = len(text_list)\r\n",
    "    MAX_LEN = MAX_LEN\r\n",
    "    input_ids = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    attention_mask = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    start_tokens = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    end_tokens = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    send_s_po = np.zeros((spo_count,2),dtype='int32')\r\n",
    "    object_start_tokens = np.zeros((spo_count,MAX_LEN,len(p2id)),dtype='int32')\r\n",
    "    object_end_tokens = np.zeros((spo_count,MAX_LEN,len(p2id)),dtype='int32')\r\n",
    "    index_vaild = -1\r\n",
    "    for k in range(ct):\r\n",
    "        context_k = text_list[k].lower().replace(' ','')\r\n",
    "        enc_context = tokenizer.encode(context_k,max_length=MAX_LEN,truncation=True) \r\n",
    "        if len(spo_list[k])==0:\r\n",
    "            continue          \r\n",
    "        start = []\r\n",
    "        S_index = []\r\n",
    "        for j in range(len(spo_list[k])):\r\n",
    "            answers_text_k = spo_list[k][j]['subject'].lower().replace(' ','')\r\n",
    "            chars = np.zeros((len(context_k)))\r\n",
    "            index = context_k.find(answers_text_k)\r\n",
    "            chars[index:index+len(answers_text_k)]=1\r\n",
    "            offsets = []\r\n",
    "            idx=0\r\n",
    "            for t in enc_context[1:]:\r\n",
    "                w = tokenizer.decode([t])\r\n",
    "                if '#' in w and len(w)>1:\r\n",
    "                    w = w.replace('#','')\r\n",
    "                if w == '[UNK]':\r\n",
    "                    w = '。'\r\n",
    "                offsets.append((idx,idx+len(w)))\r\n",
    "                idx += len(w)\r\n",
    "            toks = []\r\n",
    "            for i,(a,b) in enumerate(offsets):\r\n",
    "                sm = np.sum(chars[a:b])\r\n",
    "                if sm>0: \r\n",
    "                    toks.append(i) \r\n",
    "            if len(toks)>0:\r\n",
    "                S_start = toks[0]+1\r\n",
    "                S_end = toks[-1]+1\r\n",
    "                if (S_start,S_end) not in start:\r\n",
    "                    index_vaild += 1\r\n",
    "                    start.append((S_start,S_end))\r\n",
    "                    input_ids[index_vaild,:len(enc_context)] = enc_context\r\n",
    "                    attention_mask[index_vaild,:len(enc_context)] = 1\r\n",
    "                    start_tokens[index_vaild,S_start] = 1\r\n",
    "                    end_tokens[index_vaild,S_end] = 1\r\n",
    "                    send_s_po[index_vaild,0] = S_start\r\n",
    "                    send_s_po[index_vaild,1] = S_end\r\n",
    "                    S_index.append([j,index_vaild])\r\n",
    "                else:\r\n",
    "                    S_index.append([j,index_vaild])\r\n",
    "        if len(S_index) > 0:\r\n",
    "            for index_ in range(len(S_index)):\r\n",
    "                #随机选取object的首位，如果选取错误，则作为负样本\r\n",
    "                object_text_k = spo_list[k][S_index[index_][0]]['object'].lower().replace(' ','')\r\n",
    "                predicate = spo_list[k][S_index[index_][0]]['predicate']\r\n",
    "                p_id = p2id[predicate]\r\n",
    "                chars = np.zeros((len(context_k)))\r\n",
    "                index = context_k.find(object_text_k)\r\n",
    "                chars[index:index+len(object_text_k)]=1\r\n",
    "                offsets = [] \r\n",
    "                idx = 0\r\n",
    "                for t in enc_context[1:]:\r\n",
    "                    w = tokenizer.decode([t])\r\n",
    "                    if '#' in w and len(w)>1:\r\n",
    "                        w = w.replace('#','')\r\n",
    "                    if w == '[UNK]':\r\n",
    "                        w = '。'\r\n",
    "                    offsets.append((idx,idx+len(w)))\r\n",
    "                    idx += len(w)\r\n",
    "                toks = []\r\n",
    "                for i,(a,b) in enumerate(offsets):\r\n",
    "                    sm = np.sum(chars[a:b])\r\n",
    "                    if sm>0: \r\n",
    "                        toks.append(i) \r\n",
    "                if len(toks)>0:\r\n",
    "                    id_label[p_id] = predicate\r\n",
    "                    P_start = toks[0]+1\r\n",
    "                    P_end = toks[-1]+1\r\n",
    "                    object_start_tokens[S_index[index_][1]][P_start,p_id] = 1\r\n",
    "                    object_end_tokens[S_index[index_][1]][P_end,p_id] = 1\r\n",
    "    return input_ids[:index_vaild], attention_mask[:index_vaild], start_tokens[:index_vaild], end_tokens[:index_vaild], send_s_po[:index_vaild], \\\r\n",
    "           object_start_tokens[:index_vaild], object_end_tokens[:index_vaild], id_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22684, 128)\n"
     ]
    }
   ],
   "source": [
    "max_length = 128  \r\n",
    "model_path = '../model_dirs/bert-base-chinese'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "input_ids, attention_mask, start_tokens, end_tokens, send_s_po, object_start_tokens, object_end_tokens, id_label \\\r\n",
    "= proceed_data(train_text, train_spo, p2id, tokenizer, max_length, spo_count)\r\n",
    "\r\n",
    "print(start_tokens.shape)\r\n",
    "\r\n",
    "val_inputs = tokenizer(dev_text, max_length=max_length, padding='max_length', truncation=True, return_tensors='tf') \r\n",
    "val_input_ids, val_attention_mask = val_inputs['input_ids'], val_inputs['attention_mask']\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(true,pred):\r\n",
    "    true = tf.cast(true,tf.float32)\r\n",
    "    loss = K.sum(K.binary_crossentropy(true, pred))\r\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\r\n",
    "    \"\"\"(Conditional) Layer Normalization\r\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        center=True,\r\n",
    "        scale=True,\r\n",
    "        epsilon=None,\r\n",
    "        conditional=False,\r\n",
    "        hidden_units=None,\r\n",
    "        hidden_activation='linear',\r\n",
    "        hidden_initializer='glorot_uniform',\r\n",
    "        **kwargs):\r\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\r\n",
    "        self.center = center\r\n",
    "        self.scale = scale\r\n",
    "        self.conditional = conditional\r\n",
    "        self.hidden_units = hidden_units\r\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\r\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\r\n",
    "        self.epsilon = epsilon or 1e-12\r\n",
    "        \r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        if self.conditional:\r\n",
    "            masks = mask if mask is not None else []\r\n",
    "            masks = [m[None] for m in masks if m is not None]\r\n",
    "            if len(masks) == 0:\r\n",
    "                return None\r\n",
    "            else:\r\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\r\n",
    "        else:\r\n",
    "            return mask\r\n",
    "        \r\n",
    "    def build(self, input_shape):\r\n",
    "        super(LayerNormalization, self).build(input_shape)\r\n",
    "        if self.conditional:\r\n",
    "            shape = (input_shape[0][-1],)\r\n",
    "        else:\r\n",
    "            shape = (input_shape[-1],)\r\n",
    "        if self.center:\r\n",
    "            self.beta = self.add_weight(\r\n",
    "                shape=shape, initializer='zeros', name='beta')\r\n",
    "        if self.scale:\r\n",
    "            self.gamma = self.add_weight(\r\n",
    "                shape=shape, initializer='ones', name='gamma')\r\n",
    "        if self.conditional:\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\r\n",
    "                    units=self.hidden_units,\r\n",
    "                    activation=self.hidden_activation,\r\n",
    "                    use_bias=False,\r\n",
    "                    kernel_initializer=self.hidden_initializer)\r\n",
    "            if self.center:\r\n",
    "                self.beta_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "            if self.scale:\r\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\r\n",
    "        \"\"\"\r\n",
    "        if self.conditional:\r\n",
    "            inputs, cond = inputs\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                cond = self.hidden_dense(cond)\r\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\r\n",
    "                cond = K.expand_dims(cond, 1)\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta_dense(cond) + self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\r\n",
    "        else:\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma\r\n",
    "        outputs = inputs\r\n",
    "        if self.center:\r\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\r\n",
    "            outputs = outputs - mean\r\n",
    "        if self.scale:\r\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\r\n",
    "            std = K.sqrt(variance + self.epsilon)\r\n",
    "            outputs = outputs / std\r\n",
    "            outputs = outputs * gamma\r\n",
    "        if self.center:\r\n",
    "            outputs = outputs + beta\r\n",
    "        return outputs\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject(inputs):\r\n",
    "    \"\"\"根据subject_ids从output中取出subject的向量表征\r\n",
    "    \"\"\"\r\n",
    "    output, subject_ids = inputs\r\n",
    "    start = tf.gather(output,subject_ids[:,0],axis=1,batch_dims=1)\r\n",
    "    end = tf.gather(output,subject_ids[:,1],axis=1,batch_dims=1)\r\n",
    "    subject = tf.keras.layers.Concatenate(axis=1)([start, end])\r\n",
    "    return subject\r\n",
    "'''\r\n",
    "   output.shape = (None,128,768)\r\n",
    "   subjudec_ids.shape = (None,2)\r\n",
    "   start.shape = (None,None,768)\r\n",
    "   subject.shape = (None,None,1536)\r\n",
    "   subject[:,0].shape = (None,1536)\r\n",
    "   这一部分给出各个变量的shape应该一目了然\r\n",
    "'''\r\n",
    "   \r\n",
    "def build_model_2(pretrained_path, MAX_LEN, p2id):\r\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    s_po_index =  tf.keras.layers.Input((2,), dtype=tf.int32)\r\n",
    "    \r\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, output_hidden_states=True)\r\n",
    "    outputs = bert_model(ids, attention_mask=att)\r\n",
    "    x, _, hidden_states  = outputs[:3]\r\n",
    "    layer_1 = hidden_states[-1]\r\n",
    "    start_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    start_logits = tf.keras.layers.Lambda(lambda x: x**2)(start_logits)\r\n",
    "    \r\n",
    "    end_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    end_logits = tf.keras.layers.Lambda(lambda x: x**2)(end_logits)\r\n",
    "    \r\n",
    "    subject_1 = extract_subject([layer_1,s_po_index])\r\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject_1])\r\n",
    "    \r\n",
    "    op_out_put_start = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_start = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_start)\r\n",
    "    \r\n",
    "    op_out_put_end = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_end = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_end)\r\n",
    "    \r\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[start_logits, end_logits, op_out_put_start, op_out_put_end])\r\n",
    "    model_2 = tf.keras.models.Model(inputs=[ids, att], outputs=[start_logits,end_logits])\r\n",
    "    model_3 = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[op_out_put_start, op_out_put_end])\r\n",
    "    return model, model_2, model_3\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rematch_text_word(tokenizer,text,enc_context,enc_start,enc_end):\r\n",
    "    span = [a.span()[0] for a in re.finditer(' ', text)]\r\n",
    "    decode_list = [tokenizer.decode([i]) for i in enc_context][1:]\r\n",
    "    start = 0\r\n",
    "    end = 0\r\n",
    "    len_start = 0\r\n",
    "    for i in range(len(decode_list)):\r\n",
    "        if i ==  enc_start - 1:\r\n",
    "            start = len_start\r\n",
    "        j = decode_list[i]\r\n",
    "        if '#' in j and len(j)>1:\r\n",
    "            j = j.replace('#','')\r\n",
    "        if j == '[UNK]':\r\n",
    "            j = '。'\r\n",
    "        len_start += len(j)\r\n",
    "        if i == enc_end - 1:\r\n",
    "            end = len_start\r\n",
    "            break\r\n",
    "    for span_index in span:\r\n",
    "        if start >= span_index:\r\n",
    "            start += 1\r\n",
    "            end += 1\r\n",
    "        if end > span_index and span_index>start:\r\n",
    "            end += 1\r\n",
    "    return text[start:end]\r\n",
    "\r\n",
    "\r\n",
    "class Metrics(tf.keras.callbacks.Callback):\r\n",
    "    def __init__(self,model_2, model_3, id2tag, va_text_list, va_spo_list, va_input_ids, va_attention_mask, tokenizer):\r\n",
    "        super(Metrics, self).__init__()\r\n",
    "        self.model_2 = model_2\r\n",
    "        self.model_3 = model_3\r\n",
    "        self.id2tag = id2tag\r\n",
    "        self.va_input_ids = va_input_ids\r\n",
    "        self.va_attention_mask = va_attention_mask\r\n",
    "        self.va_spo_list = va_spo_list\r\n",
    "        self.va_text_list = va_text_list\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        \r\n",
    "    def on_train_begin(self, logs=None):\r\n",
    "        self.val_f1s = []\r\n",
    "        self.best_val_f1 = 0\r\n",
    "    \r\n",
    "    def get_same_element_index(self,ob_list):\r\n",
    "        return [i for (i, v) in enumerate(ob_list) if v == 1]\r\n",
    "    \r\n",
    "    def evaluate_data(self):\r\n",
    "        Y1 = self.model_2.predict([self.va_input_ids,self.va_attention_mask])\r\n",
    "        question=[]\r\n",
    "        answer=[]\r\n",
    "        for m in range(len(Y1[0])):\r\n",
    "            for z in self.va_spo_list[m]:\r\n",
    "                question.append((z['subject'],z['predicate'],z['object']))\r\n",
    "            start = np.where(Y1[0][m]>0.5)[0]\r\n",
    "            end = np.where(Y1[1][m]>0.5)[0]\r\n",
    "            subjects = []\r\n",
    "            for i in start:\r\n",
    "                j = end[end >= i]\r\n",
    "                if len(j) > 0:\r\n",
    "                    j = j[0]\r\n",
    "                    subjects.append((i, j))\r\n",
    "            if subjects:\r\n",
    "                token_ids_2 = np.repeat([self.va_input_ids[m]], len(subjects), 0)\r\n",
    "                attention_mask_2 = np.repeat([self.va_attention_mask[m]], len(subjects), 0)\r\n",
    "                subjects = np.array(subjects)\r\n",
    "                object_preds_start,object_preds_end = self.model_3.predict([token_ids_2, attention_mask_2, subjects])\r\n",
    "                for subject,object_start,object_end in zip(subjects,object_preds_start,object_preds_end):\r\n",
    "                    sub = rematch_text_word(self.tokenizer,self.va_text_list[m],self.va_input_ids[m],subject[0],subject[1])\r\n",
    "                    start = np.argwhere(object_start > 0.5)\r\n",
    "                    end = np.argwhere(object_end > 0.5)\r\n",
    "                    for _start, predicate1 in start:\r\n",
    "                        for _end, predicate2 in end:\r\n",
    "                            if _start <= _end and predicate1 == predicate2:\r\n",
    "                                ans = rematch_text_word(self.tokenizer,self.va_text_list[m],self.va_input_ids[m],_start,_end)\r\n",
    "                                answer.append((sub,self.id2tag[predicate1],ans))\r\n",
    "                                break\r\n",
    "        Q = set(question)\r\n",
    "        S = set(answer)\r\n",
    "        f1 = 2*len(Q&S)/(len(Q)+len(S))\r\n",
    "        return f1\r\n",
    "    \r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        logs = logs or {}\r\n",
    "        _val_f1 = self.evaluate_data()\r\n",
    "        self.val_f1s.append(_val_f1)\r\n",
    "        logs['val_f1'] = _val_f1\r\n",
    "        if _val_f1 > self.best_val_f1:\r\n",
    "#             self.model.save_weights('./model_/09_f1={}_model.hdf5'.format(_val_f1))\r\n",
    "            self.best_val_f1 = _val_f1\r\n",
    "            print(\"best f1: {} \\n\".format(self.best_val_f1))\r\n",
    "        else:\r\n",
    "            print(\"val f1: {}, but not the best f1 \\n\".format(_val_f1))\r\n",
    "        return      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_path = '../model_dirs/bert-base-chinese'\r\n",
    "# MAX_LEN = 128\r\n",
    "# # config = BertConfig.from_json_file('../model_dirs/bert-base-chinese/config.json')\r\n",
    "# # TFBertModel.from_pretrained(pretrained_path, config=config)\r\n",
    "# K.clear_session()\r\n",
    "# model,model_2,model_3 = build_model_2(pretrained_path,  MAX_LEN, p2id)\r\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\r\n",
    "# model.compile(loss={'lambda': new_loss,\r\n",
    "#                 'lambda_1': new_loss,\r\n",
    "#                 'lambda_2': new_loss,\r\n",
    "#                 'lambda_3': new_loss},optimizer=optimizer)\r\n",
    "# model.fit([input_ids, attention_mask, send_s_po], [start_tokens, end_tokens, object_start_tokens, object_end_tokens], \\\r\n",
    "#         epochs=10, batch_size=8, callbacks=[Metrics(model_2, model_3 ,id2p, dev_text, dev_spo, val_input_ids, val_attention_mask, tokenizer)])\r\n",
    "\r\n",
    "# h5_path = '../model_dirs/fine_tune_relation_extraction/tf_model.h5'\r\n",
    "# model.save_weights(h5_path)\r\n",
    "# checkpoint_path = '../model_dirs/fine_tune_relation_extraction/checkpoints/my_checkpoint'\r\n",
    "# model.save_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001AF36104F28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001AF36104F28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = '../model_dirs/bert-base-chinese'\r\n",
    "checkpoint_path = '../model_dirs/fine_tune_relation_extraction/checkpoints/my_checkpoint'\r\n",
    "h5_path = '../model_dirs/fine_tune_relation_extraction/tf_model.h5'\r\n",
    "MAX_LEN = 128\r\n",
    "model,model_2,model_3 = build_model_2(pretrained_path, MAX_LEN, p2id)\r\n",
    "model.load_weights(h5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 3\r\n",
    "x1 = [input_ids[[idx]], attention_mask[[idx]]]\r\n",
    "sub_start_tokens, sub_end_tokens = model_2.predict(x1)\r\n",
    "sub_start_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'predicate': '身高',\n",
       "   'object_type': 'Number',\n",
       "   'subject_type': '人物',\n",
       "   'object': '70公分',\n",
       "   'subject': '爱德华·尼科·埃尔南迪斯'},\n",
       "  {'predicate': '出生日期',\n",
       "   'object_type': 'Date',\n",
       "   'subject_type': '人物',\n",
       "   'object': '1986',\n",
       "   'subject': '爱德华·尼科·埃尔南迪斯'},\n",
       "  {'predicate': '国籍',\n",
       "   'object_type': '国家',\n",
       "   'subject_type': '人物',\n",
       "   'object': '哥伦比亚',\n",
       "   'subject': '爱德华·尼科·埃尔南迪斯'}],\n",
       " 14)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spo[idx], p2id[train_spo[idx][0]['predicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'爱德华·尼科·埃尔南迪斯'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_start_idx = int(np.argwhere(sub_start_tokens[0,:,0] > 0.5)[0])\r\n",
    "sub_end_idx = int(np.argwhere(sub_end_tokens[0,:,0] > 0.5)[0])\r\n",
    "sub_text = tokenizer.decode(input_ids[idx][sub_start_idx:sub_end_idx+1]).replace(' ','')\r\n",
    "sub_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 49)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = [input_ids[[idx]], attention_mask[[idx]], send_s_po[[idx]]]\r\n",
    "obj_start_tokens, obj_end_tokens = model_3.predict(x2)\r\n",
    "obj_start_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 14 17\n",
      "1986 出生日期\n",
      "25 27 14\n",
      "70公分 身高\n",
      "28 31 21\n",
      "哥伦比亚 国籍\n"
     ]
    }
   ],
   "source": [
    "obj_start_idx = np.argwhere(obj_start_tokens[0] > 0.5)\r\n",
    "obj_end_idx = np.argwhere(obj_end_tokens[0] > 0.5)\r\n",
    "for _start, predicate1 in obj_start_idx:\r\n",
    "    for _end, predicate2 in obj_end_idx:\r\n",
    "        if _start <= _end and predicate1 == predicate2:\r\n",
    "            print(_start, _end, predicate1)\r\n",
    "            obj_text = tokenizer.decode(input_ids[idx][_start:_end+1]).replace(' ','')\r\n",
    "            print(obj_text, id2p[predicate1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tfs': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}