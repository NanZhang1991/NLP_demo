{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "# from ast import literal_eval\r\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "import re\r\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集整理\r\n",
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(path, nrows=False):\r\n",
    "    if nrows:\r\n",
    "        df = pd.read_json(path, nrows=nrows, lines=True)\r\n",
    "    else:\r\n",
    "        df = pd.read_json(path, lines=True)\r\n",
    "    df = df[['text', 'spo_list']]\r\n",
    "    return df\r\n",
    "\r\n",
    "def merge_df(dir_path):\r\n",
    "    total_df = pd.DataFrame()\r\n",
    "    for fn in os.listdir(dir_path):\r\n",
    "        df = json_to_df(os.path.join(dir_path, fn))\r\n",
    "        df_fn = fn[:fn.rfind('.')]\r\n",
    "        df.insert(0, 'fn', df_fn)\r\n",
    "        total_df =  total_df.append(df)\r\n",
    "    total_df.reset_index(drop=True, inplace=True)\r\n",
    "    print(f'original data size: {total_df.shape}') #\r\n",
    "    print(f'original data sample: {df.sample(5)}')\r\n",
    "    return total_df   \r\n",
    "\r\n",
    "def read_schemads(path_or_df):\r\n",
    "    if not isinstance(path_or_df, pd.DataFrame):\r\n",
    "        print(1)\r\n",
    "        schemads_path = path_or_df\r\n",
    "        predicate_data = pd.read_json(schemads_path, lines=True)\r\n",
    "        id2p = predicate_data['predicate'].drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    else:\r\n",
    "        df = path_or_df\r\n",
    "        id2p = df['spo_list'].apply(lambda spo_list: [spo['predicate'] for spo in spo_list])\r\n",
    "        id2p = id2p.explode().drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    p2id = dict(zip(id2p.values(), id2p.keys()))\r\n",
    "    print(f'length of p2id :{len(p2id)}')#\r\n",
    "    print(f'random p2id sample:{random.sample(p2id.items(), 5)}')#\r\n",
    "    return id2p, p2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 百度三元组关系数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path ='../data/百度关系抽取数据集/train_data.json'\r\n",
    "# train_data = json_to_df(train_path, nrows=10000)\r\n",
    "# print(f'Train data size: {train_data.shape}') #\r\n",
    "\r\n",
    "# dev_path = '../data/百度关系抽取数据集/dev_data.json'\r\n",
    "# dev_data = json_to_df(dev_path, nrows=5000)\r\n",
    "# print(f'Validation data size: {dev_data.shape}') \r\n",
    "\r\n",
    "# schemads_path = '../data/百度关系抽取数据集/all_50_schemas'\r\n",
    "# id2p, p2id = read_schemads(schemads_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 招股说明书三元组数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data size: (10698, 3)\n",
      "original data sample:                    fn                                               text  \\\n",
      "40   阿科力首次公开发行股票招股说明书  晨化股份综合毛利率相对公司更低，这主要是由于其高毛利率的聚醚胺产品销售规模较小所致。2014...   \n",
      "112  阿科力首次公开发行股票招股说明书  兰州阿科力成立于2000年4月29日，注册号为6201022101300，注册资本为50万元...   \n",
      "116  阿科力首次公开发行股票招股说明书  2017年6月末，公司货币资金较上年减少1,385.96万元，减少了21.53%，主要原因是...   \n",
      "104  阿科力首次公开发行股票招股说明书  朱萌，男，1989年11月出生，中国国籍，无境外永久居留权，为实际控制人朱学军、崔小丽夫妇之...   \n",
      "8    阿科力首次公开发行股票招股说明书  中山联动第一期股权投资中心（有限合伙）：成立于2011年1月26日，统一社会信用代码为914...   \n",
      "\n",
      "                                              spo_list  \n",
      "40   [{'predicate': '营业收入', 'object_type': '金额', 's...  \n",
      "112  [{'predicate': '成立日期', 'object_type': '日期', 's...  \n",
      "116  [{'predicate': '货币资金', 'object_type': '金额', 's...  \n",
      "104  [{'predicate': '出生日期', 'object_type': '日期', 's...  \n",
      "8    [{'predicate': '成立日期', 'object_type': '日期', 's...  \n",
      "length of p2id :116\n",
      "random p2id sample:[('应收账款余额', 67), ('其他流动资产', 65), ('营业外支出', 31), ('短期借款', 44), ('出具日期', 3)]\n"
     ]
    }
   ],
   "source": [
    "dir_path = '../data/招股说明书三元组数据集'\r\n",
    "df = merge_df(dir_path)\r\n",
    "id2p, p2id = read_schemads(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spo(spo_list):\r\n",
    "    for spo in spo_list:\r\n",
    "        spo['predicate'] = spo['predicate'].lower()\r\n",
    "        spo['subject'] = spo['subject'].lower()\r\n",
    "        spo['object'] = spo['object'].lower()\r\n",
    "    return spo_list\r\n",
    "\r\n",
    "def data_clean(df):\r\n",
    "    df.dropna(how='any', inplace=True)\r\n",
    "    df = df[df['spo_list'].apply(lambda x: len(x)>0)]\r\n",
    "    df.drop_duplicates(subset=['text'], inplace=True)\r\n",
    "    df.reset_index(drop=True, inplace=True)\r\n",
    "    df['text'] = df['text'].str.lower()\r\n",
    "    df['spo_list'] = df['spo_list'].apply(clean_spo)\r\n",
    "    print(f'Real data size is {df.shape[0]}')\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real data size is 8436\n"
     ]
    }
   ],
   "source": [
    "df = data_clean(df)\r\n",
    "# train_data = data_clean(train_data)\r\n",
    "# dev_data = data_clean(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (10698, 3)\n",
      "Validation data size: (10698, 3)\n",
      "spo_count 30498\n"
     ]
    }
   ],
   "source": [
    "train_size=0.9\r\n",
    "\r\n",
    "# train_data = df.sample(frac=train_size,random_state=200)\r\n",
    "# dev_data = df.drop(train_data.index)\r\n",
    "train_data = df\r\n",
    "dev_data = df \r\n",
    "dev_data.reset_index(drop=True, inplace=True)\r\n",
    "train_data.reset_index(drop=True, inplace=True)\r\n",
    "print(f'Train data size: {train_data.shape}') #\r\n",
    "print(f'Validation data size: {dev_data.shape}') \r\n",
    "\r\n",
    "spo_single_count = df['spo_list'].apply(lambda x: len(x))\r\n",
    "spo_count = spo_single_count.sum()\r\n",
    "print('spo_count', spo_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_data['text'].to_list()\r\n",
    "train_spo = train_data['spo_list'].to_list()\r\n",
    "\r\n",
    "\r\n",
    "dev_text = dev_data['text'].to_list()\r\n",
    "dev_spo = dev_data['spo_list'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签集分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       spo_list  count  compliance \n",
      "0  主营业务成本-日期-金额     33            0\n",
      "1  主营业务收入-日期-金额    233            1\n",
      "2    任职公司-人物-公司   3243            0\n",
      "3    任职公司-人物-日期      1            0\n",
      "4    任职日期-人物-日期   3160            0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2160x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "\r\n",
    "dataset_name = '招股说明书'\r\n",
    "spo = df['spo_list'].explode().reset_index(drop=True)\r\n",
    "spo_group = spo.apply(lambda x: '-'.join([x['predicate'] , x['subject_type'], x['object_type']]))\r\n",
    "spo_group_count = spo_group.groupby(spo_group).count().reset_index(name='count')\r\n",
    "spo_group_count['compliance '] = spo_group_count['count'].apply(lambda x: 1 if 200<x<500 else 0)\r\n",
    "print(spo_group_count.head())\r\n",
    "spo_group_count.to_csv('../data/' + dataset_name + '_spo_group_count.csv', index=False, encoding='utf_8_sig')\r\n",
    "spo_group_count.plot(kind='bar', figsize=(30,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_data(text_list, spo_list, p2id, tokenizer, MAX_LEN, spo_count):\r\n",
    "    id_label = {}\r\n",
    "    ct = len(text_list)\r\n",
    "    MAX_LEN = MAX_LEN\r\n",
    "    input_ids = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    attention_mask = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    start_tokens = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    end_tokens = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    send_s_po = np.zeros((spo_count,2),dtype='int32')\r\n",
    "    object_start_tokens = np.zeros((spo_count,MAX_LEN,len(p2id)),dtype='int32')\r\n",
    "    object_end_tokens = np.zeros((spo_count,MAX_LEN,len(p2id)),dtype='int32')\r\n",
    "    index_vaild = -1\r\n",
    "    for k in range(ct):\r\n",
    "        context_k = text_list[k].lower().replace(' ','')\r\n",
    "        enc_context = tokenizer.encode(context_k,max_length=MAX_LEN,truncation=True)      \r\n",
    "        start = []\r\n",
    "        S_index = []\r\n",
    "        for j in range(len(spo_list[k])):\r\n",
    "            answers_text_k = spo_list[k][j]['subject'].lower().replace(' ','')\r\n",
    "            chars = np.zeros((len(context_k)))\r\n",
    "            index = context_k.find(answers_text_k)\r\n",
    "            chars[index:index+len(answers_text_k)]=1\r\n",
    "            offsets = []\r\n",
    "            idx=0\r\n",
    "            for t in enc_context[1:]:\r\n",
    "                w = tokenizer.decode([t])\r\n",
    "                if '#' in w and len(w)>1:\r\n",
    "                    w = w.replace('#','')\r\n",
    "                if w == '[UNK]':\r\n",
    "                    w = '。'\r\n",
    "                offsets.append((idx,idx+len(w)))\r\n",
    "                idx += len(w)\r\n",
    "            toks = []\r\n",
    "            for i,(a,b) in enumerate(offsets):\r\n",
    "                sm = np.sum(chars[a:b])\r\n",
    "                if sm>0: \r\n",
    "                    toks.append(i) \r\n",
    "            if len(toks)>0:\r\n",
    "                S_start = toks[0]+1\r\n",
    "                S_end = toks[-1]+1\r\n",
    "                if (S_start,S_end) not in start:\r\n",
    "                    index_vaild += 1\r\n",
    "                    start.append((S_start,S_end))\r\n",
    "                    input_ids[index_vaild,:len(enc_context)] = enc_context\r\n",
    "                    attention_mask[index_vaild,:len(enc_context)] = 1\r\n",
    "                    start_tokens[index_vaild,S_start] = 1\r\n",
    "                    end_tokens[index_vaild,S_end] = 1\r\n",
    "                    send_s_po[index_vaild,0] = S_start\r\n",
    "                    send_s_po[index_vaild,1] = S_end\r\n",
    "                    S_index.append([j,index_vaild])\r\n",
    "                else:\r\n",
    "                    S_index.append([j,index_vaild])\r\n",
    "        if len(S_index) > 0:\r\n",
    "            for index_ in range(len(S_index)):\r\n",
    "                #随机选取object的首位，如果选取错误，则作为负样本\r\n",
    "                object_text_k = spo_list[k][S_index[index_][0]]['object'].lower().replace(' ','')\r\n",
    "                predicate = spo_list[k][S_index[index_][0]]['predicate']\r\n",
    "                p_id = p2id[predicate]\r\n",
    "                chars = np.zeros((len(context_k)))\r\n",
    "                index = context_k.find(object_text_k)\r\n",
    "                chars[index:index+len(object_text_k)]=1\r\n",
    "                offsets = [] \r\n",
    "                idx = 0\r\n",
    "                for t in enc_context[1:]:\r\n",
    "                    w = tokenizer.decode([t])\r\n",
    "                    if '#' in w and len(w)>1:\r\n",
    "                        w = w.replace('#','')\r\n",
    "                    if w == '[UNK]':\r\n",
    "                        w = '。'\r\n",
    "                    offsets.append((idx,idx+len(w)))\r\n",
    "                    idx += len(w)\r\n",
    "                toks = []\r\n",
    "                for i,(a,b) in enumerate(offsets):\r\n",
    "                    sm = np.sum(chars[a:b])\r\n",
    "                    if sm>0: \r\n",
    "                        toks.append(i) \r\n",
    "                if len(toks)>0:\r\n",
    "                    id_label[p_id] = predicate\r\n",
    "                    P_start = toks[0]+1\r\n",
    "                    P_end = toks[-1]+1\r\n",
    "                    object_start_tokens[S_index[index_][1]][P_start,p_id] = 1\r\n",
    "                    object_end_tokens[S_index[index_][1]][P_end,p_id] = 1\r\n",
    "    return input_ids[:index_vaild], attention_mask[:index_vaild], start_tokens[:index_vaild], end_tokens[:index_vaild], send_s_po[:index_vaild], \\\r\n",
    "           object_start_tokens[:index_vaild], object_end_tokens[:index_vaild], id_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13039, 256)\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "max_length = 256  \r\n",
    "model_path = '../model_dirs/bert-base-chinese'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "input_ids, attention_mask, start_tokens, end_tokens, send_s_po, object_start_tokens, object_end_tokens, id_label \\\r\n",
    "= proceed_data(train_text, train_spo, p2id, tokenizer, max_length, spo_count)\r\n",
    "\r\n",
    "print(start_tokens.shape)\r\n",
    "\r\n",
    "val_inputs = tokenizer(dev_text, max_length=max_length, padding='max_length', truncation=True, return_tensors='tf') \r\n",
    "val_input_ids, val_attention_mask = val_inputs['input_ids'], val_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(true,pred):\r\n",
    "    true = tf.cast(true,tf.float32)\r\n",
    "    loss = K.sum(K.binary_crossentropy(true, pred))\r\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\r\n",
    "    \"\"\"(Conditional) Layer Normalization\r\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        center=True,\r\n",
    "        scale=True,\r\n",
    "        epsilon=None,\r\n",
    "        conditional=False,\r\n",
    "        hidden_units=None,\r\n",
    "        hidden_activation='linear',\r\n",
    "        hidden_initializer='glorot_uniform',\r\n",
    "        **kwargs):\r\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\r\n",
    "        self.center = center\r\n",
    "        self.scale = scale\r\n",
    "        self.conditional = conditional\r\n",
    "        self.hidden_units = hidden_units\r\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\r\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\r\n",
    "        self.epsilon = epsilon or 1e-12\r\n",
    "        \r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        if self.conditional:\r\n",
    "            masks = mask if mask is not None else []\r\n",
    "            masks = [m[None] for m in masks if m is not None]\r\n",
    "            if len(masks) == 0:\r\n",
    "                return None\r\n",
    "            else:\r\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\r\n",
    "        else:\r\n",
    "            return mask\r\n",
    "        \r\n",
    "    def build(self, input_shape):\r\n",
    "        super(LayerNormalization, self).build(input_shape)\r\n",
    "        if self.conditional:\r\n",
    "            shape = (input_shape[0][-1],)\r\n",
    "        else:\r\n",
    "            shape = (input_shape[-1],)\r\n",
    "        if self.center:\r\n",
    "            self.beta = self.add_weight(\r\n",
    "                shape=shape, initializer='zeros', name='beta')\r\n",
    "        if self.scale:\r\n",
    "            self.gamma = self.add_weight(\r\n",
    "                shape=shape, initializer='ones', name='gamma')\r\n",
    "        if self.conditional:\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\r\n",
    "                    units=self.hidden_units,\r\n",
    "                    activation=self.hidden_activation,\r\n",
    "                    use_bias=False,\r\n",
    "                    kernel_initializer=self.hidden_initializer)\r\n",
    "            if self.center:\r\n",
    "                self.beta_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "            if self.scale:\r\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\r\n",
    "        \"\"\"\r\n",
    "        if self.conditional:\r\n",
    "            inputs, cond = inputs\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                cond = self.hidden_dense(cond)\r\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\r\n",
    "                cond = K.expand_dims(cond, 1)\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta_dense(cond) + self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\r\n",
    "        else:\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma\r\n",
    "        outputs = inputs\r\n",
    "        if self.center:\r\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\r\n",
    "            outputs = outputs - mean\r\n",
    "        if self.scale:\r\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\r\n",
    "            std = K.sqrt(variance + self.epsilon)\r\n",
    "            outputs = outputs / std\r\n",
    "            outputs = outputs * gamma\r\n",
    "        if self.center:\r\n",
    "            outputs = outputs + beta\r\n",
    "        return outputs\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject(inputs):\r\n",
    "    \"\"\"根据subject_ids从output中取出subject的向量表征\r\n",
    "    \"\"\"\r\n",
    "    output, subject_ids = inputs\r\n",
    "    start = tf.gather(output,subject_ids[:,0],axis=1,batch_dims=1)\r\n",
    "    end = tf.gather(output,subject_ids[:,1],axis=1,batch_dims=1)\r\n",
    "    subject = tf.keras.layers.Concatenate(axis=1)([start, end])\r\n",
    "    return subject\r\n",
    "'''\r\n",
    "   output.shape = (None,128,768)\r\n",
    "   subjudec_ids.shape = (None,2)\r\n",
    "   start.shape = (None,None,768)\r\n",
    "   subject.shape = (None,None,1536)\r\n",
    "   subject[:,0].shape = (None,1536)\r\n",
    "   这一部分给出各个变量的shape应该一目了然\r\n",
    "'''\r\n",
    "   \r\n",
    "def build_model_2(pretrained_path, MAX_LEN, p2id):\r\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    s_po_index =  tf.keras.layers.Input((2,), dtype=tf.int32)\r\n",
    "    \r\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, output_hidden_states=True)\r\n",
    "    outputs = bert_model(ids, attention_mask=att)\r\n",
    "    x, _, hidden_states  = outputs[:3]\r\n",
    "    layer_1 = hidden_states[-1]\r\n",
    "    start_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    start_logits = tf.keras.layers.Lambda(lambda x: x**2)(start_logits)\r\n",
    "    \r\n",
    "    end_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    end_logits = tf.keras.layers.Lambda(lambda x: x**2)(end_logits)\r\n",
    "    \r\n",
    "    subject_1 = extract_subject([layer_1,s_po_index])\r\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject_1])\r\n",
    "    \r\n",
    "    op_out_put_start = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_start = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_start)\r\n",
    "    \r\n",
    "    op_out_put_end = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_end = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_end)\r\n",
    "    \r\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[start_logits, end_logits, op_out_put_start, op_out_put_end])\r\n",
    "    model_2 = tf.keras.models.Model(inputs=[ids, att], outputs=[start_logits,end_logits])\r\n",
    "    model_3 = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[op_out_put_start, op_out_put_end])\r\n",
    "    return model, model_2, model_3\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rematch_text_word(tokenizer,text,enc_context,enc_start,enc_end):\r\n",
    "    span = [a.span()[0] for a in re.finditer(' ', text)]\r\n",
    "    decode_list = [tokenizer.decode([i]) for i in enc_context][1:]\r\n",
    "    start = 0\r\n",
    "    end = 0\r\n",
    "    len_start = 0\r\n",
    "    for i in range(len(decode_list)):\r\n",
    "        if i ==  enc_start - 1:\r\n",
    "            start = len_start\r\n",
    "        j = decode_list[i]\r\n",
    "        if '#' in j and len(j)>1:\r\n",
    "            j = j.replace('#','')\r\n",
    "        if j == '[UNK]':\r\n",
    "            j = '。'\r\n",
    "        len_start += len(j)\r\n",
    "        if i == enc_end - 1:\r\n",
    "            end = len_start\r\n",
    "            break\r\n",
    "    for span_index in span:\r\n",
    "        if start >= span_index:\r\n",
    "            start += 1\r\n",
    "            end += 1\r\n",
    "        if end > span_index and span_index>start:\r\n",
    "            end += 1\r\n",
    "    return text[start:end]\r\n",
    "\r\n",
    "\r\n",
    "class Metrics(tf.keras.callbacks.Callback):\r\n",
    "    def __init__(self,model_2, model_3, id2tag, va_text_list, va_spo_list, va_input_ids, va_attention_mask, tokenizer):\r\n",
    "        super(Metrics, self).__init__()\r\n",
    "        self.model_2 = model_2\r\n",
    "        self.model_3 = model_3\r\n",
    "        self.id2tag = id2tag\r\n",
    "        self.va_input_ids = va_input_ids\r\n",
    "        self.va_attention_mask = va_attention_mask\r\n",
    "        self.va_spo_list = va_spo_list\r\n",
    "        self.va_text_list = va_text_list\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        \r\n",
    "    def on_train_begin(self, logs=None):\r\n",
    "        self.val_f1s = []\r\n",
    "        self.best_val_f1 = 0\r\n",
    "    \r\n",
    "    def get_same_element_index(self,ob_list):\r\n",
    "        return [i for (i, v) in enumerate(ob_list) if v == 1]\r\n",
    "    \r\n",
    "    def evaluate_data(self):\r\n",
    "        Y1 = self.model_2.predict([self.va_input_ids,self.va_attention_mask])\r\n",
    "        question=[]\r\n",
    "        answer=[]\r\n",
    "        for m in range(len(Y1[0])):\r\n",
    "            for z in self.va_spo_list[m]:\r\n",
    "                question.append((z['subject'],z['predicate'],z['object']))\r\n",
    "            start = np.where(Y1[0][m]>0.5)[0]\r\n",
    "            end = np.where(Y1[1][m]>0.5)[0]\r\n",
    "            subjects = []\r\n",
    "            for i in start:\r\n",
    "                j = end[end >= i]\r\n",
    "                if len(j) > 0:\r\n",
    "                    j = j[0]\r\n",
    "                    subjects.append((i, j))\r\n",
    "            if subjects:\r\n",
    "                token_ids_2 = np.repeat([self.va_input_ids[m]], len(subjects), 0)\r\n",
    "                attention_mask_2 = np.repeat([self.va_attention_mask[m]], len(subjects), 0)\r\n",
    "                subjects = np.array(subjects)\r\n",
    "                object_preds_start,object_preds_end = self.model_3.predict([token_ids_2, attention_mask_2, subjects])\r\n",
    "                for subject,object_start,object_end in zip(subjects,object_preds_start,object_preds_end):\r\n",
    "                    sub = rematch_text_word(self.tokenizer,self.va_text_list[m],self.va_input_ids[m],subject[0],subject[1])\r\n",
    "                    start = np.argwhere(object_start > 0.5)\r\n",
    "                    end = np.argwhere(object_end > 0.5)\r\n",
    "                    for _start, predicate1 in start:\r\n",
    "                        for _end, predicate2 in end:\r\n",
    "                            if _start <= _end and predicate1 == predicate2:\r\n",
    "                                ans = rematch_text_word(self.tokenizer,self.va_text_list[m],self.va_input_ids[m],_start,_end)\r\n",
    "                                answer.append((sub,self.id2tag[predicate1],ans))\r\n",
    "                                break\r\n",
    "        Q = set(question)\r\n",
    "        S = set(answer)\r\n",
    "        f1 = 2*len(Q&S)/(len(Q)+len(S))\r\n",
    "        return f1\r\n",
    "    \r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        logs = logs or {}\r\n",
    "        _val_f1 = self.evaluate_data()\r\n",
    "        self.val_f1s.append(_val_f1)\r\n",
    "        logs['val_f1'] = _val_f1\r\n",
    "        if _val_f1 > self.best_val_f1:\r\n",
    "            self.model.save_weights('../model_dirs/fine_tune_relation_extraction/09_f1={}_model.hdf5'.format(round(_val_f1,4)))\r\n",
    "            self.best_val_f1 = _val_f1\r\n",
    "            print(\"best f1: {} \\n\".format(self.best_val_f1))\r\n",
    "        else:\r\n",
    "            print(\"val f1: {}, but not the best f1 \\n\".format(_val_f1))\r\n",
    "        return      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000240535D1F28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000240535D1F28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "3260/3260 [==============================] - 2143s 653ms/step - loss: 1218.5392 - lambda_loss: 37.3852 - lambda_1_loss: 29.0943 - lambda_2_loss: 596.6782 - lambda_3_loss: 555.3812\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "best f1: 0.4131093362709997 \n",
      "\n",
      "Epoch 2/20\n",
      "1531/3260 [=============>................] - ETA: 19:00 - loss: 62.3360 - lambda_loss: 9.4436 - lambda_1_loss: 10.2522 - lambda_2_loss: 21.3380 - lambda_3_loss: 21.3022"
     ]
    }
   ],
   "source": [
    "pretrained_path = '../model_dirs/bert-base-chinese'\r\n",
    "MAX_LEN = max_length\r\n",
    "# config = BertConfig.from_json_file('../model_dirs/bert-base-chinese/config.json')\r\n",
    "# TFBertModel.from_pretrained(pretrained_path, config=config)\r\n",
    "K.clear_session()\r\n",
    "model,model_2,model_3 = build_model_2(pretrained_path,  MAX_LEN, p2id)\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\r\n",
    "model.compile(loss={'lambda': new_loss,\r\n",
    "                'lambda_1': new_loss,\r\n",
    "                'lambda_2': new_loss,\r\n",
    "                'lambda_3': new_loss},optimizer=optimizer)\r\n",
    "model.fit([input_ids, attention_mask, send_s_po], [start_tokens, end_tokens, object_start_tokens, object_end_tokens], \\\r\n",
    "        epochs=20, batch_size=4, callbacks=[Metrics(model_2, model_3 ,id2p, dev_text, dev_spo, val_input_ids, val_attention_mask, tokenizer)])\r\n",
    "\r\n",
    "h5_path = '../model_dirs/fine_tune_relation_extraction/tf_model.h5'\r\n",
    "model.save_weights(h5_path)\r\n",
    "checkpoint_path = '../model_dirs/fine_tune_relation_extraction/checkpoints/my_checkpoint'\r\n",
    "model.save_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = '../model_dirs/bert-base-chinese'\r\n",
    "checkpoint_path = '../model_dirs/fine_tune_relation_extraction/checkpoints/my_checkpoint'\r\n",
    "h5_path = '../model_dirs/fine_tune_relation_extraction/tf_model.h5'\r\n",
    "MAX_LEN = max_length\r\n",
    "model,model_2,model_3 = build_model_2(pretrained_path, MAX_LEN, p2id)\r\n",
    "model.load_weights(h5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'predicate': '会议召开日期',\n",
       "   'object_type': '日期',\n",
       "   'subject_type': '公司',\n",
       "   'object': '2016年5月27日',\n",
       "   'subject': '畅联物流',\n",
       "   'subject_index': {'begin': 11, 'end': 15},\n",
       "   'object_index': {'begin': 0, 'end': 10}},\n",
       "  {'predicate': '任职日期',\n",
       "   'object_type': '日期',\n",
       "   'subject_type': '人物',\n",
       "   'object': '2016年5月27日',\n",
       "   'subject': '施俊',\n",
       "   'subject_index': {'begin': 34, 'end': 36},\n",
       "   'object_index': {'begin': 0, 'end': 10}}],\n",
       " 21)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\r\n",
    "x1 = [input_ids[[idx]], attention_mask[[idx]]]\r\n",
    "sub_start_tokens, sub_end_tokens = model_2.predict(x1)\r\n",
    "train_spo[idx], p2id[train_spo[idx][0]['predicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-acc03c0a62de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msub_start_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_start_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msub_end_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_end_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msub_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msub_start_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msub_end_idx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msub_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "sub_start_idx = int(np.argwhere(sub_start_tokens[0,:,0] > 0.5)[0])\r\n",
    "sub_end_idx = int(np.argwhere(sub_end_tokens[0,:,0] > 0.5)[0])\r\n",
    "sub_text = tokenizer.decode(input_ids[idx][sub_start_idx:sub_end_idx+1]).replace(' ','')\r\n",
    "sub_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 114)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = [input_ids[[idx]], attention_mask[[idx]], send_s_po[[idx]]]\r\n",
    "obj_start_tokens, obj_end_tokens = model_3.predict(x2)\r\n",
    "obj_start_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 24 74\n",
      "2,261.03万元 总资产\n",
      "29 34 47\n",
      "927.16万元 净资产账面价值\n"
     ]
    }
   ],
   "source": [
    "obj_start_idx = np.argwhere(obj_start_tokens[0] > 0.5)\r\n",
    "obj_end_idx = np.argwhere(obj_end_tokens[0] > 0.5)\r\n",
    "for _start, predicate1 in obj_start_idx:\r\n",
    "    for _end, predicate2 in obj_end_idx:\r\n",
    "        if _start <= _end and predicate1 == predicate2:\r\n",
    "            print(_start, _end, predicate1)\r\n",
    "            obj_text = tokenizer.decode(input_ids[idx][_start:_end+1]).replace(' ','')\r\n",
    "            print(obj_text, id2p[predicate1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 24 11:40:25 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 466.27       Driver Version: 466.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   37C    P8    N/A /  N/A |   3420MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1356    C+G   ...4__8j3eq9eme6ctt\\IGCC.exe    N/A      |\n",
      "|    0   N/A  N/A      8248    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10808    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     31648      C   ...onda3\\envs\\tfs\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "pid = os.getpid()\r\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 24 11:40:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 466.27       Driver Version: 466.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   38C    P8    N/A /  N/A |   3420MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1356    C+G   ...4__8j3eq9eme6ctt\\IGCC.exe    N/A      |\n",
      "|    0   N/A  N/A      8248    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10808    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     31648      C   ...onda3\\envs\\tfs\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tfs': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}