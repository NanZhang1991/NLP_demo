{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "# from ast import literal_eval\r\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "import re\r\n",
    "import os\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集整理\r\n",
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(path, nrows=False):\r\n",
    "    if nrows:\r\n",
    "        df = pd.read_json(path, nrows=nrows, lines=True)\r\n",
    "    else:\r\n",
    "        df = pd.read_json(path, lines=True)\r\n",
    "    df = df[['text', 'spo_list']]\r\n",
    "    return df\r\n",
    "\r\n",
    "def merge_df(dir_path):\r\n",
    "    total_df = pd.DataFrame()\r\n",
    "    for fn in os.listdir(dir_path):\r\n",
    "        df = json_to_df(os.path.join(dir_path, fn))\r\n",
    "        df_fn = fn[:fn.rfind('.')]\r\n",
    "        df.insert(0, 'fn', df_fn)\r\n",
    "        total_df =  total_df.append(df)\r\n",
    "    total_df.reset_index(drop=True, inplace=True)\r\n",
    "    print(f'original data size: {total_df.shape}') #\r\n",
    "    print(f'original data sample: {df.sample(5)}')\r\n",
    "    return total_df   \r\n",
    "\r\n",
    "def read_schemads(path_or_df):\r\n",
    "    if not isinstance(path_or_df, pd.DataFrame):\r\n",
    "        print(1)\r\n",
    "        schemads_path = path_or_df\r\n",
    "        predicate_data = pd.read_json(schemads_path, lines=True)\r\n",
    "        id2p = predicate_data['predicate'].drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    else:\r\n",
    "        df = path_or_df\r\n",
    "        id2p = df['spo_list'].apply(lambda spo_list: [spo['predicate'] for spo in spo_list])\r\n",
    "        id2p = id2p.explode().drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    p2id = dict(zip(id2p.values(), id2p.keys()))\r\n",
    "    print(f'length of p2id :{len(p2id)}')#\r\n",
    "    print(f'random p2id sample:{random.sample(p2id.items(), 5)}')#\r\n",
    "    return id2p, p2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 百度三元组关系数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path ='../data/百度关系抽取数据集/train_data.json'\r\n",
    "# train_data = json_to_df(train_path, nrows=10000)\r\n",
    "# print(f'Train data size: {train_data.shape}') #\r\n",
    "\r\n",
    "# dev_path = '../data/百度关系抽取数据集/dev_data.json'\r\n",
    "# dev_data = json_to_df(dev_path, nrows=5000)\r\n",
    "# print(f'Validation data size: {dev_data.shape}') \r\n",
    "\r\n",
    "# schemads_path = '../data/百度关系抽取数据集/all_50_schemas'\r\n",
    "# id2p, p2id = read_schemads(schemads_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 招股说明书三元组数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data size: (10698, 3)\n",
      "original data sample:                   fn                                               text  \\\n",
      "87  阿科力首次公开发行股票招股说明书  2016年-2017年上半年，公司开始建设募集资金投资项目，在建工程大幅增长导致资产总额增长...   \n",
      "73  阿科力首次公开发行股票招股说明书  2016年末，公司库存商品余额为590.45万元，较上年末减少了233.61万元降幅为28....   \n",
      "23  阿科力首次公开发行股票招股说明书  2014年末，资本公积期末余额5,405.39万元，较2013年末增加86.05%，主要原因...   \n",
      "44  阿科力首次公开发行股票招股说明书  根据江苏省高新技术企业认定管理工作协调小组办公室于2017年4月13日发布《关于申报开展20...   \n",
      "64  阿科力首次公开发行股票招股说明书  冯凯燕，女，1973年10月生，中国国籍，无境外永久居留权，大学本科学历。1994年7月至1...   \n",
      "\n",
      "                                             spo_list  \n",
      "87  [{'predicate': '净资产账面价值', 'object_type': '金额',...  \n",
      "73  [{'predicate': '存货', 'object_type': '金额', 'sub...  \n",
      "23  [{'predicate': '资本公积', 'object_type': '金额', 's...  \n",
      "44  [{'predicate': '出具日期', 'object_type': '日期', 's...  \n",
      "64  [{'predicate': '出生日期', 'object_type': '日期', 's...  \n",
      "length of p2id :116\n",
      "random p2id sample:[('预计负债', 104), ('速动比率', 18), ('变更日期', 4), ('预收款项', 79), ('投资收益', 101)]\n"
     ]
    }
   ],
   "source": [
    "dir_path = '../data/招股说明书三元组数据集'\r\n",
    "df = merge_df(dir_path)\r\n",
    "id2p, p2id = read_schemads(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spo(spo_list):\r\n",
    "    for spo in spo_list:\r\n",
    "        spo['predicate'] = spo['predicate'].lower()\r\n",
    "        spo['subject'] = spo['subject'].lower()\r\n",
    "        spo['object'] = spo['object'].lower()\r\n",
    "    return spo_list\r\n",
    "\r\n",
    "def data_clean(df):\r\n",
    "    df.dropna(how='any', inplace=True)\r\n",
    "    df = df[df['spo_list'].apply(lambda x: len(x)>0)]\r\n",
    "    df.drop_duplicates(subset=['text'], inplace=True)\r\n",
    "    df.reset_index(drop=True, inplace=True)\r\n",
    "    df['text'] = df['text'].str.lower()\r\n",
    "    df['spo_list'] = df['spo_list'].apply(clean_spo)\r\n",
    "    print(f'Real data size is {df.shape[0]}')\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real data size is 8436\n"
     ]
    }
   ],
   "source": [
    "df = data_clean(df)\r\n",
    "# train_data = data_clean(train_data)\r\n",
    "# dev_data = data_clean(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (8436, 3)\n",
      "Validation data size: (8436, 3)\n",
      "spo_count 24451\n"
     ]
    }
   ],
   "source": [
    "train_size=0.9\r\n",
    "\r\n",
    "# train_data = df.sample(frac=train_size,random_state=200)\r\n",
    "# dev_data = df.drop(train_data.index)\r\n",
    "train_data = df\r\n",
    "dev_data = df \r\n",
    "dev_data.reset_index(drop=True, inplace=True)\r\n",
    "train_data.reset_index(drop=True, inplace=True)\r\n",
    "print(f'Train data size: {train_data.shape}') #\r\n",
    "print(f'Validation data size: {dev_data.shape}') \r\n",
    "\r\n",
    "spo_single_count = df['spo_list'].apply(lambda x: len(x))\r\n",
    "spo_count = spo_single_count.sum()\r\n",
    "print('spo_count', spo_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spo_count 24451\n"
     ]
    }
   ],
   "source": [
    "train_text = train_data['text'].to_list()\r\n",
    "train_spo = train_data['spo_list'].to_list()\r\n",
    "\r\n",
    "\r\n",
    "dev_text = dev_data['text'].to_list()\r\n",
    "dev_spo = dev_data['spo_list'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签集分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    成立日期-公司-日期\n",
      "1    任职公司-人物-公司\n",
      "2    任职公司-人物-公司\n",
      "3    其他收入-日期-金额\n",
      "4    出具日期-文件-日期\n",
      "Name: spo_list, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "dataset_name = '招股说明书'\r\n",
    "spo = df['spo_list'].explode().reset_index(drop=True)\r\n",
    "spo_group = spo.apply(lambda x: '-'.join([x['predicate'] , x['subject_type'], x['object_type']]))\r\n",
    "print(spo_group.head())\r\n",
    "spo_group_count = spo_group.groupby(spo_group).count()\r\n",
    "spo_group_count.to_csv('../data/' + dataset_name + '_spo_group_count.csv', encoding='utf_8_sig')\r\n",
    "spo_group_count.plot(kind='bar', figsize=(30,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_data(text_list, spo_list, p2id, tokenizer, MAX_LEN, spo_count):\r\n",
    "    id_label = {}\r\n",
    "    ct = len(text_list)\r\n",
    "    MAX_LEN = MAX_LEN\r\n",
    "    input_ids = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    attention_mask = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    start_tokens = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    end_tokens = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    send_s_po = np.zeros((spo_count,2),dtype='int32')\r\n",
    "    object_start_tokens = np.zeros((spo_count,MAX_LEN,len(p2id)),dtype='int32')\r\n",
    "    object_end_tokens = np.zeros((spo_count,MAX_LEN,len(p2id)),dtype='int32')\r\n",
    "    index_vaild = -1\r\n",
    "    for k in range(ct):\r\n",
    "        context_k = text_list[k].lower().replace(' ','')\r\n",
    "        enc_context = tokenizer.encode(context_k,max_length=MAX_LEN,truncation=True)      \r\n",
    "        start = []\r\n",
    "        S_index = []\r\n",
    "        for j in range(len(spo_list[k])):\r\n",
    "            answers_text_k = spo_list[k][j]['subject'].lower().replace(' ','')\r\n",
    "            chars = np.zeros((len(context_k)))\r\n",
    "            index = context_k.find(answers_text_k)\r\n",
    "            chars[index:index+len(answers_text_k)]=1\r\n",
    "            offsets = []\r\n",
    "            idx=0\r\n",
    "            for t in enc_context[1:]:\r\n",
    "                w = tokenizer.decode([t])\r\n",
    "                if '#' in w and len(w)>1:\r\n",
    "                    w = w.replace('#','')\r\n",
    "                if w == '[UNK]':\r\n",
    "                    w = '。'\r\n",
    "                offsets.append((idx,idx+len(w)))\r\n",
    "                idx += len(w)\r\n",
    "            toks = []\r\n",
    "            for i,(a,b) in enumerate(offsets):\r\n",
    "                sm = np.sum(chars[a:b])\r\n",
    "                if sm>0: \r\n",
    "                    toks.append(i) \r\n",
    "            if len(toks)>0:\r\n",
    "                S_start = toks[0]+1\r\n",
    "                S_end = toks[-1]+1\r\n",
    "                if (S_start,S_end) not in start:\r\n",
    "                    index_vaild += 1\r\n",
    "                    start.append((S_start,S_end))\r\n",
    "                    input_ids[index_vaild,:len(enc_context)] = enc_context\r\n",
    "                    attention_mask[index_vaild,:len(enc_context)] = 1\r\n",
    "                    start_tokens[index_vaild,S_start] = 1\r\n",
    "                    end_tokens[index_vaild,S_end] = 1\r\n",
    "                    send_s_po[index_vaild,0] = S_start\r\n",
    "                    send_s_po[index_vaild,1] = S_end\r\n",
    "                    S_index.append([j,index_vaild])\r\n",
    "                else:\r\n",
    "                    S_index.append([j,index_vaild])\r\n",
    "        if len(S_index) > 0:\r\n",
    "            for index_ in range(len(S_index)):\r\n",
    "                #随机选取object的首位，如果选取错误，则作为负样本\r\n",
    "                object_text_k = spo_list[k][S_index[index_][0]]['object'].lower().replace(' ','')\r\n",
    "                predicate = spo_list[k][S_index[index_][0]]['predicate']\r\n",
    "                p_id = p2id[predicate]\r\n",
    "                chars = np.zeros((len(context_k)))\r\n",
    "                index = context_k.find(object_text_k)\r\n",
    "                chars[index:index+len(object_text_k)]=1\r\n",
    "                offsets = [] \r\n",
    "                idx = 0\r\n",
    "                for t in enc_context[1:]:\r\n",
    "                    w = tokenizer.decode([t])\r\n",
    "                    if '#' in w and len(w)>1:\r\n",
    "                        w = w.replace('#','')\r\n",
    "                    if w == '[UNK]':\r\n",
    "                        w = '。'\r\n",
    "                    offsets.append((idx,idx+len(w)))\r\n",
    "                    idx += len(w)\r\n",
    "                toks = []\r\n",
    "                for i,(a,b) in enumerate(offsets):\r\n",
    "                    sm = np.sum(chars[a:b])\r\n",
    "                    if sm>0: \r\n",
    "                        toks.append(i) \r\n",
    "                if len(toks)>0:\r\n",
    "                    id_label[p_id] = predicate\r\n",
    "                    P_start = toks[0]+1\r\n",
    "                    P_end = toks[-1]+1\r\n",
    "                    object_start_tokens[S_index[index_][1]][P_start,p_id] = 1\r\n",
    "                    object_end_tokens[S_index[index_][1]][P_end,p_id] = 1\r\n",
    "    return input_ids[:index_vaild], attention_mask[:index_vaild], start_tokens[:index_vaild], end_tokens[:index_vaild], send_s_po[:index_vaild], \\\r\n",
    "           object_start_tokens[:index_vaild], object_end_tokens[:index_vaild], id_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13039, 256)\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "max_length = 256  \r\n",
    "model_path = '../model_dirs/bert-base-chinese'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\r\n",
    "input_ids, attention_mask, start_tokens, end_tokens, send_s_po, object_start_tokens, object_end_tokens, id_label \\\r\n",
    "= proceed_data(train_text, train_spo, p2id, tokenizer, max_length, spo_count)\r\n",
    "\r\n",
    "print(start_tokens.shape)\r\n",
    "\r\n",
    "val_inputs = tokenizer(dev_text, max_length=max_length, padding='max_length', truncation=True, return_tensors='tf') \r\n",
    "val_input_ids, val_attention_mask = val_inputs['input_ids'], val_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(true,pred):\r\n",
    "    true = tf.cast(true,tf.float32)\r\n",
    "    loss = K.sum(K.binary_crossentropy(true, pred))\r\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\r\n",
    "    \"\"\"(Conditional) Layer Normalization\r\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        center=True,\r\n",
    "        scale=True,\r\n",
    "        epsilon=None,\r\n",
    "        conditional=False,\r\n",
    "        hidden_units=None,\r\n",
    "        hidden_activation='linear',\r\n",
    "        hidden_initializer='glorot_uniform',\r\n",
    "        **kwargs):\r\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\r\n",
    "        self.center = center\r\n",
    "        self.scale = scale\r\n",
    "        self.conditional = conditional\r\n",
    "        self.hidden_units = hidden_units\r\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\r\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\r\n",
    "        self.epsilon = epsilon or 1e-12\r\n",
    "        \r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        if self.conditional:\r\n",
    "            masks = mask if mask is not None else []\r\n",
    "            masks = [m[None] for m in masks if m is not None]\r\n",
    "            if len(masks) == 0:\r\n",
    "                return None\r\n",
    "            else:\r\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\r\n",
    "        else:\r\n",
    "            return mask\r\n",
    "        \r\n",
    "    def build(self, input_shape):\r\n",
    "        super(LayerNormalization, self).build(input_shape)\r\n",
    "        if self.conditional:\r\n",
    "            shape = (input_shape[0][-1],)\r\n",
    "        else:\r\n",
    "            shape = (input_shape[-1],)\r\n",
    "        if self.center:\r\n",
    "            self.beta = self.add_weight(\r\n",
    "                shape=shape, initializer='zeros', name='beta')\r\n",
    "        if self.scale:\r\n",
    "            self.gamma = self.add_weight(\r\n",
    "                shape=shape, initializer='ones', name='gamma')\r\n",
    "        if self.conditional:\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\r\n",
    "                    units=self.hidden_units,\r\n",
    "                    activation=self.hidden_activation,\r\n",
    "                    use_bias=False,\r\n",
    "                    kernel_initializer=self.hidden_initializer)\r\n",
    "            if self.center:\r\n",
    "                self.beta_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "            if self.scale:\r\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\r\n",
    "        \"\"\"\r\n",
    "        if self.conditional:\r\n",
    "            inputs, cond = inputs\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                cond = self.hidden_dense(cond)\r\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\r\n",
    "                cond = K.expand_dims(cond, 1)\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta_dense(cond) + self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\r\n",
    "        else:\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma\r\n",
    "        outputs = inputs\r\n",
    "        if self.center:\r\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\r\n",
    "            outputs = outputs - mean\r\n",
    "        if self.scale:\r\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\r\n",
    "            std = K.sqrt(variance + self.epsilon)\r\n",
    "            outputs = outputs / std\r\n",
    "            outputs = outputs * gamma\r\n",
    "        if self.center:\r\n",
    "            outputs = outputs + beta\r\n",
    "        return outputs\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject(inputs):\r\n",
    "    \"\"\"根据subject_ids从output中取出subject的向量表征\r\n",
    "    \"\"\"\r\n",
    "    output, subject_ids = inputs\r\n",
    "    start = tf.gather(output,subject_ids[:,0],axis=1,batch_dims=1)\r\n",
    "    end = tf.gather(output,subject_ids[:,1],axis=1,batch_dims=1)\r\n",
    "    subject = tf.keras.layers.Concatenate(axis=1)([start, end])\r\n",
    "    return subject\r\n",
    "'''\r\n",
    "   output.shape = (None,128,768)\r\n",
    "   subjudec_ids.shape = (None,2)\r\n",
    "   start.shape = (None,None,768)\r\n",
    "   subject.shape = (None,None,1536)\r\n",
    "   subject[:,0].shape = (None,1536)\r\n",
    "   这一部分给出各个变量的shape应该一目了然\r\n",
    "'''\r\n",
    "   \r\n",
    "def build_model_2(pretrained_path, MAX_LEN, p2id):\r\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    s_po_index =  tf.keras.layers.Input((2,), dtype=tf.int32)\r\n",
    "    \r\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, output_hidden_states=True)\r\n",
    "    outputs = bert_model(ids, attention_mask=att)\r\n",
    "    x, _, hidden_states  = outputs[:3]\r\n",
    "    layer_1 = hidden_states[-1]\r\n",
    "    start_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    start_logits = tf.keras.layers.Lambda(lambda x: x**2)(start_logits)\r\n",
    "    \r\n",
    "    end_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    end_logits = tf.keras.layers.Lambda(lambda x: x**2)(end_logits)\r\n",
    "    \r\n",
    "    subject_1 = extract_subject([layer_1,s_po_index])\r\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject_1])\r\n",
    "    \r\n",
    "    op_out_put_start = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_start = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_start)\r\n",
    "    \r\n",
    "    op_out_put_end = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_end = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_end)\r\n",
    "    \r\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[start_logits, end_logits, op_out_put_start, op_out_put_end])\r\n",
    "    model_2 = tf.keras.models.Model(inputs=[ids, att], outputs=[start_logits,end_logits])\r\n",
    "    model_3 = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[op_out_put_start, op_out_put_end])\r\n",
    "    return model, model_2, model_3\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rematch_text_word(tokenizer,text,enc_context,enc_start,enc_end):\r\n",
    "    span = [a.span()[0] for a in re.finditer(' ', text)]\r\n",
    "    decode_list = [tokenizer.decode([i]) for i in enc_context][1:]\r\n",
    "    start = 0\r\n",
    "    end = 0\r\n",
    "    len_start = 0\r\n",
    "    for i in range(len(decode_list)):\r\n",
    "        if i ==  enc_start - 1:\r\n",
    "            start = len_start\r\n",
    "        j = decode_list[i]\r\n",
    "        if '#' in j and len(j)>1:\r\n",
    "            j = j.replace('#','')\r\n",
    "        if j == '[UNK]':\r\n",
    "            j = '。'\r\n",
    "        len_start += len(j)\r\n",
    "        if i == enc_end - 1:\r\n",
    "            end = len_start\r\n",
    "            break\r\n",
    "    for span_index in span:\r\n",
    "        if start >= span_index:\r\n",
    "            start += 1\r\n",
    "            end += 1\r\n",
    "        if end > span_index and span_index>start:\r\n",
    "            end += 1\r\n",
    "    return text[start:end]\r\n",
    "\r\n",
    "\r\n",
    "class Metrics(tf.keras.callbacks.Callback):\r\n",
    "    def __init__(self,model_2, model_3, id2tag, va_text_list, va_spo_list, va_input_ids, va_attention_mask, tokenizer):\r\n",
    "        super(Metrics, self).__init__()\r\n",
    "        self.model_2 = model_2\r\n",
    "        self.model_3 = model_3\r\n",
    "        self.id2tag = id2tag\r\n",
    "        self.va_input_ids = va_input_ids\r\n",
    "        self.va_attention_mask = va_attention_mask\r\n",
    "        self.va_spo_list = va_spo_list\r\n",
    "        self.va_text_list = va_text_list\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        \r\n",
    "    def on_train_begin(self, logs=None):\r\n",
    "        self.val_f1s = []\r\n",
    "        self.best_val_f1 = 0\r\n",
    "    \r\n",
    "    def get_same_element_index(self,ob_list):\r\n",
    "        return [i for (i, v) in enumerate(ob_list) if v == 1]\r\n",
    "    \r\n",
    "    def evaluate_data(self):\r\n",
    "        Y1 = self.model_2.predict([self.va_input_ids,self.va_attention_mask])\r\n",
    "        question=[]\r\n",
    "        answer=[]\r\n",
    "        for m in range(len(Y1[0])):\r\n",
    "            for z in self.va_spo_list[m]:\r\n",
    "                question.append((z['subject'],z['predicate'],z['object']))\r\n",
    "            start = np.where(Y1[0][m]>0.5)[0]\r\n",
    "            end = np.where(Y1[1][m]>0.5)[0]\r\n",
    "            subjects = []\r\n",
    "            for i in start:\r\n",
    "                j = end[end >= i]\r\n",
    "                if len(j) > 0:\r\n",
    "                    j = j[0]\r\n",
    "                    subjects.append((i, j))\r\n",
    "            if subjects:\r\n",
    "                token_ids_2 = np.repeat([self.va_input_ids[m]], len(subjects), 0)\r\n",
    "                attention_mask_2 = np.repeat([self.va_attention_mask[m]], len(subjects), 0)\r\n",
    "                subjects = np.array(subjects)\r\n",
    "                object_preds_start,object_preds_end = self.model_3.predict([token_ids_2, attention_mask_2, subjects])\r\n",
    "                for subject,object_start,object_end in zip(subjects,object_preds_start,object_preds_end):\r\n",
    "                    sub = rematch_text_word(self.tokenizer,self.va_text_list[m],self.va_input_ids[m],subject[0],subject[1])\r\n",
    "                    start = np.argwhere(object_start > 0.5)\r\n",
    "                    end = np.argwhere(object_end > 0.5)\r\n",
    "                    for _start, predicate1 in start:\r\n",
    "                        for _end, predicate2 in end:\r\n",
    "                            if _start <= _end and predicate1 == predicate2:\r\n",
    "                                ans = rematch_text_word(self.tokenizer,self.va_text_list[m],self.va_input_ids[m],_start,_end)\r\n",
    "                                answer.append((sub,self.id2tag[predicate1],ans))\r\n",
    "                                break\r\n",
    "        Q = set(question)\r\n",
    "        S = set(answer)\r\n",
    "        f1 = 2*len(Q&S)/(len(Q)+len(S))\r\n",
    "        return f1\r\n",
    "    \r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        logs = logs or {}\r\n",
    "        _val_f1 = self.evaluate_data()\r\n",
    "        self.val_f1s.append(_val_f1)\r\n",
    "        logs['val_f1'] = _val_f1\r\n",
    "        if _val_f1 > self.best_val_f1:\r\n",
    "            self.model.save_weights('../model_dirs/fine_tune_relation_extraction/09_f1={}_model.hdf5'.format(round(_val_f1,4)))\r\n",
    "            self.best_val_f1 = _val_f1\r\n",
    "            print(\"best f1: {} \\n\".format(self.best_val_f1))\r\n",
    "        else:\r\n",
    "            print(\"val f1: {}, but not the best f1 \\n\".format(_val_f1))\r\n",
    "        return      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000240535D1F28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000240535D1F28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "3260/3260 [==============================] - 2143s 653ms/step - loss: 1218.5392 - lambda_loss: 37.3852 - lambda_1_loss: 29.0943 - lambda_2_loss: 596.6782 - lambda_3_loss: 555.3812\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "best f1: 0.4131093362709997 \n",
      "\n",
      "Epoch 2/20\n",
      "1531/3260 [=============>................] - ETA: 19:00 - loss: 62.3360 - lambda_loss: 9.4436 - lambda_1_loss: 10.2522 - lambda_2_loss: 21.3380 - lambda_3_loss: 21.3022"
     ]
    }
   ],
   "source": [
    "pretrained_path = '../model_dirs/bert-base-chinese'\r\n",
    "MAX_LEN = max_length\r\n",
    "# config = BertConfig.from_json_file('../model_dirs/bert-base-chinese/config.json')\r\n",
    "# TFBertModel.from_pretrained(pretrained_path, config=config)\r\n",
    "K.clear_session()\r\n",
    "model,model_2,model_3 = build_model_2(pretrained_path,  MAX_LEN, p2id)\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\r\n",
    "model.compile(loss={'lambda': new_loss,\r\n",
    "                'lambda_1': new_loss,\r\n",
    "                'lambda_2': new_loss,\r\n",
    "                'lambda_3': new_loss},optimizer=optimizer)\r\n",
    "model.fit([input_ids, attention_mask, send_s_po], [start_tokens, end_tokens, object_start_tokens, object_end_tokens], \\\r\n",
    "        epochs=20, batch_size=4, callbacks=[Metrics(model_2, model_3 ,id2p, dev_text, dev_spo, val_input_ids, val_attention_mask, tokenizer)])\r\n",
    "\r\n",
    "h5_path = '../model_dirs/fine_tune_relation_extraction/tf_model.h5'\r\n",
    "model.save_weights(h5_path)\r\n",
    "checkpoint_path = '../model_dirs/fine_tune_relation_extraction/checkpoints/my_checkpoint'\r\n",
    "model.save_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../model_dirs/bert-base-chinese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = '../model_dirs/bert-base-chinese'\r\n",
    "checkpoint_path = '../model_dirs/fine_tune_relation_extraction/checkpoints/my_checkpoint'\r\n",
    "h5_path = '../model_dirs/fine_tune_relation_extraction/tf_model.h5'\r\n",
    "MAX_LEN = max_length\r\n",
    "model,model_2,model_3 = build_model_2(pretrained_path, MAX_LEN, p2id)\r\n",
    "model.load_weights(h5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'predicate': '会议召开日期',\n",
       "   'object_type': '日期',\n",
       "   'subject_type': '公司',\n",
       "   'object': '2016年5月27日',\n",
       "   'subject': '畅联物流',\n",
       "   'subject_index': {'begin': 11, 'end': 15},\n",
       "   'object_index': {'begin': 0, 'end': 10}},\n",
       "  {'predicate': '任职日期',\n",
       "   'object_type': '日期',\n",
       "   'subject_type': '人物',\n",
       "   'object': '2016年5月27日',\n",
       "   'subject': '施俊',\n",
       "   'subject_index': {'begin': 34, 'end': 36},\n",
       "   'object_index': {'begin': 0, 'end': 10}}],\n",
       " 21)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\r\n",
    "x1 = [input_ids[[idx]], attention_mask[[idx]]]\r\n",
    "sub_start_tokens, sub_end_tokens = model_2.predict(x1)\r\n",
    "train_spo[idx], p2id[train_spo[idx][0]['predicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-acc03c0a62de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msub_start_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_start_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msub_end_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_end_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msub_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msub_start_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msub_end_idx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msub_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "sub_start_idx = int(np.argwhere(sub_start_tokens[0,:,0] > 0.5)[0])\r\n",
    "sub_end_idx = int(np.argwhere(sub_end_tokens[0,:,0] > 0.5)[0])\r\n",
    "sub_text = tokenizer.decode(input_ids[idx][sub_start_idx:sub_end_idx+1]).replace(' ','')\r\n",
    "sub_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 114)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = [input_ids[[idx]], attention_mask[[idx]], send_s_po[[idx]]]\r\n",
    "obj_start_tokens, obj_end_tokens = model_3.predict(x2)\r\n",
    "obj_start_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 24 74\n",
      "2,261.03万元 总资产\n",
      "29 34 47\n",
      "927.16万元 净资产账面价值\n"
     ]
    }
   ],
   "source": [
    "obj_start_idx = np.argwhere(obj_start_tokens[0] > 0.5)\r\n",
    "obj_end_idx = np.argwhere(obj_end_tokens[0] > 0.5)\r\n",
    "for _start, predicate1 in obj_start_idx:\r\n",
    "    for _end, predicate2 in obj_end_idx:\r\n",
    "        if _start <= _end and predicate1 == predicate2:\r\n",
    "            print(_start, _end, predicate1)\r\n",
    "            obj_text = tokenizer.decode(input_ids[idx][_start:_end+1]).replace(' ','')\r\n",
    "            print(obj_text, id2p[predicate1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 24 11:40:25 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 466.27       Driver Version: 466.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   37C    P8    N/A /  N/A |   3420MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1356    C+G   ...4__8j3eq9eme6ctt\\IGCC.exe    N/A      |\n",
      "|    0   N/A  N/A      8248    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10808    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     31648      C   ...onda3\\envs\\tfs\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "pid = os.getpid()\r\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 24 11:40:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 466.27       Driver Version: 466.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   38C    P8    N/A /  N/A |   3420MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1356    C+G   ...4__8j3eq9eme6ctt\\IGCC.exe    N/A      |\n",
      "|    0   N/A  N/A      8248    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10808    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     31648      C   ...onda3\\envs\\tfs\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tfs': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}