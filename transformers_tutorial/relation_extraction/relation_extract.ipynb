{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import zipfile\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "# from ast import literal_eval\r\n",
    "from transformers import BertTokenizer,  BertConfig, TFBertModel\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "import re\r\n",
    "import os\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\r\n",
    "print(gpus)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据集整理\r\n",
    "## 读取数据"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def un_zip(file_name):  \r\n",
    "    \"\"\"unzip zip file\"\"\"  \r\n",
    "    zip_file = zipfile.ZipFile(file_name)\r\n",
    "    for name in zip_file.namelist():  \r\n",
    "        zip_file.extract(name,'../data')\r\n",
    "    zip_file.close()\r\n",
    "    dir_path = file_name.rsplit('.', 1)[0]\r\n",
    "    return dir_path\r\n",
    "\r\n",
    "def json_to_df(path, nrows=False):\r\n",
    "    if nrows:\r\n",
    "        df = pd.read_json(path, nrows=nrows, lines=True)\r\n",
    "    else:\r\n",
    "        df = pd.read_json(path, lines=True)\r\n",
    "    df = df[['text', 'spo_list']]\r\n",
    "    return df\r\n",
    "\r\n",
    "def merge_df(path):\r\n",
    "    if path.rsplit('.', 1)[1]=='zip':\r\n",
    "        dir_path = un_zip(path)\r\n",
    "    else:\r\n",
    "        dir_path = path\r\n",
    "    total_df = pd.DataFrame()\r\n",
    "    for fn in os.listdir(dir_path):\r\n",
    "        df = json_to_df(os.path.join(dir_path, fn))\r\n",
    "        df_fn = fn[:fn.rfind('.')]\r\n",
    "        df.insert(0, 'fn', df_fn)\r\n",
    "        total_df =  total_df.append(df)\r\n",
    "    total_df.reset_index(drop=True, inplace=True)\r\n",
    "    print(f'original data size: {total_df.shape}') #\r\n",
    "    # print(f'original data sample: {df.sample(5)}\\n')\r\n",
    "    return total_df   \r\n",
    "\r\n",
    "def read_schemads(path_or_df):\r\n",
    "    if not isinstance(path_or_df, pd.DataFrame):\r\n",
    "        print(1)\r\n",
    "        schemads_path = path_or_df\r\n",
    "        predicate_data = pd.read_json(schemads_path, lines=True)\r\n",
    "        id2p = predicate_data['predicate'].drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    else:\r\n",
    "        df = path_or_df\r\n",
    "        id2p = df['spo_list'].apply(lambda spo_list: [spo['predicate'] for spo in spo_list])\r\n",
    "        id2p = id2p.explode().drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "    p2id = dict(zip(id2p.values(), id2p.keys()))\r\n",
    "    print(f'length of p2id :{len(p2id)}')#\r\n",
    "    print(f'random p2id sample:{random.sample(p2id.items(), 5)}')#\r\n",
    "    return id2p, p2id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 百度三元组关系数据集"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# train_path ='../data/百度关系抽取数据集/train_data.json'\r\n",
    "# train_data = json_to_df(train_path, nrows=10000)\r\n",
    "# print(f'Train data size: {train_data.shape}') #\r\n",
    "\r\n",
    "# dev_path = '../data/百度关系抽取数据集/dev_data.json'\r\n",
    "# dev_data = json_to_df(dev_path, nrows=5000)\r\n",
    "# print(f'Validation data size: {dev_data.shape}') \r\n",
    "\r\n",
    "# schemads_path = '../data/百度关系抽取数据集/all_50_schemas'\r\n",
    "# id2p, p2id = read_schemads(schemads_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 招股说明书三元组数据集"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "path1 = './data/三元组数据集_云测标注'\r\n",
    "df1 = merge_df(path1)\r\n",
    "\r\n",
    "path2 = './data/三元组数据集_内部标注'\r\n",
    "df2 = merge_df(path2)\r\n",
    "\r\n",
    "path3 = './data/三元组数据集_云测8.5前标注'\r\n",
    "df3 = merge_df(path3)\r\n",
    "\r\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\r\n",
    "print(f'df.shape: {df.shape}')\r\n",
    "id2p, p2id = read_schemads(df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original data size: (9500, 3)\n",
      "original data size: (837, 3)\n",
      "original data size: (12173, 3)\n",
      "df.shape: (22510, 3)\n",
      "length of p2id :163\n",
      "random p2id sample:[('合同负债', 154), ('营业收入', 41), ('任职日期', 3), ('应付票据', 51), ('存货', 33)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 清洗数据"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def clean_spo(spo_list):\r\n",
    "    for spo in spo_list:\r\n",
    "        spo['predicate'] = spo['predicate'].lower()\r\n",
    "        spo['subject'] = spo['subject'].lower()\r\n",
    "        spo['object'] = spo['object'].lower()\r\n",
    "    return spo_list\r\n",
    "\r\n",
    "def data_clean(df):\r\n",
    "    df.dropna(how='any', inplace=True)\r\n",
    "    df = df[df['spo_list'].apply(lambda x: len(x)>0)]\r\n",
    "    df.drop_duplicates(subset=['text'], inplace=True)\r\n",
    "    df.reset_index(drop=True, inplace=True)\r\n",
    "    df['text'] = df['text'].str.lower()\r\n",
    "    df['spo_list'] = df['spo_list'].apply(clean_spo)\r\n",
    "    print(f'Real data size is {df.shape[0]}')\r\n",
    "    return df\r\n",
    "    \r\n",
    "df = data_clean(df)\r\n",
    "# train_data = data_clean(train_data)\r\n",
    "# dev_data = data_clean(dev_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Real data size is 12798\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 划分数据集"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_size=0.9\r\n",
    "train_data = df.sample(frac=train_size,random_state=200)\r\n",
    "dev_data = df.drop(train_data.index)\r\n",
    "dev_data.reset_index(drop=True, inplace=True)\r\n",
    "train_data.reset_index(drop=True, inplace=True)\r\n",
    "print(f'Train data size: {train_data.shape}') #\r\n",
    "print(f'Validation data size: {dev_data.shape}') \r\n",
    "\r\n",
    "train_text = train_data['text'].to_list()\r\n",
    "train_spo = train_data['spo_list'].to_list()\r\n",
    "\r\n",
    "dev_text = dev_data['text'].to_list()\r\n",
    "dev_spo = dev_data['spo_list'].to_list()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train data size: (11518, 3)\n",
      "Validation data size: (1280, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 标签集分布"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def calculate_tag(df, pid, dataset_name, label_range):\r\n",
    "    '''\r\n",
    "    label_range: Number range of a single label\r\n",
    "    '''\r\n",
    "    start, end = label_range\r\n",
    "    spo = df['spo_list'].explode().reset_index(drop=True)\r\n",
    "    spo_group = spo.apply(lambda x: '-'.join([x['predicate'] , x['subject_type'], x['object_type']]))\r\n",
    "    spo_group_count = spo_group.groupby(spo_group).count().reset_index(name='count')\r\n",
    "    spo_group_count['quantity_required'] = spo_group_count['count'].apply(lambda x: start if start <= x < end  else x)\r\n",
    "    spo_group_count['compliance'] = spo_group_count['count'].apply(lambda x: 1 if start < x < end  else 0)\r\n",
    "    spo_group_count.to_csv('./data/' + dataset_name + '_spo_group_count.csv', index=False, encoding='utf_8_sig')\r\n",
    "    spo_group_count.plot(kind='bar', figsize=(30,15))\r\n",
    "    print(f\"Total number of p2id is {len(p2id)}, actual number of labels that meet the requirements is {spo_group_count['compliance'].sum()}\")\r\n",
    "    print(f\"Spo count is {spo_group_count['count'].sum()}, number of valid labels is {spo_group_count['quantity_required'].sum()}\")\r\n",
    "    return spo_group_count\r\n",
    "\r\n",
    "dataset_name = '招股说明书'\r\n",
    "label_range = [200, float('inf')]\r\n",
    "spo_group_count = calculate_tag(df, p2id, dataset_name, label_range)\r\n",
    "spo_count = spo_group_count['count'].sum()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of p2id is 163, actual number of labels that meet the requirements is 47"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_15232/3998165496.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'招股说明书'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mlabel_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mspo_group_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mspo_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspo_group_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_15232/3998165496.py\u001b[0m in \u001b[0;36mcalculate_tag\u001b[1;34m(df, pid, dataset_name, label_range)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mspo_group_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_spo_group_count.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf_8_sig'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mspo_group_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Total number of p2id is {len(p2id)}, actual number of labels that meet the requirements is {spo_group_count['compliance'].sum()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Spo count is {spo_group_count['count'].sum()}, number of valid labels is {spo_group_count['quantity_required'].sum()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mspo_group_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py3.7\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m                 \u001b[1;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py3.7\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py3.7\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    539\u001b[0m                 )\n\u001b[0;32m    540\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py3.7\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def proceed_data(text_list, spo_list, p2id, tokenizer, MAX_LEN, spo_count):\r\n",
    "    id_label = {}\r\n",
    "    ct = len(text_list)\r\n",
    "    MAX_LEN = MAX_LEN\r\n",
    "    input_ids = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    attention_mask = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    start_tokens = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    end_tokens = np.zeros((spo_count,MAX_LEN),dtype='int32')\r\n",
    "    send_s_po = np.zeros((spo_count,2),dtype='int32')\r\n",
    "    object_start_tokens = np.zeros((spo_count,MAX_LEN,len(p2id)),dtype='int32')\r\n",
    "    object_end_tokens = np.zeros((spo_count,MAX_LEN,len(p2id)),dtype='int32')\r\n",
    "    index_vaild = -1\r\n",
    "    for k in range(ct):\r\n",
    "        context_k = text_list[k].lower().replace(' ','')\r\n",
    "        enc_context = tokenizer.encode(context_k,max_length=MAX_LEN,truncation=True)      \r\n",
    "        start = []\r\n",
    "        S_index = []\r\n",
    "        for j in range(len(spo_list[k])):\r\n",
    "            answers_text_k = spo_list[k][j]['subject'].lower().replace(' ','')\r\n",
    "            chars = np.zeros((len(context_k)))\r\n",
    "            index = context_k.find(answers_text_k)\r\n",
    "            chars[index:index+len(answers_text_k)]=1\r\n",
    "            offsets = []\r\n",
    "            idx=0\r\n",
    "            for t in enc_context[1:]:\r\n",
    "                w = tokenizer.decode([t])\r\n",
    "                if '#' in w and len(w)>1:\r\n",
    "                    w = w.replace('#','')\r\n",
    "                if w == '[UNK]':\r\n",
    "                    w = '。'\r\n",
    "                offsets.append((idx,idx+len(w)))\r\n",
    "                idx += len(w)\r\n",
    "            toks = []\r\n",
    "            for i,(a,b) in enumerate(offsets):\r\n",
    "                sm = np.sum(chars[a:b])\r\n",
    "                if sm>0: \r\n",
    "                    toks.append(i) \r\n",
    "            if len(toks)>0:\r\n",
    "                S_start = toks[0]+1\r\n",
    "                S_end = toks[-1]+1\r\n",
    "                if (S_start,S_end) not in start:\r\n",
    "                    index_vaild += 1\r\n",
    "                    start.append((S_start,S_end))\r\n",
    "                    input_ids[index_vaild,:len(enc_context)] = enc_context\r\n",
    "                    attention_mask[index_vaild,:len(enc_context)] = 1\r\n",
    "                    start_tokens[index_vaild,S_start] = 1\r\n",
    "                    end_tokens[index_vaild,S_end] = 1\r\n",
    "                    send_s_po[index_vaild,0] = S_start\r\n",
    "                    send_s_po[index_vaild,1] = S_end\r\n",
    "                    S_index.append([j,index_vaild])\r\n",
    "                else:\r\n",
    "                    S_index.append([j,index_vaild])\r\n",
    "        if len(S_index) > 0:\r\n",
    "            for index_ in range(len(S_index)):\r\n",
    "                #随机选取object的首位，如果选取错误，则作为负样本\r\n",
    "                object_text_k = spo_list[k][S_index[index_][0]]['object'].lower().replace(' ','')\r\n",
    "                predicate = spo_list[k][S_index[index_][0]]['predicate']\r\n",
    "                p_id = p2id[predicate]\r\n",
    "                chars = np.zeros((len(context_k)))\r\n",
    "                index = context_k.find(object_text_k)\r\n",
    "                chars[index:index+len(object_text_k)]=1\r\n",
    "                offsets = [] \r\n",
    "                idx = 0\r\n",
    "                for t in enc_context[1:]:\r\n",
    "                    w = tokenizer.decode([t])\r\n",
    "                    if '#' in w and len(w)>1:\r\n",
    "                        w = w.replace('#','')\r\n",
    "                    if w == '[UNK]':\r\n",
    "                        w = '。'\r\n",
    "                    offsets.append((idx,idx+len(w)))\r\n",
    "                    idx += len(w)\r\n",
    "                toks = []\r\n",
    "                for i,(a,b) in enumerate(offsets):\r\n",
    "                    sm = np.sum(chars[a:b])\r\n",
    "                    if sm>0: \r\n",
    "                        toks.append(i) \r\n",
    "                if len(toks)>0:\r\n",
    "                    id_label[p_id] = predicate\r\n",
    "                    P_start = toks[0]+1\r\n",
    "                    P_end = toks[-1]+1\r\n",
    "                    object_start_tokens[S_index[index_][1]][P_start,p_id] = 1\r\n",
    "                    object_end_tokens[S_index[index_][1]][P_end,p_id] = 1\r\n",
    "    return input_ids[:index_vaild], attention_mask[:index_vaild], start_tokens[:index_vaild], end_tokens[:index_vaild], send_s_po[:index_vaild], \\\r\n",
    "           object_start_tokens[:index_vaild], object_end_tokens[:index_vaild], id_label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "MAX_LEN = 256  \r\n",
    "pretrained_path = '../model_dirs/chinese-bert-wwm-ext'  \r\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_path)\r\n",
    "input_ids, attention_mask, start_tokens, end_tokens, send_s_po, object_start_tokens, object_end_tokens, id_label \\\r\n",
    "= proceed_data(train_text, train_spo, p2id, tokenizer, MAX_LEN, spo_count)\r\n",
    "\r\n",
    "print(f'start tokens shape: {start_tokens.shape}')\r\n",
    "\r\n",
    "val_inputs = tokenizer(dev_text, max_length=MAX_LEN, padding='max_length', truncation=True, return_tensors='tf') \r\n",
    "val_input_ids, val_attention_mask = val_inputs['input_ids'], val_inputs['attention_mask']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start tokens shape: (19892, 512)\n",
      "CPU times: user 1min 45s, sys: 395 ms, total: 1min 45s\n",
      "Wall time: 1min 46s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-10 05:07:08.111851: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-10 05:07:08.112008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-10 05:07:08.112409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:08:00.0 name: GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2021-08-10 05:07:08.112443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-10 05:07:08.112456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-08-10 05:07:08.112465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-08-10 05:07:08.112472: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-10 05:07:08.112484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-10 05:07:08.112492: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-10 05:07:08.112498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-08-10 05:07:08.112506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-08-10 05:07:08.112549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-10 05:07:08.112918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-10 05:07:08.113252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-08-10 05:07:08.113275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-10 05:07:08.448400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-10 05:07:08.448429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-08-10 05:07:08.448435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-08-10 05:07:08.448592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-10 05:07:08.448940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-10 05:07:08.449252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-10 05:07:08.449556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8725 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:08:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def new_loss(true,pred):\r\n",
    "    true = tf.cast(true,tf.float32)\r\n",
    "    loss = K.sum(K.binary_crossentropy(true, pred))\r\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\r\n",
    "    \"\"\"(Conditional) Layer Normalization\r\n",
    "    hidden_*系列参数仅为有条件输入时(conditional=True)使用\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        center=True,\r\n",
    "        scale=True,\r\n",
    "        epsilon=None,\r\n",
    "        conditional=False,\r\n",
    "        hidden_units=None,\r\n",
    "        hidden_activation='linear',\r\n",
    "        hidden_initializer='glorot_uniform',\r\n",
    "        **kwargs):\r\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\r\n",
    "        self.center = center\r\n",
    "        self.scale = scale\r\n",
    "        self.conditional = conditional\r\n",
    "        self.hidden_units = hidden_units\r\n",
    "        self.hidden_activation = tf.keras.activations.get(hidden_activation)\r\n",
    "        self.hidden_initializer = tf.keras.initializers.get(hidden_initializer)\r\n",
    "        self.epsilon = epsilon or 1e-12\r\n",
    "        \r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        if self.conditional:\r\n",
    "            masks = mask if mask is not None else []\r\n",
    "            masks = [m[None] for m in masks if m is not None]\r\n",
    "            if len(masks) == 0:\r\n",
    "                return None\r\n",
    "            else:\r\n",
    "                return K.all(K.concatenate(masks, axis=0), axis=0)\r\n",
    "        else:\r\n",
    "            return mask\r\n",
    "        \r\n",
    "    def build(self, input_shape):\r\n",
    "        super(LayerNormalization, self).build(input_shape)\r\n",
    "        if self.conditional:\r\n",
    "            shape = (input_shape[0][-1],)\r\n",
    "        else:\r\n",
    "            shape = (input_shape[-1],)\r\n",
    "        if self.center:\r\n",
    "            self.beta = self.add_weight(\r\n",
    "                shape=shape, initializer='zeros', name='beta')\r\n",
    "        if self.scale:\r\n",
    "            self.gamma = self.add_weight(\r\n",
    "                shape=shape, initializer='ones', name='gamma')\r\n",
    "        if self.conditional:\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                self.hidden_dense = tf.keras.layers.Dense(\r\n",
    "                    units=self.hidden_units,\r\n",
    "                    activation=self.hidden_activation,\r\n",
    "                    use_bias=False,\r\n",
    "                    kernel_initializer=self.hidden_initializer)\r\n",
    "            if self.center:\r\n",
    "                self.beta_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "            if self.scale:\r\n",
    "                self.gamma_dense = tf.keras.layers.Dense(\r\n",
    "                    units=shape[0], use_bias=False, kernel_initializer='zeros')\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        \"\"\"如果是条件Layer Norm，则默认以list为输入，第二个是condition\r\n",
    "        \"\"\"\r\n",
    "        if self.conditional:\r\n",
    "            inputs, cond = inputs\r\n",
    "            if self.hidden_units is not None:\r\n",
    "                cond = self.hidden_dense(cond)\r\n",
    "            for _ in range(K.ndim(inputs) - K.ndim(cond)):\r\n",
    "                cond = K.expand_dims(cond, 1)\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta_dense(cond) + self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma_dense(cond) + self.gamma\r\n",
    "        else:\r\n",
    "            if self.center:\r\n",
    "                beta = self.beta\r\n",
    "            if self.scale:\r\n",
    "                gamma = self.gamma\r\n",
    "        outputs = inputs\r\n",
    "        if self.center:\r\n",
    "            mean = K.mean(outputs, axis=-1, keepdims=True)\r\n",
    "            outputs = outputs - mean\r\n",
    "        if self.scale:\r\n",
    "            variance = K.mean(K.square(outputs), axis=-1, keepdims=True)\r\n",
    "            std = K.sqrt(variance + self.epsilon)\r\n",
    "            outputs = outputs / std\r\n",
    "            outputs = outputs * gamma\r\n",
    "        if self.center:\r\n",
    "            outputs = outputs + beta\r\n",
    "        return outputs\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_subject(inputs):\r\n",
    "    \"\"\"根据subject_ids从output中取出subject的向量表征\r\n",
    "    \"\"\"\r\n",
    "    output, subject_ids = inputs\r\n",
    "    start = tf.gather(output,subject_ids[:,0],axis=1,batch_dims=1)\r\n",
    "    end = tf.gather(output,subject_ids[:,1],axis=1,batch_dims=1)\r\n",
    "    subject = tf.keras.layers.Concatenate(axis=1)([start, end])\r\n",
    "    return subject\r\n",
    "'''\r\n",
    "   output.shape = \r\n",
    "   (None,128,768)\r\n",
    "   subjudec_ids.shape = (None,2)\r\n",
    "   start.shape = (None,None,768)\r\n",
    "   subject.shape = (None,None,1536)\r\n",
    "   subject[:,0].shape = (None,1536)\r\n",
    "   这一部分给出各个变量的shape应该一目了然\r\n",
    "'''\r\n",
    "   \r\n",
    "def build_model_2(pretrained_path, MAX_LEN, p2id):\r\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\r\n",
    "    s_po_index =  tf.keras.layers.Input((2,), dtype=tf.int32)\r\n",
    "    \r\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_path, output_hidden_states=True)\r\n",
    "    outputs = bert_model(ids, attention_mask=att)\r\n",
    "    x, _, hidden_states  = outputs[:3]\r\n",
    "    layer_1 = hidden_states[-1]\r\n",
    "    start_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    start_logits = tf.keras.layers.Lambda(lambda x: x**2)(start_logits)\r\n",
    "    \r\n",
    "    end_logits = tf.keras.layers.Dense(1,activation = 'sigmoid')(layer_1)\r\n",
    "    end_logits = tf.keras.layers.Lambda(lambda x: x**2)(end_logits)\r\n",
    "    \r\n",
    "    subject_1 = extract_subject([layer_1,s_po_index])\r\n",
    "    Normalization_1 = LayerNormalization(conditional=True)([layer_1, subject_1])\r\n",
    "    \r\n",
    "    op_out_put_start = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_start = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_start)\r\n",
    "    \r\n",
    "    op_out_put_end = tf.keras.layers.Dense(len(p2id),activation = 'sigmoid')(Normalization_1)\r\n",
    "    op_out_put_end = tf.keras.layers.Lambda(lambda x: x**4)(op_out_put_end)\r\n",
    "    \r\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[start_logits, end_logits, op_out_put_start, op_out_put_end])\r\n",
    "    model_2 = tf.keras.models.Model(inputs=[ids, att], outputs=[start_logits,end_logits])\r\n",
    "    model_3 = tf.keras.models.Model(inputs=[ids, att, s_po_index], outputs=[op_out_put_start, op_out_put_end])\r\n",
    "    return model, model_2, model_3\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rematch_text_word(tokenizer,text,enc_context,enc_start,enc_end):\r\n",
    "    span = [a.span()[0] for a in re.finditer(' ', text)]\r\n",
    "    decode_list = [tokenizer.decode([i]) for i in enc_context][1:]\r\n",
    "    start = 0\r\n",
    "    end = 0\r\n",
    "    len_start = 0\r\n",
    "    for i in range(len(decode_list)):\r\n",
    "        if i ==  enc_start - 1:\r\n",
    "            start = len_start\r\n",
    "        j = decode_list[i]\r\n",
    "        if '#' in j and len(j)>1:\r\n",
    "            j = j.replace('#','')\r\n",
    "        if j == '[UNK]':\r\n",
    "            j = '。'\r\n",
    "        len_start += len(j)\r\n",
    "        if i == enc_end - 1:\r\n",
    "            end = len_start\r\n",
    "            break\r\n",
    "    for span_index in span:\r\n",
    "        if start >= span_index:\r\n",
    "            start += 1\r\n",
    "            end += 1\r\n",
    "        if end > span_index and span_index>start:\r\n",
    "            end += 1\r\n",
    "    return text[start:end]\r\n",
    "\r\n",
    "\r\n",
    "class Metrics(tf.keras.callbacks.Callback):\r\n",
    "    def __init__(self,model_2, model_3, id2tag, va_text_list, va_spo_list, va_input_ids, va_attention_mask, tokenizer):\r\n",
    "        super(Metrics, self).__init__()\r\n",
    "        self.model_2 = model_2\r\n",
    "        self.model_3 = model_3\r\n",
    "        self.id2tag = id2tag\r\n",
    "        self.va_input_ids = va_input_ids\r\n",
    "        self.va_attention_mask = va_attention_mask\r\n",
    "        self.va_spo_list = va_spo_list\r\n",
    "        self.va_text_list = va_text_list\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        \r\n",
    "    def on_train_begin(self, logs=None):\r\n",
    "        self.val_f1s = []\r\n",
    "        self.best_val_f1 = 0\r\n",
    "    \r\n",
    "    def get_same_element_index(self,ob_list):\r\n",
    "        return [i for (i, v) in enumerate(ob_list) if v == 1]\r\n",
    "    \r\n",
    "    def evaluate_data(self):\r\n",
    "        Y1 = self.model_2.predict([self.va_input_ids,self.va_attention_mask])\r\n",
    "        question=[]\r\n",
    "        answer=[]\r\n",
    "        for m in range(len(Y1[0])):\r\n",
    "            for z in self.va_spo_list[m]:\r\n",
    "                question.append((z['subject'],z['predicate'],z['object']))\r\n",
    "            start = np.where(Y1[0][m]>0.5)[0]\r\n",
    "            end = np.where(Y1[1][m]>0.5)[0]\r\n",
    "            subjects = []\r\n",
    "            for i in start:\r\n",
    "                j = end[end >= i]\r\n",
    "                if len(j) > 0:\r\n",
    "                    j = j[0]\r\n",
    "                    subjects.append((i, j))\r\n",
    "            if subjects:\r\n",
    "                token_ids_2 = np.repeat([self.va_input_ids[m]], len(subjects), 0)\r\n",
    "                attention_mask_2 = np.repeat([self.va_attention_mask[m]], len(subjects), 0)\r\n",
    "                subjects = np.array(subjects)\r\n",
    "                object_preds_start,object_preds_end = self.model_3.predict([token_ids_2, attention_mask_2, subjects])\r\n",
    "                for subject,object_start,object_end in zip(subjects,object_preds_start,object_preds_end):\r\n",
    "                    sub = rematch_text_word(self.tokenizer,self.va_text_list[m],self.va_input_ids[m],subject[0],subject[1])\r\n",
    "                    start = np.argwhere(object_start > 0.5)\r\n",
    "                    end = np.argwhere(object_end > 0.5)\r\n",
    "                    for _start, predicate1 in start:\r\n",
    "                        for _end, predicate2 in end:\r\n",
    "                            if _start <= _end and predicate1 == predicate2:\r\n",
    "                                ans = rematch_text_word(self.tokenizer,self.va_text_list[m],self.va_input_ids[m],_start,_end)\r\n",
    "                                answer.append((sub,self.id2tag[predicate1],ans))\r\n",
    "                                break\r\n",
    "        Q = set(question)\r\n",
    "        S = set(answer)\r\n",
    "        f1 = 2*len(Q&S)/(len(Q)+len(S))\r\n",
    "        return f1\r\n",
    "    \r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        logs = logs or {}\r\n",
    "        _val_f1 = self.evaluate_data()\r\n",
    "        self.val_f1s.append(_val_f1)\r\n",
    "        logs['val_f1'] = _val_f1\r\n",
    "        if _val_f1 > self.best_val_f1:\r\n",
    "            self.model.save_weights('../model_dirs/fine_tune_relation_extraction/09_f1={}_model.hdf5'.format(round(_val_f1,4)))\r\n",
    "            self.best_val_f1 = _val_f1\r\n",
    "            print(\"best f1: {} \\n\".format(self.best_val_f1))\r\n",
    "        else:\r\n",
    "            print(\"val f1: {}, but not the best f1 \\n\".format(_val_f1))\r\n",
    "        return      "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# config = BertConfig.from_json_file('../model_dirs/chinese-bert-wwm-ext/config.json')\r\n",
    "# TFBertModel.from_pretrained(pretrained_path, config=config)\r\n",
    "K.clear_session()\r\n",
    "model,model_2,model_3 = build_model_2(pretrained_path,  MAX_LEN, p2id)\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\r\n",
    "model.compile(loss={'lambda': new_loss,\r\n",
    "                'lambda_1': new_loss,\r\n",
    "                'lambda_2': new_loss,\r\n",
    "                'lambda_3': new_loss},optimizer=optimizer)\r\n",
    "model.fit([input_ids, attention_mask, send_s_po], [start_tokens, end_tokens, object_start_tokens, object_end_tokens], \\\r\n",
    "        epochs=20, batch_size=2, callbacks=[Metrics(model_2, model_3 ,id2p, dev_text, dev_spo, val_input_ids, val_attention_mask, tokenizer)])\r\n",
    "\r\n",
    "h5_path = '../model_dirs/fine_tune_relation_extraction/tf_model.h5'\r\n",
    "model.save_weights(h5_path)\r\n",
    "checkpoint_path = '../model_dirs/fine_tune_relation_extraction/checkpoints/my_checkpoint'\r\n",
    "model.save_weights(checkpoint_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-10 05:07:08.685399: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-08-10 05:07:09.096196: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-08-10 05:07:09.096679: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../model_dirs/chinese-bert-wwm-ext.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fb8645c5050>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fb8645c5050>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-10 05:07:14.247313: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 6640427008 exceeds 10% of free system memory.\n",
      "2021-08-10 05:07:16.715966: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 6640427008 exceeds 10% of free system memory.\n",
      "2021-08-10 05:07:19.318263: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-08-10 05:07:19.330544: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 4200050000 Hz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "4973/4973 [==============================] - 663s 132ms/step - loss: 2389.1344 - lambda_loss: 53.9302 - lambda_1_loss: 39.2875 - lambda_2_loss: 1102.4303 - lambda_3_loss: 1193.4855\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "best f1: 0.2117822520507084 \n",
      "\n",
      "Epoch 2/20\n",
      "4973/4973 [==============================] - 657s 132ms/step - loss: 65.6473 - lambda_loss: 9.1369 - lambda_1_loss: 9.7145 - lambda_2_loss: 23.5745 - lambda_3_loss: 23.2214\n",
      "best f1: 0.33879310344827585 \n",
      "\n",
      "Epoch 3/20\n",
      "4973/4973 [==============================] - 657s 132ms/step - loss: 47.2121 - lambda_loss: 8.1146 - lambda_1_loss: 8.7436 - lambda_2_loss: 14.8727 - lambda_3_loss: 15.4811\n",
      "best f1: 0.3876472978464039 \n",
      "\n",
      "Epoch 4/20\n",
      "4973/4973 [==============================] - 658s 132ms/step - loss: 40.8578 - lambda_loss: 7.5516 - lambda_1_loss: 8.1569 - lambda_2_loss: 12.1924 - lambda_3_loss: 12.9569\n",
      "val f1: 0.37970401691331923, but not the best f1 \n",
      "\n",
      "Epoch 5/20\n",
      "4973/4973 [==============================] - 657s 132ms/step - loss: 37.4601 - lambda_loss: 7.3264 - lambda_1_loss: 7.8971 - lambda_2_loss: 10.8678 - lambda_3_loss: 11.3688\n",
      "val f1: 0.3859283930058285, but not the best f1 \n",
      "\n",
      "Epoch 6/20\n",
      "4973/4973 [==============================] - 657s 132ms/step - loss: 34.2037 - lambda_loss: 7.0518 - lambda_1_loss: 7.5732 - lambda_2_loss: 9.5358 - lambda_3_loss: 10.0429\n",
      "best f1: 0.4107469977610421 \n",
      "\n",
      "Epoch 7/20\n",
      "4973/4973 [==============================] - 657s 132ms/step - loss: 31.9805 - lambda_loss: 6.8298 - lambda_1_loss: 7.3106 - lambda_2_loss: 8.7189 - lambda_3_loss: 9.1212\n",
      "best f1: 0.42593694229625223 \n",
      "\n",
      "Epoch 8/20\n",
      "4973/4973 [==============================] - 657s 132ms/step - loss: 29.5538 - lambda_loss: 6.5421 - lambda_1_loss: 7.0448 - lambda_2_loss: 7.7441 - lambda_3_loss: 8.2228\n",
      "val f1: 0.4146841673502871, but not the best f1 \n",
      "\n",
      "Epoch 9/20\n",
      "4973/4973 [==============================] - 656s 132ms/step - loss: 28.0689 - lambda_loss: 6.4299 - lambda_1_loss: 6.8938 - lambda_2_loss: 7.1656 - lambda_3_loss: 7.5796\n",
      "val f1: 0.40195878392164863, but not the best f1 \n",
      "\n",
      "Epoch 10/20\n",
      "4973/4973 [==============================] - 656s 132ms/step - loss: 26.8778 - lambda_loss: 6.2743 - lambda_1_loss: 6.7704 - lambda_2_loss: 6.6914 - lambda_3_loss: 7.1417\n",
      "val f1: 0.40454914703493094, but not the best f1 \n",
      "\n",
      "Epoch 11/20\n",
      "4973/4973 [==============================] - 656s 132ms/step - loss: 25.5546 - lambda_loss: 6.1679 - lambda_1_loss: 6.5741 - lambda_2_loss: 6.2053 - lambda_3_loss: 6.6072\n",
      "val f1: 0.4087414002428167, but not the best f1 \n",
      "\n",
      "Epoch 12/20\n",
      "4973/4973 [==============================] - 657s 132ms/step - loss: 24.7735 - lambda_loss: 6.0601 - lambda_1_loss: 6.3918 - lambda_2_loss: 5.9643 - lambda_3_loss: 6.3574\n",
      "val f1: 0.40784313725490196, but not the best f1 \n",
      "\n",
      "Epoch 13/20\n",
      "4973/4973 [==============================] - 656s 132ms/step - loss: 24.0495 - lambda_loss: 6.0167 - lambda_1_loss: 6.3059 - lambda_2_loss: 5.7030 - lambda_3_loss: 6.0238\n",
      "val f1: 0.41927710843373495, but not the best f1 \n",
      "\n",
      "Epoch 14/20\n",
      "4973/4973 [==============================] - 656s 132ms/step - loss: 23.2831 - lambda_loss: 5.9069 - lambda_1_loss: 6.2649 - lambda_2_loss: 5.4175 - lambda_3_loss: 5.6938\n",
      "val f1: 0.40948712750861543, but not the best f1 \n",
      "\n",
      "Epoch 15/20\n",
      "2968/4973 [================>.............] - ETA: 4:24 - loss: 22.7494 - lambda_loss: 5.8241 - lambda_1_loss: 6.1336 - lambda_2_loss: 5.2017 - lambda_3_loss: 5.5899"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pretrained_path = '../model_dirs/chinese-bert-wwm-ext'\r\n",
    "checkpoint_path = '../model_dirs/fine_tune_relation_extraction/checkpoints/my_checkpoint'\r\n",
    "h5_path = '../model_dirs/fine_tune_relation_extraction/tf_model.h5'\r\n",
    "model,model_2,model_3 = build_model_2(pretrained_path, MAX_LEN, p2id)\r\n",
    "model.load_weights(h5_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx = 0\r\n",
    "x1 = [input_ids[[idx]], attention_mask[[idx]]]\r\n",
    "sub_start_tokens, sub_end_tokens = model_2.predict(x1)\r\n",
    "train_spo[idx], p2id[train_spo[idx][0]['predicate']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub_start_idx = int(np.argwhere(sub_start_tokens[0,:,0] > 0.5)[0])\r\n",
    "sub_end_idx = int(np.argwhere(sub_end_tokens[0,:,0] > 0.5)[0])\r\n",
    "sub_text = tokenizer.decode(input_ids[idx][sub_start_idx:sub_end_idx+1]).replace(' ','')\r\n",
    "sub_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x2 = [input_ids[[idx]], attention_mask[[idx]], send_s_po[[idx]]]\r\n",
    "obj_start_tokens, obj_end_tokens = model_3.predict(x2)\r\n",
    "obj_start_tokens.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obj_start_idx = np.argwhere(obj_start_tokens[0] > 0.5)\r\n",
    "obj_end_idx = np.argwhere(obj_end_tokens[0] > 0.5)\r\n",
    "for _start, predicate1 in obj_start_idx:\r\n",
    "    for _end, predicate2 in obj_end_idx:\r\n",
    "        if _start <= _end and predicate1 == predicate2:\r\n",
    "            print(_start, _end, predicate1)\r\n",
    "            obj_text = tokenizer.decode(input_ids[idx][_start:_end+1]).replace(' ','')\r\n",
    "            print(obj_text, id2p[predicate1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "pid = os.getpid()\r\n",
    "!kill -9 $pid"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b82c4dcea0e292ae3c5056cb1ce5f519b54b2436655e008d9c8bf4bca92723c8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('py3.7': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}