{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertPreTrainedModel, TFBertMainLayer, BertTokenizer\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-13fcf0c0cf30>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                        comment_text               label\n",
       "0  Explanation\\nWhy the edits made under my usern...  [0, 0, 0, 0, 0, 0]\n",
       "1  D'aww! He matches this background colour I'm s...  [0, 0, 0, 0, 0, 0]\n",
       "2  Hey man, I'm really not trying to edit war. It...  [0, 0, 0, 0, 0, 0]\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...  [0, 0, 0, 0, 0, 0]\n",
       "4  You, sir, are my hero. Any chance you remember...  [0, 0, 0, 0, 0, 0]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# parameters\n",
    "train_path = \"data/Toxic Comment Classification Challenge/train.csv\"\n",
    "test_path = \"data/Toxic Comment Classification Challenge/test.csv\"\n",
    "df = pd.read_csv(train_path)[:1000]\n",
    "df['label'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['comment_text', 'label']].copy()\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FULL Dataset: (1000, 2)\nTRAIN Dataset: (900, 2)\nTEST Dataset: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_size=0.9\n",
    "test_data = pd.read_csv(test_path)[:1000]\n",
    "train_data = new_df.sample(frac=train_size,random_state=200)\n",
    "val_data = new_df.drop(train_data.index).reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_tf_utils import get_initializer\n",
    "\n",
    "class TFBertForMultilabelClassification(TFBertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFBertForMultilabelClassification, self).__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(config.num_labels,\n",
    "                                                kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                                name='classifier',\n",
    "                                                activation='sigmoid')#--------------------- sigmoid激活函数\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        return outputs  # logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customDataset(dataSet):\n",
    "    inputs = tokenizer(dataSet['comment_text'].tolist(), max_length=max_length, padding='max_length', truncation=True,\\\n",
    "                   return_tensors='tf')\n",
    "    if 'label' in dataSet.columns:\n",
    "        label_list = dataSet['label'].values.tolist() \n",
    "    else:\n",
    "        label_list = None\n",
    "    result = tf.data.Dataset.from_tensor_slices((dict((k,v) for k, v in inputs.items()), label_list))          \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model_dirs/bert-base-uncased'\n",
    "# parameters\n",
    "max_length = 128\n",
    "batch_size = 6\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 2\n",
    "num_classes = 6 \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "ds_train_encoded = customDataset(train_data).shuffle(100).batch(batch_size)\n",
    "ds_val_encoded = customDataset(val_data).batch(batch_size)\n",
    "ds_test_encoded = customDataset(test_data).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at ./model_dirs/bert-base-uncased were not used when initializing TFBertForMultilabelClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForMultilabelClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForMultilabelClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForMultilabelClassification were not initialized from the model checkpoint at ./model_dirs/bert-base-uncased and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 89s 506ms/step - loss: 0.3538 - categorical_accuracy: 0.2782 - val_loss: 0.1759 - val_categorical_accuracy: 0.9400\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 76s 510ms/step - loss: 0.0975 - categorical_accuracy: 0.9770 - val_loss: 0.1381 - val_categorical_accuracy: 1.0000\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 0.1381 - categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model initialization\n",
    "model = TFBertForMultilabelClassification.from_pretrained(model_path, num_labels=num_classes)#------------6个标签\n",
    "# optimizer Adam recommended\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08, clipnorm=1)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.BinaryCrossentropy()#-----------------------------------binary_crossentropy 损失函数\n",
    "metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "# fit model\n",
    "bert_history = model.fit(ds_train_encoded, epochs= num_epochs, validation_data=ds_val_encoded)\n",
    "model.evaluate(ds_val_encoded)\n",
    "model.save_pretrained('./model_dirs/fine_tune_multiLable_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# evaluate val_set\n",
    "pred=model.predict(ds_val_encoded)[0]\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
       " array([[0.03432674, 0.0129332 , 0.01523828, 0.01330916, 0.0163554 ,\n",
       "         0.01127039],\n",
       "        [0.8315485 , 0.17829879, 0.6357816 , 0.14911345, 0.67899   ,\n",
       "         0.2686458 ],\n",
       "        [0.04567312, 0.01195491, 0.01618134, 0.01237704, 0.01863166,\n",
       "         0.01145404],\n",
       "        [0.26554868, 0.02451638, 0.05750464, 0.02063552, 0.07555198,\n",
       "         0.02412943],\n",
       "        [0.8255753 , 0.17758033, 0.62832904, 0.15576462, 0.67711455,\n",
       "         0.2649693 ],\n",
       "        [0.05843513, 0.01301575, 0.01847906, 0.01224479, 0.02047151,\n",
       "         0.01184892]], dtype=float32)>,)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "model(list(ds_val_encoded)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    for _, data in enumerate(ds_val_encoded, 0):\n",
    "        inputs = data[0]\n",
    "        targets = data[1]\n",
    "        outputs = model(inputs)[0]\n",
    "        fin_targets.extend(targets.numpy().tolist())\n",
    "        fin_outputs.extend(outputs.numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy Score = 0.86\nrecall_score (Micro) = 0.38095238095238093\nrecall_score (Macro) = 0.23703703703703702\nF1 Score (Micro) = 0.5423728813559321\nF1 Score (Macro) = 0.313997113997114\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = validation(num_epochs)\n",
    "targets = np.array(targets)\n",
    "outputs = np.array(outputs) >= 0.5\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "recall_score_micro = metrics.recall_score(targets, outputs, average='micro')\n",
    "recall_score_macro = metrics.recall_score(targets, outputs, average='macro')\n",
    "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "\n",
    "print(f\"Accuracy Score = {accuracy}\")\n",
    "print(f\"recall_score (Micro) = {recall_score_micro}\")\n",
    "print(f\"recall_score (Macro) = {recall_score_macro}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('tfs': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3565398b46c6be2ef26a938e8041889bf1c2c480b6cc17f2da400e44f7937fcd"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}