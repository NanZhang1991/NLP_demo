{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\miniconda3\\envs\\KB\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Administrator\\miniconda3\\envs\\KB\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Administrator\\miniconda3\\envs\\KB\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Administrator\\miniconda3\\envs\\KB\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Administrator\\miniconda3\\envs\\KB\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Administrator\\miniconda3\\envs\\KB\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\r\n",
    "import numpy as np\r\n",
    "from bert4keras.backend import keras, K, batch_gather\r\n",
    "from bert4keras.layers import Loss\r\n",
    "from bert4keras.layers import LayerNormalization\r\n",
    "from bert4keras.tokenizers import Tokenizer\r\n",
    "from bert4keras.models import build_transformer_model\r\n",
    "from bert4keras.optimizers import Adam, extend_with_exponential_moving_average\r\n",
    "from bert4keras.snippets import sequence_padding \r\n",
    "from bert4keras.snippets import open, to_array\r\n",
    "from keras.layers import Input, Dense, Lambda, Reshape\r\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_path = 'vocab.txt'\r\n",
    "tokenizer = Tokenizer(bert_vocab_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 4263, 2548, 1290, 185, 2225, 4906, 185, 1812, 2209, 1298, 6832, 3172, 8020, 8629, 118, 8021, 8024, 3221, 671, 855, 6716, 7770, 1372, 3300, 8203, 1062, 1146, 1520, 840, 3683, 762, 4511, 2094, 8024, 860, 7028, 8108, 1062, 3165, 8024, 1372, 3683, 7390, 6716, 6121, 3330, 7770, 671, 763, 8024, 8166, 2399, 5815, 1395, 2225, 3172, 686, 4518, 5279, 2497, 3633, 2466, 6371, 6395, 8024, 2768, 711, 1059, 4413, 2496, 791, 3297, 4765, 4638, 2768, 2399, 4511, 782, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "text = '爱德华·尼科·埃尔南迪斯（1986-），是一位身高只有70公分哥伦比亚男子，体重10公斤，只比随身行李高一些，2010年获吉尼斯世界纪录正式认证，成为全球当今最矮的成年男人'\r\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '爱', '德', '华', '·', '尼', '科', '·', '埃', '尔', '南', '迪', '斯', '（', '1986', '-', '）', '，', '是', '一', '位', '身', '高', '只', '有', '70', '公', '分', '哥', '伦', '比', '亚', '男', '子', '，', '体', '重', '10', '公', '斤', '，', '只', '比', '随', '身', '行', '李', '高', '一', '些', '，', '2010', '年', '获', '吉', '尼', '斯', '世', '界', '纪', '录', '正', '式', '认', '证', '，', '成', '为', '全', '球', '当', '今', '最', '矮', '的', '成', '年', '男', '人', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[[],\n [0],\n [1],\n [2],\n [3],\n [4],\n [5],\n [6],\n [7],\n [8],\n [9],\n [10],\n [11],\n [12],\n [13, 14, 15, 16],\n [17],\n [18],\n [19],\n [20],\n [21],\n [22],\n [23],\n [24],\n [25],\n [26],\n [27, 28],\n [29],\n [30],\n [31],\n [32],\n [33],\n [34],\n [35],\n [36],\n [37],\n [38],\n [39],\n [40, 41],\n [42],\n [43],\n [44],\n [45],\n [46],\n [47],\n [48],\n [49],\n [50],\n [51],\n [52],\n [53],\n [54],\n [55, 56, 57, 58],\n [59],\n [60],\n [61],\n [62],\n [63],\n [64],\n [65],\n [66],\n [67],\n [68],\n [69],\n [70],\n [71],\n [72],\n [73],\n [74],\n [75],\n [76],\n [77],\n [78],\n [79],\n [80],\n [81],\n [82],\n [83],\n [84],\n [85],\n []]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.rematch(text, tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(object):\r\n",
    "    \"\"\"数据生成器模版\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, data, batch_size=32, buffer_size=None):\r\n",
    "        self.data = data\r\n",
    "        self.batch_size = batch_size\r\n",
    "        if hasattr(self.data, '__len__'):\r\n",
    "            self.steps = len(self.data) // self.batch_size\r\n",
    "            if len(self.data) % self.batch_size != 0:\r\n",
    "                self.steps += 1\r\n",
    "        else:\r\n",
    "            self.steps = None\r\n",
    "        self.buffer_size = buffer_size or batch_size * 1000\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return self.steps\r\n",
    "\r\n",
    "    def sample(self, random=False):\r\n",
    "        \"\"\"采样函数，每个样本同时返回一个is_end标记\r\n",
    "        \"\"\"\r\n",
    "        if random:\r\n",
    "            if self.steps is None:\r\n",
    "\r\n",
    "                def generator():\r\n",
    "                    caches, isfull = [], False\r\n",
    "                    for d in self.data:\r\n",
    "                        caches.append(d)\r\n",
    "                        if isfull:\r\n",
    "                            i = np.random.randint(len(caches))\r\n",
    "                            yield caches.pop(i)\r\n",
    "                        elif len(caches) == self.buffer_size:\r\n",
    "                            isfull = True\r\n",
    "                    while caches:\r\n",
    "                        i = np.random.randint(len(caches))\r\n",
    "                        yield caches.pop(i)\r\n",
    "\r\n",
    "            else:\r\n",
    "\r\n",
    "                def generator():\r\n",
    "                    for i in np.random.permutation(len(self.data)):\r\n",
    "                        yield self.data.loc[i]\r\n",
    "\r\n",
    "            data = generator()\r\n",
    "        else:\r\n",
    "            data = iter(self.data.iterrows())\r\n",
    "        d_current = next(data)\r\n",
    "        for d_next in data:\r\n",
    "            yield False, d_current\r\n",
    "            d_current = d_next\r\n",
    "\r\n",
    "        yield True, d_current\r\n",
    "\r\n",
    "    def __iter__(self, random=False):\r\n",
    "        raise NotImplementedError\r\n",
    "\r\n",
    "    def forfit(self, random=True):\r\n",
    "        while True:\r\n",
    "            for d in self.__iter__(random):\r\n",
    "                yield d\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(pattern, sequence):\r\n",
    "    \"\"\"从sequence中寻找子串pattern\r\n",
    "    如果找到，返回第一个下标；否则返回-1。\r\n",
    "    \"\"\"\r\n",
    "    n = len(pattern)\r\n",
    "    for i in range(len(sequence)):\r\n",
    "        if sequence[i:i + n] == pattern:\r\n",
    "            return i\r\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 128\r\n",
    "\r\n",
    "class data_generator(DataGenerator):\r\n",
    "    \"\"\"数据生成器\r\n",
    "    \"\"\"\r\n",
    "    def __iter__(self, random=False):\r\n",
    "        batch_token_ids, batch_segment_ids = [], []\r\n",
    "        batch_subject_labels, batch_subject_ids, batch_object_labels = [], [], []\r\n",
    "        for is_end, d in self.sample(random):\r\n",
    "            token_ids, segment_ids = tokenizer.encode(d[1][1], maxlen=maxlen)\r\n",
    "            # 整理三元组 {s: [(o, p)]}\r\n",
    "            spoes = {}\r\n",
    "            for spo in d[1][2]:\r\n",
    "                print(spo)\r\n",
    "                s = tokenizer.encode(spo['subject'])[1:-1]\r\n",
    "                print(s)\r\n",
    "                p = predicate2id[spo['predicate']]\r\n",
    "                o = tokenizer.encode(spo['object'])[1:-1]\r\n",
    "                s_idx = search(s, token_ids)\r\n",
    "                o_idx = search(o, token_ids)\r\n",
    "                if s_idx != -1 and o_idx != -1:\r\n",
    "                    s = (s_idx, s_idx + len(s) - 1)\r\n",
    "                    o = (o_idx, o_idx + len(o) - 1, p)\r\n",
    "                    if s not in spoes:\r\n",
    "                        spoes[s] = []\r\n",
    "                    spoes[s].append(o)\r\n",
    "            if spoes:\r\n",
    "                # subject标签\r\n",
    "                subject_labels = np.zeros((len(token_ids), 2))\r\n",
    "                for s in spoes:\r\n",
    "                    subject_labels[s[0], 0] = 1\r\n",
    "                    subject_labels[s[1], 1] = 1\r\n",
    "                # 随机选一个subject（这里没有实现错误！这就是想要的效果！！）\r\n",
    "                start, end = np.array(list(spoes.keys())).T\r\n",
    "                start = np.random.choice(start)\r\n",
    "                end = np.random.choice(end[end >= start])\r\n",
    "                subject_ids = (start, end)\r\n",
    "                # 对应的object标签\r\n",
    "                object_labels = np.zeros((len(token_ids), len(predicate2id), 2))\r\n",
    "                for o in spoes.get(subject_ids, []):\r\n",
    "                    object_labels[o[0], o[2], 0] = 1\r\n",
    "                    object_labels[o[1], o[2], 1] = 1\r\n",
    "                # 构建batch\r\n",
    "                batch_token_ids.append(token_ids)\r\n",
    "                batch_segment_ids.append(segment_ids)\r\n",
    "                batch_subject_labels.append(subject_labels)\r\n",
    "                batch_subject_ids.append(subject_ids)\r\n",
    "                batch_object_labels.append(object_labels)\r\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\r\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\r\n",
    "                    batch_segment_ids = sequence_padding(batch_segment_ids)\r\n",
    "                    batch_subject_labels = sequence_padding(\r\n",
    "                        batch_subject_labels\r\n",
    "                    )\r\n",
    "                    batch_subject_ids = np.array(batch_subject_ids)\r\n",
    "                    batch_object_labels = sequence_padding(batch_object_labels)\r\n",
    "                    yield [\r\n",
    "                        batch_token_ids, batch_segment_ids,\r\n",
    "                        batch_subject_labels, batch_subject_ids,\r\n",
    "                        batch_object_labels\r\n",
    "                    ], None\r\n",
    "                    batch_token_ids, batch_segment_ids = [], []\r\n",
    "                    batch_subject_labels, batch_subject_ids, batch_object_labels = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "train_path = r'D:\\Documents\\project\\demo\\transformers_tutorial\\data\\百度关系抽取数据集\\train_data.json'\r\n",
    "train_data = pd.read_json(train_path, lines=True)[:4]\r\n",
    "\r\n",
    "schemads_path = 'D:/Documents/project/demo/transformers_tutorial/data/百度关系抽取数据集/all_50_schemas'\r\n",
    "predicate_data = pd.read_json(schemads_path, lines=True)\r\n",
    "id2p = predicate_data['predicate'].drop_duplicates().reset_index(drop=True).to_dict()\r\n",
    "predicate2id = dict(zip(id2p.values(), id2p.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator(train_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\r\n",
    "\r\n",
    "x = K.constant([5, 4, 6])\r\n",
    "y = K.constant([5, 2, 5])\r\n",
    "z = K.cast(K.greater(x, y),'float32')\r\n",
    "\r\n",
    "with tf.Session() as sess:\r\n",
    "    print(sess.run(z))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.  6. 11.]\n"
     ]
    }
   ],
   "source": [
    "x = K.constant([5, 4, 6])\r\n",
    "y = K.constant([5, 2, 5])\r\n",
    "z = K.sum([x,y],axis=0)\r\n",
    "with tf.Session() as sess:\r\n",
    "    print(sess.run(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-3e32dc2d6977>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrematch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.rematch(text, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('KB': conda)",
   "name": "python3710jvsc74a57bd083b5b3efe3a9978372deb74d899f5f21fa77216f13b11d502345dddf3bb4d657"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}