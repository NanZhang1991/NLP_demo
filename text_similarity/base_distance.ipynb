{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['上海市', '市级', '科技', '重大', '专项', '》']\n",
      "['上海市', '国家级', '科研', '重大', '项目']\n"
     ]
    }
   ],
   "source": [
    "from ltp import LTP\r\n",
    "ltp = LTP()\r\n",
    "\r\n",
    "sentence1 = '上海市市级科技重大专项》'\r\n",
    "sentence2= '上海市国家级科研重大项目'\r\n",
    "\r\n",
    "segment1 = ltp.seg([sentence1])[0][0]\r\n",
    "segment2 = ltp.seg([sentence2])[0][0]\r\n",
    "print(segment1)\r\n",
    "print(segment2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.25"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard(text1, text2):\r\n",
    "    intersection = set(segment1)&set(segment2)\r\n",
    "    union = set(segment1)|set(segment2)\r\n",
    "    result = len(intersection)/len(union)\r\n",
    "    return result\r\n",
    "\r\n",
    "segment1 = ['上海市', '市级', '科技', '重大', '专项']\r\n",
    "segment2 = ['上海市','国家级', '科研', '重大', '项目']\r\n",
    "jaccard(segment1, segment2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents:['2017年11月21日，全国中小企业股份转让系统有限责任公司出具《关于同意江苏鸿基节能新技术股份有限公司终止股票全国中小企业股份转让系统挂牌的函》“股转系统函 [2017]6631号”，同意鸿基节能股票自2017年11月24日在全国中小企业股份转让系统终止挂牌。', '同日，鸿基节能出具《关于江苏鸿基节能新技术股份有限公司股票终止挂牌的公告》，鸿基节能决定自2017年11月24日起终止其股票挂牌'], segments:[['2017年', '11月', '21日', '全国', '中小企业', '股份', '转让', '系统', '有限', '责任', '公司', '出具', '同意', '江苏鸿基节能新技术股份有限公司', '终止', '股票', '全国', '中小企业', '股份', '转让', '系统', '挂牌', '函', '股转', '系统', '函', '2017', '6631', '号', '同意', '鸿基节能', '股票', '2017年', '11月', '24日', '全国', '中小企业', '股份', '转让', '系统', '终止', '挂牌'], ['同日', '鸿基节能', '出具', '江苏鸿基节能新技术股份有限公司', '股票', '终止', '挂牌', '公告', '鸿基', '节能', '2017年', '11月', '24日', '终止', '股票', '挂牌']]\n",
      "[['建筑', '工程', '施工', '环境', '状态', '智能', '监测', '监控', '装置', '研究']]\n"
     ]
    }
   ],
   "source": [
    "import string\r\n",
    "from zhon.hanzi import punctuation\r\n",
    "import re\r\n",
    "from ltp import LTP\r\n",
    "ltp = LTP()\r\n",
    "full_name = '江苏鸿基节能新技术股份有限公司'\r\n",
    "short_name = '鸿基节能'\r\n",
    "ltp.add_words(words=[full_name,short_name], max_window=len(full_name))\r\n",
    "\r\n",
    "class Custom_segment:\r\n",
    "    def __init__(self):\r\n",
    "        self.stopwords = self.get_stopwords()\r\n",
    "\r\n",
    "    def get_stopwords(self):\r\n",
    "        with open('../text_process/baidu_stopwords.txt','r', encoding='utf-8') as f:\r\n",
    "            content = f.readlines()\r\n",
    "            stopwords = list(map(str.strip, content))\r\n",
    "        return  stopwords\r\n",
    "\r\n",
    "    def cut_sentence(self, text):\r\n",
    "        sents = ltp.sent_split([text])\r\n",
    "        return sents\r\n",
    "\r\n",
    "    def tokenizers(self, sents):\r\n",
    "        segs = ltp.seg(sents)[0]\r\n",
    "        rem_p_segments = map(self.remove_punctuation, segs)\r\n",
    "        rem_sw_segments = map(self.del_stopwords, rem_p_segments)\r\n",
    "        return list(rem_sw_segments)\r\n",
    "\r\n",
    "    def remove_punctuation(self, words):\r\n",
    "        new_words = [word for word in words if word not in punctuation and word not in string.punctuation]\r\n",
    "        return new_words     \r\n",
    "\r\n",
    "    def del_stopwords(self, words):\r\n",
    "        new_words = [word for word in words if word not in self.stopwords]\r\n",
    "        return new_words\r\n",
    "        \r\n",
    "    def __call__(self, text):\r\n",
    "        sents = self.cut_sentence(text)\r\n",
    "        segments = self.tokenizers(sents)\r\n",
    "        return sents, segments\r\n",
    "\r\n",
    "\r\n",
    "if __name__==\"__main__\":\r\n",
    "    text = \"2017年11月21日，全国中小企业股份转让系统有限责任公司出具《关于同意江苏鸿基节能新技术股份有限公司终止股票全国中小企业股份转让系统挂牌的函》“股转系统函 [2017]6631号”，同意鸿基节能股票自2017年11月24日在全国中小企业股份转让系统终止挂牌。同日，鸿基节能出具《关于江苏鸿基节能新技术股份有限公司股票终止挂牌的公告》，鸿基节能决定自2017年11月24日起终止其股票挂牌\"\r\n",
    "    custom_segment = Custom_segment()\r\n",
    "    sents, segments = custom_segment(text)\r\n",
    "    print(f'sents:{sents}, segments:{segments}')\r\n",
    "    segments = custom_segment.tokenizers(['建筑工程施工环境安全状态智能监测及安全监控装置研究'])\r\n",
    "    print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The degree of inclusion of text 2 in text 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_segment: ['建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '装置', '研究', '12', '3']\n",
      "right_segment: ['建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '研究', '12', '11', '好', '研究', 'sss']\n",
      "right_segment2: ['12', '3', '建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '装置', '研究']\n",
      "rate: 0.7692307692307693\n",
      "rate2: 1.0\n",
      "intersection:['建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '研究', '12', '研究']\n",
      "unique_intersection_r: ['建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '研究', '12']\n",
      "unique_intersection_l: ['建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '研究', '12']\n",
      "matching_words_r:['建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '研究', '12']\n",
      "matching_words_l:['建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '研究', '12']\n",
      "pattern_r: 建筑.*?工程.*?施工.*?环境.*?状态.*?智能.*?检测.*?监控.*?研究.*?12.*?\n",
      "pattern_l: 建筑.*?工程.*?施工.*?环境.*?状态.*?智能.*?检测.*?监控.*?研究.*?12.*?\n",
      "search_str: 建筑工程施工环境安全状态智能检测及安全监控装置研究》（12\n",
      "intersection:['12', '3', '建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '装置', '研究']\n",
      "unique_intersection_r: ['12', '3', '建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '装置', '研究']\n",
      "unique_intersection_l: ['建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '装置', '研究', '12', '3']\n",
      "matching_words_r:['12', '3', '建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '装置', '研究']\n",
      "matching_words_l:['建筑', '工程', '施工', '环境', '状态', '智能', '检测', '监控', '装置', '研究', '12', '3']\n",
      "pattern_r: 12.*?3.*?建筑.*?工程.*?施工.*?环境.*?状态.*?智能.*?检测.*?监控.*?装置.*?研究.*?\n",
      "pattern_l: 建筑.*?工程.*?施工.*?环境.*?状态.*?智能.*?检测.*?监控.*?装置.*?研究.*?12.*?3.*?\n",
      "search_str2: 建筑工程施工环境安全状态智能检测及安全监控装置研究》（12那3）\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "import random\r\n",
    "import re \r\n",
    "\r\n",
    "class Lexical_analysis:\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def inclusion_rate(left_segment, right_segment, left_sentence, right_sentence):\r\n",
    "\r\n",
    "        '''\r\n",
    "        Calculate the inclusion degree of the intersection of left segment and right segment in right segment\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        input: left_segment, right_segment,left_sentence, right_sentence\r\n",
    "        Returns\r\n",
    "        ----------\r\n",
    "        rate: float\r\n",
    "        '''\r\n",
    "        if right_sentence in left_sentence:\r\n",
    "            rate = 1.1\r\n",
    "        else:\r\n",
    "            intersection = set(left_segment) & set(right_segment)\r\n",
    "            # print(intersection)\r\n",
    "            rate = len(intersection) / len(set(right_segment))\r\n",
    "        return rate\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def get_search_str(left_segment, right_segment, left_sentence, right_sentence):\r\n",
    "        '''\r\n",
    "        find the serach string that use for add footnote in left_sentence \r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        input: left_sentence, right_sentence， left_segment, right_segment\r\n",
    "        Returns\r\n",
    "        ----------\r\n",
    "        search_str: string\r\n",
    "        '''    \r\n",
    "        if right_sentence in left_sentence:\r\n",
    "            search_str = right_sentence\r\n",
    "        else:\r\n",
    "            # Calculate the intersection of left_segment and right_segment\r\n",
    "            intersection = [seg for seg in right_segment if seg in left_segment]\r\n",
    "            print(f'intersection:{intersection}') #\r\n",
    "\r\n",
    "            # Remove duplicates and preserving order of elements\r\n",
    "            unique_intersection_r = sorted(list(dict.fromkeys(intersection).keys()),key=right_segment.index)\r\n",
    "            print(f'unique_intersection_r: {unique_intersection_r}') #\r\n",
    "            unique_intersection_l = sorted(list(dict.fromkeys(intersection).keys()),key=left_segment.index)\r\n",
    "            print(f'unique_intersection_l: {unique_intersection_l}') #\r\n",
    "\r\n",
    "            # Pick the word with the largest index in left_segment as location\r\n",
    "            # position_word_r = unique_intersection_r[-1]\r\n",
    "            # position_word_l = unique_intersection_l[-1]\r\n",
    "\r\n",
    "            # Pick the words at random as matching words\r\n",
    "            # unique_intersection_r.remove(position_word_r)\r\n",
    "            # unique_intersection_l.remove(position_word_l)\r\n",
    "            matching_words_r = sorted(unique_intersection_r, key=right_segment.index)\r\n",
    "            matching_words_l = sorted(unique_intersection_l, key=left_segment.index)\r\n",
    "            print(f'matching_words_r:{matching_words_r}')\r\n",
    "            print(f'matching_words_l:{matching_words_l}')\r\n",
    "            pattern_r = ''\r\n",
    "            for mwr in matching_words_r:\r\n",
    "                pattern_r = pattern_r + mwr + '.*?'\r\n",
    "            print(f'pattern_r: {pattern_r}') \r\n",
    "            pattern_l = ''\r\n",
    "            for mwr in matching_words_l:\r\n",
    "                pattern_l = pattern_l + mwr + '.*?'                \r\n",
    "            print(f'pattern_l: {pattern_l}')\r\n",
    "\r\n",
    "            if re.findall(pattern_r + '(.)', left_sentence) and \\\r\n",
    "                re.findall(pattern_r + '(.)', left_sentence)[0] in '》”’）':\r\n",
    "                search_str = re.findall(pattern_r + '.', left_sentence)[0]\r\n",
    "            elif re.search(pattern_r + '：“.+?”', left_sentence):\r\n",
    "                search_str = re.search(pattern_r + '：“.+?”', left_sentence).group()\r\n",
    "            elif re.findall(pattern_r, left_sentence):\r\n",
    "                search_str = re.findall(pattern_r, left_sentence)[0]\r\n",
    "            elif re.findall(pattern_l + '(.)', left_sentence) and \\\r\n",
    "                re.findall(pattern_l + '(.)', left_sentence)[0] in '》”’）':\r\n",
    "                search_str = re.findall(pattern_l + '.', left_sentence)[0]\r\n",
    "            elif re.search(pattern_l + '：“.+?”', left_sentence):\r\n",
    "                search_str = re.search(pattern_l + '：“.?”', left_sentence).group()\r\n",
    "            else :\r\n",
    "                search_str = re.findall(pattern_l, left_sentence)[0] \r\n",
    "        return search_str \r\n",
    "\r\n",
    "class Dependency_Parser:\r\n",
    "    pass\r\n",
    "\r\n",
    "if __name__==\"__main__\":\r\n",
    "\r\n",
    "    left_sentence = '《建筑工程施工环境安全状态智能检测及安全监控装置研究》（12那3）'\r\n",
    "    right_sentence = \"《建筑工程施工环境安全状态智能检测及安全监控研究》（12那11好研究sss）\"\r\n",
    "    right_sentence2 = \"（12那3）《建筑工程施工环境安全状态智能检测及安全监控装置研究》\"\r\n",
    "\r\n",
    "    left_segment = custom_segment.tokenizers([left_sentence])[0]\r\n",
    "    right_segment =custom_segment.tokenizers([right_sentence])[0]\r\n",
    "    right_segment2 =custom_segment.tokenizers([right_sentence2])[0]\r\n",
    "    print(f'left_segment: {left_segment}')\r\n",
    "    print(f'right_segment: {right_segment}')\r\n",
    "    print(f'right_segment2: {right_segment2}')\r\n",
    "\r\n",
    "    rate = Lexical_analysis.inclusion_rate(left_segment, right_segment, left_sentence, right_sentence)\r\n",
    "    rate2 = Lexical_analysis.inclusion_rate(left_segment, right_segment2, left_sentence, right_sentence2)\r\n",
    "    print(f'rate: {rate}')\r\n",
    "    print(f'rate2: {rate2}')\r\n",
    "\r\n",
    "\r\n",
    "    search_str = Lexical_analysis.get_search_str(left_segment, right_segment, left_sentence, right_sentence)\r\n",
    "    print(f'search_str: {search_str}')\r\n",
    "\r\n",
    "    search_str2 = Lexical_analysis.get_search_str(left_segment, right_segment2, left_sentence, right_sentence2) \r\n",
    "    print(f'search_str2: {search_str2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=[1,2,3]\r\n",
    "max(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[' [2017]6631号”《关于同意江苏鸿基节能新技术股份有限公司终止股票全国中小企业股份转让系统挂牌的函》，同意鸿基节能股票自2017年11月24日在全国中小企业股份转让系统']"
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_word_r = '函'\r\n",
    "pattern_r = '.*?同意.*?江苏.*?鸿基节能.*?新.*?技术.*?股份.*?有限公司.*?终止.*?股票.*?全国.*?中小企业.*?转让.*?系统.*?函'\r\n",
    "re.findall(pattern_r + '.*?', left_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['函', '同意', '技术', '全国', '中小企业', '挂牌', '系统']"
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "\n",
    "def replace_company_name(left_sentence, right_sentence):\n",
    "    if '发行人' in right_sentence and full_name in left_sentence:\n",
    "        right_sentence = right_sentence.replace('发行人', full_name)\n",
    "    elif '发行人' in right_sentence and short_name  in left_sentence:\n",
    "        right_sentence = right_sentence.replace('发行人', short_name)\n",
    "    elif full_name in right_sentence and '发行人'  in left_sentence:\n",
    "        right_sentence = right_sentence.replace(full_name, '发行人')\n",
    "    elif full_name in right_sentence and short_name in left_sentence:\n",
    "        right_sentence = right_sentence.replace(full_name, short_name)\n",
    "    elif short_name in right_sentence and '发行人' in left_sentence:\n",
    "        right_sentence = right_sentence.replace(short_name, '发行人')\n",
    "    elif short_name in right_sentence and full_name in left_sentence:\n",
    "        right_sentence = right_sentence.replace(short_name, full_name)\n",
    "    else:\n",
    "        right_sentence = right_sentence\n",
    "    return right_sentence\n"
=======
    "unique_intersection = ['全国', '中小企业', '股份', '转让', '系统', '函', '同意', '江苏', '鸿基节能', '新', '技术', '有限公司', '股票', '挂牌']\r\n",
    "random.sample(unique_intersection,  random.randint(1,len(unique_intersection)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('发行人', '江苏鸿基节能新技术股份有限公司'),\n ('发行人', '鸿基节能'),\n ('江苏鸿基节能新技术股份有限公司', '发行人'),\n ('江苏鸿基节能新技术股份有限公司', '鸿基节能'),\n ('鸿基节能', '发行人'),\n ('鸿基节能', '江苏鸿基节能新技术股份有限公司')]"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import permutations\r\n",
    "\r\n",
    "full_name = '江苏鸿基节能新技术股份有限公司'\r\n",
    "short_name = '鸿基节能'\r\n",
    "\r\n",
    "iter = permutations(['发行人', full_name, short_name], 2)\r\n",
    "list(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江苏鸿基节能新技术股份有限公司诉讼情况\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "def replace_company_name(left_sentence, right_sentence):\r\n",
    "    if '发行人' in right_sentence and full_name in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace('发行人', full_name)\r\n",
    "    elif '发行人' in right_sentence and short_name  in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace('发行人', short_name)\r\n",
    "    elif full_name in right_sentence and '发行人'  in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace(full_name, '发行人')\r\n",
    "    elif full_name in right_sentence and short_name in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace(full_name, short_name)\r\n",
    "    elif short_name in right_sentence and '发行人' in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace(short_name, '发行人')\r\n",
    "    elif short_name in right_sentence and full_name in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace(short_name, full_name)\r\n",
    "    else:\r\n",
    "        right_sentence = right_sentence\r\n",
    "    return right_sentence\r\n",
    "\r\n",
    "left_sentence = '本项目建设单位和实施主体为江苏鸿基节能新技术股份有限公司，不涉及选址和用地情况'\r\n",
    "right_sentence= '发行人诉讼情况'\r\n",
    "print(replace_company_name(left_sentence, right_sentence))"
>>>>>>> 88c04e8579e16bc9fe33a2d17de132782d04d18b
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('py3.7': conda)",
   "name": "python3710jvsc74a57bd037def44e3045786e2faeb86ea99d99641ffb6cda7629f9968e635f86f51987d0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}