{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['上海市', '市级', '科技', '重大', '专项', '》']\n",
      "['上海市', '国家级', '科研', '重大', '项目']\n"
     ]
    }
   ],
   "source": [
    "from ltp import LTP\r\n",
    "ltp = LTP()\r\n",
    "\r\n",
    "sentence1 = '上海市市级科技重大专项》'\r\n",
    "sentence2= '上海市国家级科研重大项目'\r\n",
    "\r\n",
    "segment1 = ltp.seg([sentence1])[0][0]\r\n",
    "segment2 = ltp.seg([sentence2])[0][0]\r\n",
    "print(segment1)\r\n",
    "print(segment2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.25"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard(text1, text2):\r\n",
    "    intersection = set(segment1)&set(segment2)\r\n",
    "    union = set(segment1)|set(segment2)\r\n",
    "    result = len(intersection)/len(union)\r\n",
    "    return result\r\n",
    "\r\n",
    "segment1 = ['上海市', '市级', '科技', '重大', '专项']\r\n",
    "segment2 = ['上海市','国家级', '科研', '重大', '项目']\r\n",
    "jaccard(segment1, segment2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents:['2021年他叫汤姆去拿《外衣》。', '汤姆生病了。', '他去了医院的住院部。'], segments:[['2021年', '汤姆', '去', '外衣'], ['汤姆', '生病'], ['去', '医院', '住院部']]\n",
      "['2021年', '汤姆', '去', '外衣']\n"
     ]
    }
   ],
   "source": [
    "import string\r\n",
    "from zhon.hanzi import punctuation\r\n",
    "import re\r\n",
    "from ltp import LTP\r\n",
    "ltp = LTP()\r\n",
    "full_name = '江苏鸿基节能新技术股份有限公司'\r\n",
    "short_name = '鸿基节能'\r\n",
    "\r\n",
    "ltp.add_words(words=[full_name, short_name], max_window=len(full_name))\r\n",
    "\r\n",
    "class Custom_segment:\r\n",
    "    def __init__(self):\r\n",
    "        self.stopwords = self.get_stopwords()\r\n",
    "\r\n",
    "    def get_stopwords(self):\r\n",
    "        with open('../text_process/baidu_stopwords.txt','r', encoding='utf-8') as f:\r\n",
    "            content = f.readlines()\r\n",
    "            stopwords = list(map(str.strip, content))\r\n",
    "        return  stopwords\r\n",
    "\r\n",
    "    def cut_sentence(self, text):\r\n",
    "        sents = ltp.sent_split([text])\r\n",
    "        return sents\r\n",
    "\r\n",
    "    def tokenizer(self, sent):\r\n",
    "        segs = ltp.seg([sent])[0][0]\r\n",
    "        rem_p_segments = self.remove_punctuation(segs)\r\n",
    "        rem_sw_segments = self.del_stopwords(rem_p_segments)\r\n",
    "        return list(rem_sw_segments)\r\n",
    "\r\n",
    "    def remove_punctuation(self, words):\r\n",
    "        new_words = (word for word in words if word not in punctuation and word not in string.punctuation)\r\n",
    "        return new_words     \r\n",
    "\r\n",
    "    def del_stopwords(self, words):\r\n",
    "        new_words = (word for word in words if word not in self.stopwords)\r\n",
    "        return new_words\r\n",
    "        \r\n",
    "    def __call__(self, text):\r\n",
    "        sents = self.cut_sentence(text)\r\n",
    "        segments = list(map(self.tokenizer, sents))\r\n",
    "        return sents, segments\r\n",
    "\r\n",
    "\r\n",
    "if __name__==\"__main__\":\r\n",
    "    text = \"2021年他叫汤姆去拿《外衣》。汤姆生病了。他去了医院的住院部。\"\r\n",
    "    custom_segment = Custom_segment()\r\n",
    "    sents, segments = custom_segment(text)\r\n",
    "    print(f'sents:{sents}, segments:{segments}')\r\n",
    "    segments = custom_segment.tokenizer('2021年他叫汤姆去拿《外衣》。')\r\n",
    "    print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The degree of inclusion of text 2 in text 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['项目', '建设', '单位', '实施', '主体', '江苏鸿基节能新技术股份有限公司', '不', '涉及', '选址', '情况']\n",
      "['江苏鸿基节能新技术股份有限公司', '诉讼', '情况']\n",
      "0.8947368421052632\n",
      "0.6666666666666666\n",
      "['江苏鸿基节能新技术股份有限公司', '情况']\n",
      "本项目建设单位和实施主体为江苏鸿基节能新技术股份有限公司，不涉及选址和用地情况》\n"
     ]
    }
   ],
   "source": [
    "class Lexical_analysis:\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def inclusion_rate(left_segment, right_segment):\r\n",
    "\r\n",
    "        '''\r\n",
    "        Calculate the inclusion degree of the intersection of left segment and right segment in right segment\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        input: left_segment, right_segment\r\n",
    "        Returns\r\n",
    "        ----------\r\n",
    "        similarity: float\r\n",
    "        '''\r\n",
    "        intersection = set(left_segment) & set(right_segment)\r\n",
    "        # print(intersection)\r\n",
    "        rate = len(intersection) / len(set(right_segment))\r\n",
    "        return rate\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def get_search_str(left_segment, right_segment, left_sentence):\r\n",
    "        '''\r\n",
    "        find the serach string that use for add footnote in left_sentence \r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        input: left_segment, right_segment\r\n",
    "        Returns\r\n",
    "        ----------\r\n",
    "        search_str: string\r\n",
    "        '''    \r\n",
    "        intersection = [seg for seg in left_segment if seg in right_segment]\r\n",
    "        print(intersection)\r\n",
    "        # Look for word that ends in 》） after intersection word \r\n",
    "        # special_words = [word for word in intersection if left_sentence[left_sentence.find(word) + len(word)] in '》）']\r\n",
    "        search_str=False\r\n",
    "        if left_sentence.find(intersection[-2:] + '》')\r\n",
    "        for word in intersection:\r\n",
    "            if left_sentence.find( word + '》') !=-1 :\r\n",
    "                search_str = left_sentence[:left_sentence.find(word + '》') + len(word)+1]\r\n",
    "            elif left_sentence.find( word + '）') !=-1:\r\n",
    "                search_str = left_sentence[:left_sentence.find(word + '）') + len(word)+1]\r\n",
    "        if search_str==False:\r\n",
    "            # The word with the largest index in intersection\r\n",
    "            position_word = intersection[-1]\r\n",
    "            # An index of the concluding words in left_sentence\r\n",
    "            index = left_sentence.rfind(position_word) + len(position_word)-1\r\n",
    "            if (index+1) < len(left_sentence):\r\n",
    "                if left_sentence[index+1] in '》”’）':\r\n",
    "                    search_str = left_sentence[:index+2]\r\n",
    "                else:\r\n",
    "                    search_str = left_sentence[:index+1]\r\n",
    "            else:\r\n",
    "                search_str = left_sentence[:index+1]\r\n",
    "        return search_str \r\n",
    "\r\n",
    "class Dependency_Parser:\r\n",
    "    pass\r\n",
    "\r\n",
    "if __name__==\"__main__\":\r\n",
    "\r\n",
    "    left_sentence = '本项目建设单位和实施主体为江苏鸿基节能新技术股份有限公司，不涉及选址和用地情况》'\r\n",
    "    right_sentence= '江苏鸿基节能新技术股份有限公司诉讼情况'\r\n",
    "\r\n",
    "\r\n",
    "    left_segment = custom_segment.tokenizer(left_sentence)\r\n",
    "    right_segment =custom_segment.tokenizer(right_sentence)\r\n",
    "    print(left_segment)\r\n",
    "    print(right_segment)\r\n",
    "\r\n",
    "    similarity = Lexical_analysis.inclusion_rate(left_sentence, right_sentence)\r\n",
    "    print(similarity)\r\n",
    "\r\n",
    "    similarity = Lexical_analysis.inclusion_rate(left_segment, right_segment)\r\n",
    "    print(similarity)\r\n",
    "\r\n",
    "    search_str = Lexical_analysis.get_search_str(left_segment, right_segment, left_sentence)\r\n",
    "    print(search_str)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('发行人', '江苏鸿基节能新技术股份有限公司'),\n ('发行人', '鸿基节能'),\n ('江苏鸿基节能新技术股份有限公司', '发行人'),\n ('江苏鸿基节能新技术股份有限公司', '鸿基节能'),\n ('鸿基节能', '发行人'),\n ('鸿基节能', '江苏鸿基节能新技术股份有限公司')]"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import permutations\r\n",
    "\r\n",
    "full_name = '江苏鸿基节能新技术股份有限公司'\r\n",
    "short_name = '鸿基节能'\r\n",
    "\r\n",
    "iter = permutations(['发行人', full_name, short_name], 2)\r\n",
    "list(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江苏鸿基节能新技术股份有限公司诉讼情况\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "def replace_company_name(left_sentence, right_sentence):\r\n",
    "    if '发行人' in right_sentence and full_name in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace('发行人', full_name)\r\n",
    "    elif '发行人' in right_sentence and short_name  in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace('发行人', short_name)\r\n",
    "    elif full_name in right_sentence and '发行人'  in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace(full_name, '发行人')\r\n",
    "    elif full_name in right_sentence and short_name in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace(full_name, short_name)\r\n",
    "    elif short_name in right_sentence and '发行人' in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace(short_name, '发行人')\r\n",
    "    elif short_name in right_sentence and full_name in left_sentence:\r\n",
    "        right_sentence = right_sentence.replace(short_name, full_name)\r\n",
    "    else:\r\n",
    "        right_sentence = right_sentence\r\n",
    "    return right_sentence\r\n",
    "\r\n",
    "left_sentence = '本项目建设单位和实施主体为江苏鸿基节能新技术股份有限公司，不涉及选址和用地情况'\r\n",
    "right_sentence= '发行人诉讼情况'\r\n",
    "print(replace_company_name(left_sentence, right_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'《a'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\r\n",
    "\r\n",
    "if  re.findall('《.+?》', left_sentence):\r\n",
    "    for special_str in re.findall('《.+?》', left_sentence):\r\n",
    "        if right_sentence_replace in special_str:\r\n",
    "            if left_sentence[left_sentence.index(special_str)+1] in '》）':\r\n",
    "                search_str = left_sentence[:left_sentence.index(special_str)+2]      \r\n",
    "            else:                              \r\n",
    "                search_str = special_str                            \r\n",
    "            index = right_sentences.index(right_sentence)\r\n",
    "            insert_footnotes_sentence = right_sentence + '，' + contents_df.loc[index, 'dirIndex']\r\n",
    "            result_df = result_df.append(pd.DataFrame({'paragraph_number':[paragraph_number], 'search_str':[search_str],\\\r\n",
    "                                                        'similarity':[1.1], 'insert_footnotes_sentence':[insert_footnotes_sentence]}))                        \r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('py3.7': conda)",
   "name": "python3710jvsc74a57bd037def44e3045786e2faeb86ea99d99641ffb6cda7629f9968e635f86f51987d0"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}