{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    train_set, x = train_test_split(df, \n",
    "        stratify=df['label'],\n",
    "        test_size=0.1, \n",
    "        random_state=42)\n",
    "    val_set, test_set = train_test_split(x, \n",
    "        stratify=x['label'],\n",
    "        test_size=0.5, \n",
    "        random_state=43)\n",
    "\n",
    "    return train_set,val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/THUCNewsChinese.txt\"\n",
    "# read data\n",
    "df_raw = pd.read_csv(data_path,sep=\"\\t\",header=None,names=[\"text\",\"label\"])    \n",
    "# transfer label\n",
    "df_label = pd.DataFrame({\"label\":[\"财经\",\"房产\",\"股票\",\"教育\",\"科技\",\"社会\",\"时政\",\"体育\",\"游戏\",\"娱乐\"],\"y\":list(range(10))})\n",
    "df_raw = pd.merge(df_raw,df_label,on=\"label\",how=\"left\")\n",
    "# split data\n",
    "train_data, val_data, test_data = split_dataset(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67386, text     房山顺成嘉苑在售91平米通透2居均价8200元(图)\n",
      "label                            房产\n",
      "y                                 1\n",
      "Name: 67386, dtype: object)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'房山顺成嘉苑在售91平米通透2居均价8200元(图)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = next(val_data.iterrows())\n",
    "print(s)\n",
    "review = s[1]['text']\n",
    "review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "  \n",
    "  # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length\n",
    "\treturn tokenizer.encode_plus(review, \n",
    "\t            add_special_tokens = True, # add [CLS], [SEP]\n",
    "\t            max_length = 32, # max length of the text that can go to BERT\n",
    "\t            pad_to_max_length = True, # add [PAD] tokens\n",
    "\t            return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "\t\t    truncation=True\n",
    "\t          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2791, 2255, 7556, 2768, 1649, 5723, 1762, 1545, 8440, 2398, 5101, 6858, 6851, 123, 2233, 1772, 817, 10398, 8129, 1039, 113, 1745, 114, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"D:/My_Document/Data_science/NLP/demo/Transformers/model_dirs/bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "input_ids = convert_example_to_feature(review)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2791, 2255, 7556, 2768, 1649, 5723, 1762, 1545, 8440, 2398, 5101, 6858, 6851, 123, 2233, 1772, 817, 10398, 8129, 1039, 113, 1745, 114, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(review, max_length = 32, pad_to_max_length=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 32), dtype=int32, numpy=\n",
       "array([[  101,  2791,  2255,  7556,  2768,  1649,  5723,  1762,  1545,\n",
       "         8440,  2398,  5101,  6858,  6851,   123,  2233,  1772,   817,\n",
       "        10398,  8129,  1039,   113,  1745,   114,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(1, 32), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 32), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(review, max_length = 32,  padding = 'max_length', truncation=True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def encode_examples(ds, limit=-1):\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "    \n",
    "    for index, row in ds.iterrows():\n",
    "        review = row[\"text\"]\n",
    "        label = row[\"y\"]\n",
    "#         bert_input = convert_example_to_feature(review)\n",
    "        bert_input = tokenizer(review, max_length = 32,  padding = 'max_length', truncation=True)\n",
    "  \n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append(label)\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# train dataset\n",
    "# ds_train_encoded = encode_examples(train_data).shuffle(10000).batch(batch_size)\n",
    "# # val dataset\n",
    "ds_val_encoded = encode_examples(val_data).batch(batch_size)\n",
    "# # test dataset\n",
    "# ds_test_encoded = encode_examples(test_data).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at fine_tune_model/ were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at fine_tune_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('fine_tune_model/',num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67386</th>\n",
       "      <td>房山顺成嘉苑在售91平米通透2居均价8200元(图)</td>\n",
       "      <td>房产</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150820</th>\n",
       "      <td>国内首只对冲基金成立一个月获益近2%</td>\n",
       "      <td>财经</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74822</th>\n",
       "      <td>TD数字无绳电话年内上市：欲成杀手级产品</td>\n",
       "      <td>科技</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146018</th>\n",
       "      <td>《无限传说》公布新登场角色画面欣赏</td>\n",
       "      <td>游戏</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122072</th>\n",
       "      <td>坦普：油价走高趋势不改</td>\n",
       "      <td>财经</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29755</th>\n",
       "      <td>羚牛争偶战败独自下山滋事 闯进农家院3人被困</td>\n",
       "      <td>社会</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52606</th>\n",
       "      <td>尼日尔总统任命加马蒂耶为总理</td>\n",
       "      <td>时政</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128983</th>\n",
       "      <td>《QQ三国》名人堂 低调也是炫耀�</td>\n",
       "      <td>游戏</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>中信银行与支付宝推快捷支付业务</td>\n",
       "      <td>科技</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21446</th>\n",
       "      <td>女性网上创业平台推出</td>\n",
       "      <td>科技</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text label  y\n",
       "67386   房山顺成嘉苑在售91平米通透2居均价8200元(图)    房产  1\n",
       "150820          国内首只对冲基金成立一个月获益近2%    财经  0\n",
       "74822         TD数字无绳电话年内上市：欲成杀手级产品    科技  4\n",
       "146018           《无限传说》公布新登场角色画面欣赏    游戏  8\n",
       "122072                 坦普：油价走高趋势不改    财经  0\n",
       "29755       羚牛争偶战败独自下山滋事 闯进农家院3人被困    社会  5\n",
       "52606               尼日尔总统任命加马蒂耶为总理    时政  6\n",
       "128983           《QQ三国》名人堂 低调也是炫耀�    游戏  8\n",
       "938                中信银行与支付宝推快捷支付业务    科技  4\n",
       "21446                   女性网上创业平台推出    科技  4"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['房山顺成嘉苑在售91平米通透2居均价8200元(图)',\n",
       " '国内首只对冲基金成立一个月获益近2%',\n",
       " 'TD数字无绳电话年内上市：欲成杀手级产品',\n",
       " '《无限传说》公布新登场角色画面欣赏',\n",
       " '坦普：油价走高趋势不改',\n",
       " '羚牛争偶战败独自下山滋事 闯进农家院3人被困',\n",
       " '尼日尔总统任命加马蒂耶为总理',\n",
       " '《QQ三国》名人堂 低调也是炫耀�',\n",
       " '中信银行与支付宝推快捷支付业务',\n",
       " '女性网上创业平台推出']"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[:10]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id = (tokenizer(val_data[:10]['text'].tolist(), max_length = 32,  padding = 'max_length', truncation=True, return_tensors='tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       "array([  101,  2791,  2255,  7556,  2768,  1649,  5723,  1762,  1545,\n",
       "        8440,  2398,  5101,  6858,  6851,   123,  2233,  1772,   817,\n",
       "       10398,  8129,  1039,   113,  1745,   114,   102,     0,     0,\n",
       "           0,     0,     0,     0,     0])>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]房山顺成嘉苑在售91平米通透2居均价820##0元(图)[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(tokenizer.convert_ids_to_tokens(list(ds_val_encoded)[0][0]['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]房山顺成嘉苑在售91平米通透2居均价820##0元(图)[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(tokenizer.convert_ids_to_tokens(input_id['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[-0.06825325, 10.845659  , -1.4365877 , -1.1311263 , -2.2768779 ,\n",
       "         1.2029951 , -1.2820653 , -1.1390705 , -1.3907384 , -4.198648  ],\n",
       "       [ 8.0618515 ,  0.9518317 ,  2.5615802 , -1.6324717 , -0.25369108,\n",
       "        -1.3738074 , -1.5680287 , -2.930448  , -4.0349464 , -4.084395  ],\n",
       "       [-0.9568867 , -2.1097174 ,  0.18050869, -0.76184905,  6.479282  ,\n",
       "        -1.5816749 , -1.2020916 , -1.1032    , -0.3342142 , -2.411711  ],\n",
       "       [-2.7709982 ,  0.8198834 , -1.387241  , -2.170227  ,  1.0532678 ,\n",
       "        -3.150462  , -2.0198505 , -1.335979  , 10.740458  , -0.4670856 ],\n",
       "       [ 7.5408697 ,  1.4736296 ,  4.245677  , -2.4453602 , -0.9757267 ,\n",
       "        -2.1691797 , -1.2424012 , -2.7686968 , -3.8558693 , -3.9255178 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_id)\n",
    "logits = outputs.logits\n",
    "logits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 4, 8, 0, 5, 6, 8, 4, 4]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def y_lable(logits):\n",
    "    for i in range(len(logits)):\n",
    "        y = tf.argmax(tf.nn.softmax(logits[i], axis=-1)).numpy()\n",
    "        yield y\n",
    "list(y_lable(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(ds_val_encoded)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dict(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 4, 8, 0, 5, 6, 8, 4, 4]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=model.predict(dict(input_id))[0]\n",
    "list(y_lable(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 4, 8, 0, 5, 6, 8, 4, 4]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(list(ds_val_encoded)[0][0])\n",
    "logits = outputs.logits\n",
    "list(y_lable(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 4, 8, 0, 5, 6, 8, 4, 4]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ds_val_encoded)[0][1].numpy().tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.8]",
   "language": "python",
   "name": "conda-env-py3.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
